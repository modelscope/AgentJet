astune:
  model:
    path: /mnt/data/model_cache/modelscope/hub/Qwen/Qwen/Qwen2___5-14B-Instruct
  data:
    max_prompt_length: 3000
    max_response_length: 15000
    train_batch_size: 32
  rollout:
    use_agentscope_protocol: True
    agentscope_learn_protocol: null
    use_step_reward_from_env: False
    binary_reward: False
    force_no_think: False
    force_think: False
    compute_madness_checklist:
      - "nonsense"
    gamma: 1.0
    agent_madness_termination: True # terminate_after_gone_mad
    agent_madness_reward: -1.0  # customize the reward when agent is detected as gone mad
    add_special_success_reward: False
    temperature: 0.9
    top_p: 1.0
    max_env_len: 4096
    max_response_length_in_one_turn: 4096
    max_model_len: 18000
    multi_turn:
      max_sample_per_task: 30
      max_steps: 30
    step_skip_action: 0 # skip action generation every N steps, 0 means never skip
    submit_oversample_multiplier: 1.5
    num_repeat: 4
    val_kwargs:
      temperature: 0.0
      top_k: -1
      top_p: 1.0
  context_manager: # context manager protocol is used ONLY when `use_agentscope_protocol=False`
    context_manager_type: "linear"
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512
    auto_context_cm:
      train_sp_action: False
      token_num_trigger_clip: 8000
    sliding_window_cm:
      enable_llm_memory_extraction: False
    linear_think_cm:
      remove_think_before_submit_as_action: False
      extract_box_before_submit_as_action: False
      train_history_infer_token: True



########################## verl config below ##########################
trainer:
  eval_pass_n: 3
  hfmodelpath: ""
  experiment_name: "read_yaml_name"
  n_gpus_per_node: 8
  nnodes: 1

env_service:
  env_type: "appworld"
  env_url: "http://127.0.0.1:8000"
  env_feedin_preference: code # code, text, box









########################## deprecated below ##########################
data:
  val_batch_size: 100000000000
  return_raw_chat: True
  filter_overlong_prompts: True
  truncation: error
  fast_eval: True


algorithm:
  task_norm_patch: False

actor_rollout_ref:
  hybrid_engine: True
  actor:
    override_ppo_mini_batch_num: -1
    entropy_coeff: 0
    loss_agg_mode: seq-mean-token-mean
  rollout:
    name: vllm
    mode: async
    use_agentscope_protocol: False
    debug_llm_io: False
    multi_turn:
      completion_callback: beyondagent.module.trainer.simple_completion_callback.SimpleCompletionCallback
      enable: True
      format: llama3_json
      max_steps: 30
      tool_config_path: null
    custom_dataflow_cls:
      path: ""
      name: ""
    train_sp_action: False
    tensor_model_parallel_size: 1
    max_num_seqs: 10
    gpu_memory_utilization: 0.9
    max_env_worker: 64
    enable_oversample: False
    enable_request_id: False
    env_array: False


thread_pool:
  max_workers: 5
