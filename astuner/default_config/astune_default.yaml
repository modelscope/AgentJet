# ------------------ main configuration ------------------
astuner:
  project_name: "astune_default_project"
  experiment_name: "read_yaml_name"
  experiment_dir: "auto"  # {exp-dir}/{experiment_name}
  backbone: debug # `debug` or `trinity` or `verl`


  model:
    # which model should be trained
    path: /path/to/model/such/as/Qwen/Qwen2___5-14B-Instruct

  data:
    # max number of tokens for prompt
    max_prompt_length: 3000
    # max number of tokens for response
    max_response_length: 15000
    # how many tasks per training batch
    train_batch_size: 32
    # [Hint]: The final number of samples per update will be: N_{sample} = (data.train_batch_size * rollout.num_repeat * rollout.multi_turn.expected_steps)


  rollout:

    # the path to the workflow class
    agentscope_workflow: tutorial.example_appworld.appworld->ExampleAgentScopeWorkflow

    # whether or not to disable all tool calls
    agentscope_disable_toolcalls: False

    # maximum number of parallel environments / simulate workers
    max_env_worker: 128

    # step reward gamma (experimental, do not change)
    gamma: 1.0

    # monitor LLM's abormal behaviors during rollout
    compute_madness_checklist:
      - "nonsense"
    # send signal to terminate context tracing when LLM is losing control
    agent_madness_termination: True # terminate_after_gone_mad
    # punish the LLM when it is detected as lost control
    agent_madness_reward: -1.0

    # max response length in one turn
    max_response_length_in_one_turn: 4096

    # max token length allowed for the model during rollout
    max_model_len: 18000

    multi_turn:
      # how many samples should be collected for each task run
      max_sample_per_task: 30
      # limit the maximum steps for each task
      max_steps: 30
      # the expected steps for each task, used to calculate the training batch size for trinity
      expected_steps: 1

    # TP size for rollout engine
    tensor_model_parallel_size: 1

    # the number of vllm engines, number of gpus for infer is `n_vllm_engine*tensor_model_parallel_size`, this argument is NOT effective when NOT using trinity
    n_vllm_engine: 2

    # how many sequences are allowed to be processed in parallel by each vllm engine
    max_num_seqs: 10

    # the usage of infer engine, options: (vllm, sglang)
    name: vllm

    # how many times a task should be repeated
    num_repeat: 4

    # rollout kwargs
    temperature: 0.9
    top_p: 1.0

    # validation kwargs
    val_kwargs:
      temperature: 0.0
      top_k: -1
      top_p: 1.0
      do_sample: False
      num_repeat: 1


  task_reader:
    type: env_service # `env_service` or `jsonl_dataset_file` or `huggingface_dat_repo` or `data_generation` or `random_dummy`
    # when `type == jsonl_dataset_file`
    jsonl_dataset_file:
      training:
        file_path: "/path/to/training/data.jsonl"
      validation:
        file_path: "/path/to/validation/data.jsonl"
    # when `type == env_service`
    env_service:
      env_type: "appworld"
      env_url: "http://127.0.0.1:8080"
      env_action_preference: code # code, text, box
      training_split: train
      validation_split: dev
    # when `type == huggingface_dat_repo`
    huggingface_dat_repo:
      dataset_path: "gsm8k"
      training_split: "train"
      validation_split: "validation"
    # when `type == data_generation`
    data_generation:
      document_reader:
        document_path:
          - 'dataset/document/your-document1.pdf'
          - 'dataset/document/your-document2.pdf'
        languages:
          - eng
        chunk_size: 5120
        split_by: "sentence"
        cache_enabled: true
      query_reader:
        type: jsonl_dataset_file
        jsonl_dataset_file:
          training:
            file_path: 'dataset/jsonl/your-queries.jsonl'
      task_num: 10
      llm_model: qwen-long
      llm_response_length: 8192
      num_workers: 32
      sampling_params:
        temperature: 0
      deduplication_filter:
        enabled: true
        params:
          similarity_threshold: 0.8
          db_path: ./.similarity_db
          model: text-embedding-v4
          api_key: null # load from the env
          base_url: https://dashscope.aliyuncs.com/compatible-mode/v1


  task_judge:
    judge_type: customized_protocol  # Options: 'customized_protocol', 'rubrics_auto_grader'

    # when `judge_type == customized_protocol`
    judge_protocol: astuner.task_judge.env_service_as_judge->EnvServiceJudge

    # the helper LLM model used for LLM-AS-Judge
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512

    # when `judge_type == rubrics_auto_grader`
    rubrics_auto_grader:
      model_name: qwen-max
      grader_mode: pointwise
      language: en
      query_specific_generate_number: 1
      enable_categorization: false
      categories_number: 5
      grader_name: "auto_grader"
      query_field: main_query
      answer_field: final_answer
      reference_field: answer
      custom_evaluation_prompt: null # dict or PromptTemplate or None
      input_data_type: jsonl_dataset_file # `env_service` or `jsonl_dataset_file` or `huggingface_dat_repo`
      jsonl_dataset_file:
        training:
          file_path: "tutorial/example_rm_auto_grader/rubrics_train.jsonl"
      # Pointwise mode settings
      min_score: 0
      max_score: 1


  # when backbone is `debug`, debug related configurations
  debug:
    debug_max_parallel: 16
    debug_first_n_tasks: 2
    debug_vllm_port: 18000
    debug_vllm_seed: 12345
    debug_tensor_parallel_size: 4


  # trainer common configurations
  trainer_common:
    val_before_train: False
    val_pass_n: 4
    save_freq: 20
    test_freq: 20
    total_epochs: 50
    nnodes: 1
    n_gpus_per_node: 8
    logger: swanlab
    algorithm:
      adv_estimator: grpo
      use_kl_in_reward: False
    mini_batch_num: 1
    fsdp_config:
      param_offload: True
      optimizer_offload: True
    optim:
      lr: 1e-6
    use_kl_loss: True
    kl_loss_coef: 0.002
    kl_loss_type: low_var_kl
    ulysses_sequence_parallel_size: 1
    checkpoint_base_dir: ./saved_checkpoints


  # context tracker protocol is valid ONLY when `use_agentscope_protocol=False`
  context_tracker:
    context_tracker_type: "linear"
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512
    max_env_len: 4096


  # DO NOT EDIT, FOR ROBOT TESTING PURPOSE ONLY. NOT FOR HUMAN.
  execute_test: False        # DO NOT EDIT, FOR ROBOT TESTING PURPOSE ONLY. NOT FOR HUMAN.
  execute_testing_lambda: "" # DO NOT EDIT, FOR ROBOT TESTING PURPOSE ONLY. NOT FOR HUMAN.
