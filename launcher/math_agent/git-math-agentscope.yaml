# ------------------ 选择backbone ------------------
backbone:
  type: trinity
  backbone_config:
    trinity: astune/default_config/trinity_default.yaml
    verl: astune/default_config/astune_default.yaml

# ------------------ 主要配置 ------------------
astune:
  project_name: appworld_astune

  task_reader:
    type: huggingface_dat_repo # ✨✨✨✨ `env_service` or `dataset_file` or `huggingface_dat_repo`
    # 如果选择 `env_service` 以下配置生效
    env_service:
      env_type: "appworld"
      env_url: "http://127.0.0.1:8080"
      env_action_preference: code # code, text, box
      training_split: train
      validation_split: dev
    # 如果选择 `dataset_file` 以下配置生效
    dataset_file:
      training:
        file_path: "xxxx.jsonl"
      validation:
        file_path: "xxxx.jsonl"
    # 如果选择 `huggingface_dat_repo` 以下配置生效
    huggingface_dat_repo:
      dataset_path: '/mnt/data_cpfs/qingxu.fu/dataset/openai/gsm8k/main'
      training_split: "train"
      validation_split: "test"

  task_judge:
    # ✨✨✨✨ 编写并选择评价函数
    judge_protocol: astune.task_judge.math_answer_as_judge->MathAnswerAndLlmAsJudge

  # 实验名称：建议不修改，直接读取yaml文件名称
  experiment_name: "read_yaml_name"

  model:
    # ✨✨✨✨ 设置待训练的模型
    path: /mnt/data/model_cache/modelscope/hub/Qwen/Qwen/Qwen2___5-14B-Instruct

  data:
    max_prompt_length: 3000
    max_response_length: 15000
    train_batch_size: 32

  rollout:
    use_agentscope_protocol: True
    agentscope_learn_protocol: tutorial.math_agent->ExampleMathLearn # ✨✨✨✨ 编写并选择Agent
    max_env_worker: 128
    use_step_reward_from_env: False
    binary_reward: False
    force_no_think: False
    force_think: False
    mode: async
    compute_madness_checklist:
      - "nonsense"
    gamma: 1.0
    agent_madness_termination: True # terminate_after_gone_mad
    agent_madness_reward: -1.0  # customize the reward when agent is detected as gone mad
    add_special_success_reward: False
    temperature: 0.9
    top_p: 1.0
    max_env_len: 4096
    max_response_length_in_one_turn: 4096
    max_model_len: 18000
    multi_turn:
      max_sample_per_task: 30
      max_steps: 30
    step_skip_action: 0 # skip action generation every N steps, 0 means never skip
    submit_oversample_multiplier: 1.5
    enable_oversample: True
    num_repeat: 4
    name: vllm
    val_kwargs:
      temperature: 0.0
      top_k: -1
      top_p: 1.0
      do_sample: False

  context_manager: # context manager protocol is used ONLY when `use_agentscope_protocol=False`
    context_manager_type: "linear"
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512
    auto_context_cm:
      train_sp_action: False
      token_num_trigger_clip: 8000
    sliding_window_cm:
      enable_llm_memory_extraction: False
    linear_think_cm:
      remove_think_before_submit_as_action: False
      extract_box_before_submit_as_action: False
      train_history_infer_token: True

  debug:
    debug_max_parallel: 1
    debug_first_n_tasks: 1
    debug_vllm_port: 18000
    debug_vllm_seed: 12345
    debug_tensor_parallel_size: 4


# ------------------ 修改trinity训练参数，如果使用verl则忽略该部分 ------------------
trinity:  # 修改trinity训练参数，如果使用verl则忽略该部分
  algorithm:
    algorithm_type: multi_step_grpo
    optimizer:
      lr: 1e-6
    repeat_times: 6
  buffer:
    batch_size: 8
    explorer_input:
      eval_tasksets: []
      taskset:
        default_workflow_type: astune_workflow
        format:
          prompt_key: question
          response_key: answer
        name: gsm8k
        path: http://localhost:8080
        rollout_args:
          temperature: 1.0
        split: train
        storage_type: astune
        subset_name: appworld
    total_epochs: 1000
    train_batch_size: 36
    trainer_input:
      experience_buffer:
        max_read_timeout: 18000
        name: agentscope_gsm8k_buffer
        storage_type: queue
  checkpoint_root_dir: ./trinity_checkpoints
  cluster:
    gpu_per_node: 8
    node_num: 1
  explorer:
    eval_interval: 999999
    max_repeat_times_per_runner: 1
    max_timeout: 7200
    rollout_model:
      dtype: bfloat16
      enable_auto_tool_choice: true
      enable_history: true
      enable_openai_api: true
      enable_prefix_caching: false
      enable_thinking: false
      enforce_eager: true
      engine_num: 2
      seed: 42
      tensor_parallel_size: 1
      tool_call_parser: hermes
    runner_per_model: 12
  model:
    max_model_len: 21000
    max_response_tokens: 16000
  monitor:
    monitor_type: swanlab
  synchronizer:
    sync_interval: 2
    sync_method: nccl
    sync_style: dynamic_by_explorer
    sync_timeout: 1200
  trainer:
    grad_clip: 1.0
    max_token_len_per_gpu: 24576
    save_interval: 100
    ulysses_sequence_parallel_size: 2
    use_dynamic_bsz: true


# ------------------ 不需要修改 ------------------
hydra:
  searchpath:
    - file://external/verl/verl/trainer/config  # verl only
    - file://astune/default_config

# ------------------ 不需要修改 ------------------
defaults:
  - ppo_trainer
  - default
  - _self_


# ------------------ 修改verl训练参数，如果使用trinity则忽略该部分 ------------------
# trainer:
#   val_before_train: False
#   n_gpus_per_node: 8
#   nnodes: 1
#   save_freq: 20
#   test_freq: 20
#   total_epochs: 50
#   logger:
#     - console
#     - swanlab
#   experiment_name: "read_yaml_name"
#   project_name: appworldnew
#   validation_data_dir: "experiments/exp_default/validation_log"
#   rollout_data_dir: "experiments/exp_default/rollout_log"
#   critic_warmup: 0
#   eval_pass_n: 4

# env_service:
#   env_url: http://localhost:8080
#   env_type: appworld
#   env_action_preference: code

# algorithm:
#   adv_estimator: grpo
#   use_kl_in_reward: False

# data:
#   train_batch_size: 32
#   max_prompt_length: 3000
#   max_response_length: 15000
#   return_raw_chat: True
#   fast_eval: True  # 是否裁剪测试集
#   train_files: train(read_from_env_service)
#   val_files: val(read_from_env_service)

# actor_rollout_ref:
#   actor:
#     override_ppo_mini_batch_num: 1
#     ppo_epochs: 1
#     ppo_mini_batch_size: 16
#     optim:
#       lr: 1e-6
#     use_kl_loss: True
#     kl_loss_coef: 0.002
#     kl_loss_type: low_var_kl
#     ppo_micro_batch_size_per_gpu: 1
#     ppo_max_token_len_per_gpu: 18000
#     use_dynamic_bsz: True
#     # ulysses_sequence_parallel_size: 1
#     fsdp_config:
#       param_offload: True
#       optimizer_offload: True
#   rollout:
#     max_num_seqs: 128
#     debug_llm_io: False
#     name: vllm
#     mode: async
#     max_env_len: 3000     # 裁剪环境反馈
#     response_length: 3000
#     prompt_length: 15000
#     max_model_len: 18000
#     log_prob_max_token_len_per_gpu: 18000
#     temperature: 0.9
#     top_p: 1.0
#     val_kwargs:
#       top_k: -1
#       top_p: 1.0
#       temperature: 0
#     gamma: 1.0
#     max_env_worker: 256
#     enforce_eager: True
#     force_no_think: False
#     force_think: True
#     context_manager_type: linear
#     # train_sp_action: False
#     gpu_memory_utilization: 0.85
#     log_prob_micro_batch_size_per_gpu: 2
#     tensor_model_parallel_size: 4
#     enable_oversample: True
#     submit_oversample_multiplier: 2
#     add_special_success_reward: True
#     train_history_infer_token: False
#     use_agentscope_protocol: True
#     binary_reward: False
#     terminate_after_gone_mad: True
#     agent_madness_reward: -1.0
#     step_skip_action: 0
#     use_step_reward_from_env: False
#     agentscope_learn_protocol: astune.agentscope_flow_example->ExampleAgentScopeLearnProtocol
#     compute_madness_checklist:
#       - "nonsense"
#   ref:
#     use_dynamic_bsz: True
#     log_prob_micro_batch_size_per_gpu: 4
#     log_prob_max_token_len_per_gpu: 18000
#     fsdp_config:
#       param_offload: True

#   model:
#     path: /mnt/data/model_cache/modelscope/hub/Qwen/Qwen/Qwen2___5-14B-Instruct
#     use_remove_padding: True
#     use_qwen3: False
#     enable_gradient_checkpointing: True


# critic:
#   ppo_max_token_len_per_gpu: 18000
#   forward_max_token_len_per_gpu: 18000
