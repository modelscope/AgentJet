actor_rollout_ref:
  actor:
    entropy_coeff: 0
    fsdp_config:
      optimizer_offload: true
      param_offload: true
    kl_loss_coef: 0.002
    kl_loss_type: low_var_kl
    loss_agg_mode: seq-mean-token-mean
    optim:
      lr: 1.0e-06
    override_ppo_mini_batch_num: 1
    ppo_epochs: 1
    ppo_max_token_len_per_gpu: 18000
    ppo_micro_batch_size_per_gpu: 1
    ppo_mini_batch_size: 16
    use_dynamic_bsz: true
    use_kl_loss: true
  hybrid_engine: true
  model:
    enable_gradient_checkpointing: true
    use_remove_padding: true
  ref:
    fsdp_config:
      param_offload: true
    log_prob_max_token_len_per_gpu: 18000
    log_prob_micro_batch_size_per_gpu: 4
    use_dynamic_bsz: true
  rollout:
    custom_dataflow_cls:
      name: ''
      path: ''
    enable_oversample: false
    enforce_eager: true
    gamma: 1.0
    gpu_memory_utilization: 0.9
    log_prob_max_token_len_per_gpu: 18000
    log_prob_micro_batch_size_per_gpu: 4
    max_env_worker: 64
    max_num_seqs: 10
    mode: async
    name: vllm
    ppo_micro_batch_size_per_gpu: 1
    step_skip_action: 0
    submit_oversample_multiplier: 1.5
    temperature: 0.9
    tensor_model_parallel_size: 1
    top_p: 1.0
algorithm:
  adv_estimator: grpo
  task_norm_patch: false
  use_kl_in_reward: false
astuner:
  backbone: debug
  context_tracker:
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512
    context_tracker_type: linear
    max_env_len: 4096
  data:
    max_prompt_length: 3000
    max_response_length: 15000
    train_batch_size: 120
  debug:
    debug_first_n_tasks: 1
    debug_max_parallel: 1
    debug_tensor_parallel_size: 4
    debug_vllm_port: 18000
    debug_vllm_seed: 12345
  execute_test: false
  execute_testing_lambda: ''
  experiment_dir: tests/temp/sample
  experiment_name: sample
  model:
    path: ''
  project_name: unittest
  rollout:
    agent_madness_reward: -1.0
    agent_madness_termination: true
    agentscope_disable_toolcalls: false
    agentscope_learn_protocol: tutorial.example_math_agent.math_agent->ExampleMathLearn
    compute_madness_checklist:
    - nonsense
    - wrong_toolcall
    gamma: 1.0
    max_env_worker: 256
    max_model_len: 18000
    max_num_seqs: 256
    max_response_length_in_one_turn: 4096
    multi_turn:
      expected_steps: 1
      max_sample_per_task: 4
      max_steps: 30
    n_vllm_engine: 2
    name: vllm
    num_repeat: 8
    temperature: 0.7
    tensor_model_parallel_size: 4
    top_p: 1.0
    use_agentscope_protocol: true
    val_kwargs:
      do_sample: false
      num_repeat: 1
      temperature: 0.0
      top_k: -1
      top_p: 1.0
  task_judge:
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512
    judge_protocol: astuner.task_judge.math_answer_as_judge->MathAnswerAsJudge
    judge_type: customized_protocol
    rubrics_auto_grader:
      aggregation_mode: keep_all
      answer_field: final_answer
      dataset_file:
        training:
          file_path: tutorial/example_rm_auto_grader/rubrics_train.jsonl
      generate_number: 1
      grader_mode: pointwise
      grader_name: auto_grader
      input_data_type: dataset_file
      language: en
      max_epochs: 2
      max_retries: 3
      max_score: 1
      min_score: 0
      model_name: qwen-max
      num_reference_samples: 20
      query_field: main_query
      reference_field: answer
      sampling_mode: all_samples
      success_threshold: 0.7
  task_reader:
    dataset_file:
      training:
        file_path: /path/to/training/data.jsonl
      validation:
        file_path: /path/to/validation/data.jsonl
    env_service:
      env_action_preference: code
      env_type: appworld
      env_url: http://127.0.0.1:8080
      training_split: train
      validation_split: dev
    huggingface_dat_repo:
      dataset_path: ''
      training_split: train
      validation_split: test
    type: huggingface_dat_repo
  trainer_common:
    algorithm:
      adv_estimator: grpo
      task_norm_patch: false
      use_kl_in_reward: false
    fsdp_config:
      optimizer_offload: true
      param_offload: true
    kl_loss_coef: 0.002
    kl_loss_type: low_var_kl
    logger:
    - console
    - swanlab
    mini_batch_num: 1
    n_gpus_per_node: 8
    nnodes: 1
    optim:
      lr: 1.0e-06
    save_freq: 99999
    test_freq: 99999
    total_epochs: 99999
    ulysses_sequence_parallel_size: 1
    use_kl_loss: true
    val_before_train: false
    val_pass_n: 4
data:
  filter_overlong_prompts: true
  return_raw_chat: true
  truncation: error
  val_batch_size: 100000000000
defaults:
- verl_default
- astune_default
- _self_
hydra:
  searchpath:
  - file://astuner/default_config
  - file://astuner/default_config/verl
trainer:
  critic_warmup: 0
  experiment_name: read_yaml_name
  hfmodelpath: ''
  logger:
  - console
  - swanlab
  n_gpus_per_node: 8
  nnodes: 1
  project_name: astune_default_project
  save_freq: 20
  test_freq: 20
  total_epochs: 50
  val_before_train: false
  val_pass_n: 4
