{"config":{"indexing":"full","lang":["en","zh"],"min_search_length":2,"prebuild_index":true,"separator":"[\\s\\-\\.\\(\\)\\/]+"},"docs":[{"location":"","text":"AgentJet (AJet) is a cutting-edge, user-friendly agent tuning framework designed to optimize LLM models and agent workflows. \ud83d\udee9\ufe0f Key Features Get Started with Ease AgentJet simplifies the process of tuning the models that power your agent workflows. It supports nearly all major agent frameworks (e.g. agentscope , langchain ), as well as framwork-less agents built from HTTP requests. Rich Tutorial Library Rich examples as beginner's tutorial: math agent , werewolves rpg , appworld ... All with step-by-step guides. Covering various agentic frameworks. Reliable and Reproducible Checkout AgentJet's community-powered, robot-assisted open-benchmarking system . Share progress, compare training backbones, discover bugs and iterate faster than ever! Click here to see AgentJet performance across tasks/versions/backbones. Multi-agent and Multi-turn Built to support advanced multi-agent and multi-turn LLM workflows, AgentJet intergrates timeline-merging algorithms that automatically analyze and consolidate each agent's LLM timeline, accelerating training speed 1.5x ~ 10x. High Resolution Logging Log token-level rollout details, capturing token IDs, token loss masks , and token log probabilities with web UI display . This Support workflow development, agent diagnostics, and facilitate research on advanced LLM algorithm studies. Any Training Engine Support multiple training engines as backbone ( VeRL and Trinity-RFT ). Tinker backbone support will be released soon. Choose from vLLM and SGLang as you wish. Say goodbye to training engine gaps. \ud83d\udee9\ufe0f Quick Start Click Here for the Full Installation Document We recommend using uv for dependency management. Click here for details and other training backbone (e.g. Trinity-RFT) options. Clone the Repository: git clone https://github.com/modelscope/AgentJet.git cd AgentJet Set up Environment: uv venv --python = 3 .10.16 && source .venv/bin/activate uv pip install -e . [ verl ] # Note: flash-attn must be installed after other dependencies uv pip install flash_attn == 2 .8.3 --no-build-isolation --no-cache-dir Train the First Agent: # You can start training your first agent with a single command using a pre-configured YAML file ajet --conf tutorial/example_math_agent/math_agent.yaml \ud83d\udee9\ufe0f Example Library Explore our rich library of examples to kickstart your journey: Math Agent Training a math agent that can write Python code to solve mathematical problems. AppWorld Agent Creating an AppWorld agent using AgentScope and training it for real-world tasks. Werewolves Game Developing Werewolves RPG agents and training them for strategic gameplay. Learning to Ask Learning to ask questions like a doctor for medical consultation scenarios. Countdown Game Writing a countdown game using AgentScope and solving it with RL. Frozen Lake Solving a frozen lake walking puzzle using AgentJet's reinforcement learning. \ud83d\udee9\ufe0f Core Concepts AgentJet makes agent fine-tuning straightforward by separating the developer interface from the internal execution logic. \ud83d\udee9\ufe0f The User-Centric Interface To optimize an agent, you provide three core inputs: Trainable Workflow Define your agent logic by inheriting the Workflow class, supporting both simple and multi-agent setups. Task Reader Load training tasks from JSONL files, HuggingFace datasets, or auto-generate from documents. Task Judger Evaluates agent outputs and assigns rewards to guide the training process. \ud83d\udee9\ufe0f Internal System Architecture The internal system orchestrates several specialized modules to handle the complexities of RL training and agent interactions. Module Description Launcher Manages background service processes (Ray, vLLM) and routes the backbone Task Rollout Bridges LLM engines and manages the Gym environment lifecycle Task Runner Executes the AgentScope workflow and calculates rewards Model Tuner Forwards inference requests from the workflow to the LLM engine Context Tracker Monitors LLM calls and automatically merges shared-history timelines (1.5x-10x efficiency boost) \ud83d\udee9\ufe0f Next Steps Installation Set up AgentJet environment and dependencies. Quick Start Run your first training in minutes. First Agent Build and train your own agent from scratch. Examples Explore detailed training examples.","title":"Overview"},{"location":"#key-features","text":"","title":"\ud83d\udee9\ufe0f Key Features"},{"location":"#quick-start","text":"","title":"\ud83d\udee9\ufe0f Quick Start"},{"location":"#example-library","text":"Explore our rich library of examples to kickstart your journey:","title":"\ud83d\udee9\ufe0f Example Library"},{"location":"#core-concepts","text":"AgentJet makes agent fine-tuning straightforward by separating the developer interface from the internal execution logic. \ud83d\udee9\ufe0f The User-Centric Interface To optimize an agent, you provide three core inputs:","title":"\ud83d\udee9\ufe0f Core Concepts"},{"location":"#next-steps","text":"","title":"\ud83d\udee9\ufe0f Next Steps"},{"location":"en/agent_framework_support/","text":"AgentJet currently supports (and has been tested with) the following agentic frameworks. AgentScope Agent-Oriented Programming for Building LLM Applications. LangChain LangChain provides the engineering platform and open source frameworks developers use to build, test, and deploy reliable AI agents. OpenAI SDK The OpenAI Agents SDK allows you to build agentic AI applications in a lightweight and easy-to-use package with minimal abstractions. By the way, both vLLM and SGLang offer compatible services. Raw HTTP Why use the Agent SDKs and all these abstractions? If you want to take control of the foundation of LLM Agents, in this AI era, you can always start from scratch and build your own \"high-scrapers\". More Agentic Frameworks We are still testing all other agentic frameworks. Meanwhile, \"Raw HTTP\" methods can provide `base_url` and `api_key` as AgentJet endpoint, which theoretically can support most agentic frameworks.","title":"Agentic Frameworks"},{"location":"en/beast_logger/","text":"Beast-logger is a logging kit built for LLM systems, providing reliable high-resolution token-level LLM activity that is unprecedented in any other projects. Here is how to use beast-logger in AgentJet. Usage in AgentJet Start training or debugging with AgentJet launcher. Wait until the first batch is completed. Locate log files. By default, they will be placed at saved_experiments/${experiment_name} . For example: saved_experiments/benchmark_frozenlake_20251223_2305 Run beast_logger_go command in the VSCode terminal (or any other software with port-forwarding ability) to start the web log-viewer. Click http://127.0.0.1:8181 to open it (VSCode will automatically forward this port from server -> your local computer) Fill the ABSOLUTE path of the log files and click submit . Hint: absolute path is recommended. However, you can also use relative path, if beast_logger_go command is launched at same working dir. Warning: Beast-logger recursively scans this path, thus, where possible, selects the innermost directory containing the fewest files to read logs faster. Choose entry to display Yellow tokens: tokens that are excluded from loss computation. Blue tokens: tokens that participant loss computation. Hovor your mouse on one of the tokens: show the logprob value of that token.","title":"Beast Logger"},{"location":"en/beast_logger/#usage-in-agentjet","text":"Start training or debugging with AgentJet launcher. Wait until the first batch is completed. Locate log files. By default, they will be placed at saved_experiments/${experiment_name} . For example: saved_experiments/benchmark_frozenlake_20251223_2305 Run beast_logger_go command in the VSCode terminal (or any other software with port-forwarding ability) to start the web log-viewer. Click http://127.0.0.1:8181 to open it (VSCode will automatically forward this port from server -> your local computer) Fill the ABSOLUTE path of the log files and click submit . Hint: absolute path is recommended. However, you can also use relative path, if beast_logger_go command is launched at same working dir. Warning: Beast-logger recursively scans this path, thus, where possible, selects the innermost directory containing the fewest files to read logs faster. Choose entry to display Yellow tokens: tokens that are excluded from loss computation. Blue tokens: tokens that participant loss computation. Hovor your mouse on one of the tokens: show the logprob value of that token.","title":"Usage in AgentJet"},{"location":"en/component/","text":"","title":"Component"},{"location":"en/configuration/","text":"This page provides a detailed description of the configuration files for AgentJet. Overview AgentJet uses YAML-format configuration files to set up data, algorithms, rewards, logging, and other runtime behaviors. Default Configuration The default config is located at ajet/default_config/ajet_default.yaml . At a high level, a typical config contains a single root section ajet , which is divided into several logical parts: Basic Metadata \u2014 Project name, experiment name, experiment directory, and backbone selection project_name , experiment_name , experiment_dir backbone : Select training backend ( debug , trinity , or verl ) Data & Reward \u2014 How to load data and evaluate agents task_reader : Load training/validation samples task_judge : Evaluate agents and compute rewards data : Prompt/response length and batch sizes Model & Rollout \u2014 Model configuration and agent interaction model : Base model to train rollout : Agent-environment interaction settings context_tracker : Conversation/history management Model Configuration Specifying the Model config.yaml ajet : model : path : path/to/model Source Type Example Local file /mnt/data/models/Qwen2.5-14B-Instruct HuggingFace repo Qwen/Qwen2.5-14B-Instruct (auto-downloaded) Environment Variables for LLM-as-Judge If using LLM-as-a-Judge, configure these environment variables: # DashScope API key for remote LLM calling export DASHSCOPE_API_KEY = 'sk-xxxxxx|sk-yyyyyy' export DASHSCOPE_API_KEY_BACKUP = 'sk-zzzzzz' Data Configuration Task Reader task_reader defines how to read training and validation data. EnvService JSONL File HuggingFace ajet : task_reader : type : env_service env_service : env_type : \"appworld\" env_url : \"http://127.0.0.1:8080\" env_action_preference : code training_split : train validation_split : dev ajet : task_reader : type : jsonl_dataset_file jsonl_dataset_file : training : file_path : \"data/train.jsonl\" validation : file_path : \"data/val.jsonl\" ajet : task_reader : type : huggingface_dat_repo huggingface_dat_repo : dataset_path : \"gsm8k\" training_split : \"train\" validation_split : \"validation\" Task Judge task_judge evaluates agent performance and calculates rewards. config.yaml ajet : task_judge : judge_type : customized_protocol # or 'rubrics_auto_grader' judge_protocol : ajet.task_judge.env_service_as_judge->EnvServiceJudge alien_llm_model : qwen3-235b-a22b-instruct-2507 alien_llm_response_length : 512 Option Description customized_protocol Use a custom Python class for scoring rubrics_auto_grader Use LLM-based automatic grading Training Configuration Backend Selection AgentJet supports three training backends: Backend Description trinity Default. Flexible and scalable framework for RL fine-tuning verl Volcano Engine reinforcement learning for LLMs debug Allows breakpoint debugging in IDEs config.yaml ajet : backbone : trinity # debug, trinity, or verl Rollout Configuration Controls agent behavior during environment interaction: config.yaml ajet : rollout : user_workflow : tutorial.example_appworld.appworld->ExampleAgentScopeWorkflow max_env_worker : 128 temperature : 0.9 top_p : 1.0 name : vllm n_vllm_engine : 2 num_repeat : 4 Parameter Description user_workflow Path to workflow implementation class temperature / top_p Sampling parameters name Inference engine (e.g., vllm ) n_vllm_engine Number of vLLM engines (Trinity only) Common Training Parameters config.yaml ajet : trainer_common : total_epochs : 50 save_freq : 20 test_freq : 20 val_before_train : False val_pass_n : 4 nnodes : 1 n_gpus_per_node : 8 mini_batch_num : 1 fsdp_config : param_offload : True optimizer_offload : True Parameter Description total_epochs Total training epochs save_freq Checkpoint save frequency (steps) test_freq Validation frequency (steps) nnodes / n_gpus_per_node Distributed training setup fsdp_config FSDP memory optimization Optimization Algorithms config.yaml ajet : trainer_common : algorithm : adv_estimator : grpo use_kl_in_reward : False optim : lr : 1e-6 use_kl_loss : True kl_loss_coef : 0.002 kl_loss_type : low_var_kl Parameter Description adv_estimator Advantage estimator (e.g., grpo ) lr Learning rate use_kl_loss Include KL divergence in loss kl_loss_coef KL loss coefficient Debug Mode When backbone: debug , additional settings are available: config.yaml ajet : debug : debug_max_parallel : 16 debug_first_n_tasks : 2 debug_vllm_port : 18000 debug_vllm_seed : 12345 debug_tensor_parallel_size : 4 Debug Mode Use Cases Limiting tasks : Quickly verify the pipeline on a few tasks Fixing randomness : debug_vllm_seed helps reproduce issues Reduced parallelism : Easier to debug with smaller concurrency Logging & Monitoring Logger Selection config.yaml ajet : trainer_common : logger : swanlab # console, wandb, or swanlab Logger Description console Standard output for quick progress checking wandb Weights & Biases experiment tracking swanlab SwanLab logging Output Structure All experiment outputs are saved in ./launcher_record/{experiment_name} : Directory Contents Logs Logs and error messages Metrics Training metrics (depends on logger) Checkpoint Model checkpoints Next Steps Math Agent See all configurations applied in a real training example. Visualization Monitor and visualize your training progress.","title":"Configuration"},{"location":"en/configuration/#overview","text":"AgentJet uses YAML-format configuration files to set up data, algorithms, rewards, logging, and other runtime behaviors. Default Configuration The default config is located at ajet/default_config/ajet_default.yaml . At a high level, a typical config contains a single root section ajet , which is divided into several logical parts: Basic Metadata \u2014 Project name, experiment name, experiment directory, and backbone selection project_name , experiment_name , experiment_dir backbone : Select training backend ( debug , trinity , or verl ) Data & Reward \u2014 How to load data and evaluate agents task_reader : Load training/validation samples task_judge : Evaluate agents and compute rewards data : Prompt/response length and batch sizes Model & Rollout \u2014 Model configuration and agent interaction model : Base model to train rollout : Agent-environment interaction settings context_tracker : Conversation/history management","title":"Overview"},{"location":"en/configuration/#model-configuration","text":"","title":"Model Configuration"},{"location":"en/configuration/#specifying-the-model","text":"config.yaml ajet : model : path : path/to/model Source Type Example Local file /mnt/data/models/Qwen2.5-14B-Instruct HuggingFace repo Qwen/Qwen2.5-14B-Instruct (auto-downloaded)","title":"Specifying the Model"},{"location":"en/configuration/#environment-variables-for-llm-as-judge","text":"If using LLM-as-a-Judge, configure these environment variables: # DashScope API key for remote LLM calling export DASHSCOPE_API_KEY = 'sk-xxxxxx|sk-yyyyyy' export DASHSCOPE_API_KEY_BACKUP = 'sk-zzzzzz'","title":"Environment Variables for LLM-as-Judge"},{"location":"en/configuration/#data-configuration","text":"","title":"Data Configuration"},{"location":"en/configuration/#task-reader","text":"task_reader defines how to read training and validation data. EnvService JSONL File HuggingFace ajet : task_reader : type : env_service env_service : env_type : \"appworld\" env_url : \"http://127.0.0.1:8080\" env_action_preference : code training_split : train validation_split : dev ajet : task_reader : type : jsonl_dataset_file jsonl_dataset_file : training : file_path : \"data/train.jsonl\" validation : file_path : \"data/val.jsonl\" ajet : task_reader : type : huggingface_dat_repo huggingface_dat_repo : dataset_path : \"gsm8k\" training_split : \"train\" validation_split : \"validation\"","title":"Task Reader"},{"location":"en/configuration/#task-judge","text":"task_judge evaluates agent performance and calculates rewards. config.yaml ajet : task_judge : judge_type : customized_protocol # or 'rubrics_auto_grader' judge_protocol : ajet.task_judge.env_service_as_judge->EnvServiceJudge alien_llm_model : qwen3-235b-a22b-instruct-2507 alien_llm_response_length : 512 Option Description customized_protocol Use a custom Python class for scoring rubrics_auto_grader Use LLM-based automatic grading","title":"Task Judge"},{"location":"en/configuration/#training-configuration","text":"","title":"Training Configuration"},{"location":"en/configuration/#backend-selection","text":"AgentJet supports three training backends: Backend Description trinity Default. Flexible and scalable framework for RL fine-tuning verl Volcano Engine reinforcement learning for LLMs debug Allows breakpoint debugging in IDEs config.yaml ajet : backbone : trinity # debug, trinity, or verl","title":"Backend Selection"},{"location":"en/configuration/#rollout-configuration","text":"Controls agent behavior during environment interaction: config.yaml ajet : rollout : user_workflow : tutorial.example_appworld.appworld->ExampleAgentScopeWorkflow max_env_worker : 128 temperature : 0.9 top_p : 1.0 name : vllm n_vllm_engine : 2 num_repeat : 4 Parameter Description user_workflow Path to workflow implementation class temperature / top_p Sampling parameters name Inference engine (e.g., vllm ) n_vllm_engine Number of vLLM engines (Trinity only)","title":"Rollout Configuration"},{"location":"en/configuration/#common-training-parameters","text":"config.yaml ajet : trainer_common : total_epochs : 50 save_freq : 20 test_freq : 20 val_before_train : False val_pass_n : 4 nnodes : 1 n_gpus_per_node : 8 mini_batch_num : 1 fsdp_config : param_offload : True optimizer_offload : True Parameter Description total_epochs Total training epochs save_freq Checkpoint save frequency (steps) test_freq Validation frequency (steps) nnodes / n_gpus_per_node Distributed training setup fsdp_config FSDP memory optimization","title":"Common Training Parameters"},{"location":"en/configuration/#optimization-algorithms","text":"config.yaml ajet : trainer_common : algorithm : adv_estimator : grpo use_kl_in_reward : False optim : lr : 1e-6 use_kl_loss : True kl_loss_coef : 0.002 kl_loss_type : low_var_kl Parameter Description adv_estimator Advantage estimator (e.g., grpo ) lr Learning rate use_kl_loss Include KL divergence in loss kl_loss_coef KL loss coefficient","title":"Optimization Algorithms"},{"location":"en/configuration/#debug-mode","text":"When backbone: debug , additional settings are available: config.yaml ajet : debug : debug_max_parallel : 16 debug_first_n_tasks : 2 debug_vllm_port : 18000 debug_vllm_seed : 12345 debug_tensor_parallel_size : 4 Debug Mode Use Cases Limiting tasks : Quickly verify the pipeline on a few tasks Fixing randomness : debug_vllm_seed helps reproduce issues Reduced parallelism : Easier to debug with smaller concurrency","title":"Debug Mode"},{"location":"en/configuration/#logging-monitoring","text":"","title":"Logging &amp; Monitoring"},{"location":"en/configuration/#logger-selection","text":"config.yaml ajet : trainer_common : logger : swanlab # console, wandb, or swanlab Logger Description console Standard output for quick progress checking wandb Weights & Biases experiment tracking swanlab SwanLab logging","title":"Logger Selection"},{"location":"en/configuration/#output-structure","text":"All experiment outputs are saved in ./launcher_record/{experiment_name} : Directory Contents Logs Logs and error messages Metrics Training metrics (depends on logger) Checkpoint Model checkpoints","title":"Output Structure"},{"location":"en/configuration/#next-steps","text":"","title":"Next Steps"},{"location":"en/data_generation/","text":"Introduction Data Generation Reader provides an intelligent data generation method designed to simplify the creation of high-quality training data. The method is flexible and efficient, capable of generating domain-specific tasks based on few-shot examples and optional documents. Method Data Generation Reader employs a two-stage task generation process: Stage 1 (Optional): Document-based Data Generation This stage is optional. Document-based Data Generation generates knowledge-based tasks based on the provided documents. Users can provide one or more documents (supporting formats like PDF, Word, TXT, etc.): According to the Anti-Money Laundering and Counter-Terrorist Financing Ordinance and related Guideline, banks are required to identify and take reasonable measures to verify the identity of the beneficial owner of corporate customers so that the bank is ... The generator reads the document content and guides the LLM to batch-generate tasks related to the document content: [ { \"main_query\" : \"What are the key requirements of Customer Due Diligence in AML procedures?\" , \"related_doc\" : \"Customer Due Diligence measures should include: (a) identifying the customer and verifying the customer's identity...\" }, { \"main_query\" : \"How should financial institutions handle Suspicious Transaction Reports?\" , \"related_doc\" : \"When someone knows or suspects that any property represents the proceeds of an indictable offense...\" } ... ] If documents are provided for data generation, the data generated in this stage will be added to the validation task set for the subsequent training process. Stage 2: Few-shot Data Generation This stage generates the final training tasks. Few-shot Data Generation combines a few user-provided tasks with the knowledge-based tasks generated in the first stage, and use the documents as references to generate training tasks. First, the user needs to provide a few task examples: { \"main_query\" : \"Can banks ask corporate customers to provide information of its ownership?\" , \"answer\" : \"According to the Anti-Money Laundering and ...\" } { \"main_query\" : \"Can a bank close my account?\" , \"answer\" : \"Either a customer or a bank may close an account at any time subject to any specific terms and ...\" } ... These examples will be merged with the tasks generated in the first stage to form an example task set. The generator will sample from this set to be used as few-shot demonstrations, and combined with relevant documents, guide the LLM to batch-generate training tasks: [ { \"main_query\" : \"Are financial institutions required to verify the source of funds for corporate clients during account opening?\" }, { \"main_query\" : \"What are the requirements for banks to verify customer identities under anti-money laundering regulations?\" } ... ] Quick Start Data Generation Reader can load a few user-provided tasks and optional documents (in various formats such as PDF, Word, and TXT) from a local path, then generates tasks and loads them as training tasks. Step 1: Prepare data Provide a few example tasks: { \"main_query\" : \"What is the capital of France?\" , \"answer\" : \"...\" } { \"main_query\" : \"How to cook pasta?\" , \"answer\" : \"...\" } (Optional) Provide documents and place them in the specified directory: mkdir -p dataset/document cp your-document.pdf dataset/document/ Step 2: Generate Training Tasks Method 1: Integrate Data Generation into the Training Pipeline Copy and modify the key configuration parameters in ajet/default_config/ajet_default.yaml , and set ajet.task_reader.type to data_generation to enable this reader. ajet : task_reader : type : data_generation # when `type == data_generation` data_generation : # Document reader configuration document_reader : document_path : - 'dataset/document/your-document1.pdf' - 'dataset/document/your-document2.pdf' languages : - eng # Task reader (for existing tasks) query_reader : type : jsonl_dataset_file jsonl_dataset_file : training : file_path : 'dataset/jsonl/your-queries.jsonl' # Number of tasks to generate task_num : 10 # LLM config llm_model : qwen-long llm_response_length : 8192 num_workers : 32 sampling_params : temperature : 0 # Task filtering config deduplication_filter : enabled : true params : similarity_threshold : 0.8 db_path : ./.similarity_db model : text-embedding-v4 api_key : null # load from the env base_url : https://dashscope.aliyuncs.com/compatible-mode/v1 Method 2: Run the Generation Script from ajet.data_generator.config import * from ajet.task_reader.data_generator_reader import DataGeneratorTaskReader def run (): config = TaskReaderConfig ( data_generation = DataGenerationConfig ( document_reader = DocumentReaderConfig ( document_path = [ 'dataset/document/your-document1.pdf' , 'dataset/document/your-document2.pdf' ], languages = [ \"eng\" ], chunk_size = 5120 , split_by = \"sentence\" , ), query_reader = QueryReaderConfig ( type = \"jsonl_dataset_file\" , jsonl_dataset_file = DatasetFileConfig ( training = TrainingDatasetConfig ( file_path = 'dataset/jsonl/your-queries.jsonl' ) ), ), task_num = 50 , llm_model = \"qwen-long\" , num_workers = 16 , sampling_params = SamplingParamsConfig ( temperature = 0.0 ), deduplication_filter = DeduplicationFilterConfig ( enabled = True , params = DeduplicationFilterParamsConfig ( similarity_threshold = 0.8 , model = \"text-embedding-v4\" , ), ), ) ) reader = DataGeneratorTaskReader ( reader_config = config ) run () Generated Task Examples Based on user-provided documents (optional) and a few task examples, the Data Generation Reader can batch-generate training tasks: [ { \"main_query\" : \"Are financial institutions required to verify the source of funds for corporate clients during account opening?\" }, { \"main_query\" : \"What are the requirements for banks to verify customer identities under anti-money laundering regulations?\" } ... ] Detailed Config Options Parameter Path Type Default Required Description document_reader.document_path list[str] - No List of document file paths. Supports PDF, Word, TXT, and more. document_reader.languages list[str] ['eng'] No List of document languages for OCR and text parsing, e.g., eng (English), chs (Simplified Chinese). query_reader.type str jsonl_dataset_file Yes Reader type. Options: jsonl_dataset_file , env_service , huggingface_dat_repo . query_reader.jsonl_dataset_file.training.file_path str - Yes Path to the training tasks JSONL file (when type: jsonl_dataset_file ). task_num int 10 Yes Number of tasks to generate. The actual number may be reduced by filtering. llm_model str qwen-long Yes LLM model name used for task generation. llm_response_length int 8192 No Maximum number of tokens in the LLM response. num_workers int 32 No Number of parallel worker threads for speeding up task generation. sampling_params.temperature float 0 No Sampling temperature. 0 means greedy decoding (deterministic output); higher values make outputs more random. deduplication_filter.enabled bool true No Whether to enable the deduplication filter. deduplication_filter.params.similarity_threshold float 0.8 Yes Similarity threshold (0\u20131). Tasks above this threshold will be filtered out. deduplication_filter.params.db_path str ./.similarity_db No Path to the similarity database used to cache embeddings. deduplication_filter.params.model str text-embedding-v4 Yes Embedding model used to compute similarity. deduplication_filter.params.api_key str null No API key. If null , it will be loaded from the DASHSCOPE_API_KEY environment variable. deduplication_filter.params.base_url str https://dashscope.aliyuncs.com/compatible-mode/v1 No Base URL for the embedding API.","title":"Data Generation"},{"location":"en/data_generation/#introduction","text":"Data Generation Reader provides an intelligent data generation method designed to simplify the creation of high-quality training data. The method is flexible and efficient, capable of generating domain-specific tasks based on few-shot examples and optional documents.","title":"Introduction"},{"location":"en/data_generation/#method","text":"Data Generation Reader employs a two-stage task generation process:","title":"Method"},{"location":"en/data_generation/#stage-1-optional-document-based-data-generation","text":"This stage is optional. Document-based Data Generation generates knowledge-based tasks based on the provided documents. Users can provide one or more documents (supporting formats like PDF, Word, TXT, etc.): According to the Anti-Money Laundering and Counter-Terrorist Financing Ordinance and related Guideline, banks are required to identify and take reasonable measures to verify the identity of the beneficial owner of corporate customers so that the bank is ... The generator reads the document content and guides the LLM to batch-generate tasks related to the document content: [ { \"main_query\" : \"What are the key requirements of Customer Due Diligence in AML procedures?\" , \"related_doc\" : \"Customer Due Diligence measures should include: (a) identifying the customer and verifying the customer's identity...\" }, { \"main_query\" : \"How should financial institutions handle Suspicious Transaction Reports?\" , \"related_doc\" : \"When someone knows or suspects that any property represents the proceeds of an indictable offense...\" } ... ] If documents are provided for data generation, the data generated in this stage will be added to the validation task set for the subsequent training process.","title":"Stage 1 (Optional): Document-based Data Generation"},{"location":"en/data_generation/#stage-2-few-shot-data-generation","text":"This stage generates the final training tasks. Few-shot Data Generation combines a few user-provided tasks with the knowledge-based tasks generated in the first stage, and use the documents as references to generate training tasks. First, the user needs to provide a few task examples: { \"main_query\" : \"Can banks ask corporate customers to provide information of its ownership?\" , \"answer\" : \"According to the Anti-Money Laundering and ...\" } { \"main_query\" : \"Can a bank close my account?\" , \"answer\" : \"Either a customer or a bank may close an account at any time subject to any specific terms and ...\" } ... These examples will be merged with the tasks generated in the first stage to form an example task set. The generator will sample from this set to be used as few-shot demonstrations, and combined with relevant documents, guide the LLM to batch-generate training tasks: [ { \"main_query\" : \"Are financial institutions required to verify the source of funds for corporate clients during account opening?\" }, { \"main_query\" : \"What are the requirements for banks to verify customer identities under anti-money laundering regulations?\" } ... ]","title":"Stage 2: Few-shot Data Generation"},{"location":"en/data_generation/#quick-start","text":"Data Generation Reader can load a few user-provided tasks and optional documents (in various formats such as PDF, Word, and TXT) from a local path, then generates tasks and loads them as training tasks.","title":"Quick Start"},{"location":"en/data_generation/#step-1-prepare-data","text":"Provide a few example tasks: { \"main_query\" : \"What is the capital of France?\" , \"answer\" : \"...\" } { \"main_query\" : \"How to cook pasta?\" , \"answer\" : \"...\" } (Optional) Provide documents and place them in the specified directory: mkdir -p dataset/document cp your-document.pdf dataset/document/","title":"Step 1: Prepare data"},{"location":"en/data_generation/#step-2-generate-training-tasks","text":"","title":"Step 2: Generate Training Tasks"},{"location":"en/data_generation/#method-1-integrate-data-generation-into-the-training-pipeline","text":"Copy and modify the key configuration parameters in ajet/default_config/ajet_default.yaml , and set ajet.task_reader.type to data_generation to enable this reader. ajet : task_reader : type : data_generation # when `type == data_generation` data_generation : # Document reader configuration document_reader : document_path : - 'dataset/document/your-document1.pdf' - 'dataset/document/your-document2.pdf' languages : - eng # Task reader (for existing tasks) query_reader : type : jsonl_dataset_file jsonl_dataset_file : training : file_path : 'dataset/jsonl/your-queries.jsonl' # Number of tasks to generate task_num : 10 # LLM config llm_model : qwen-long llm_response_length : 8192 num_workers : 32 sampling_params : temperature : 0 # Task filtering config deduplication_filter : enabled : true params : similarity_threshold : 0.8 db_path : ./.similarity_db model : text-embedding-v4 api_key : null # load from the env base_url : https://dashscope.aliyuncs.com/compatible-mode/v1","title":"Method 1: Integrate Data Generation into the Training Pipeline"},{"location":"en/data_generation/#method-2-run-the-generation-script","text":"from ajet.data_generator.config import * from ajet.task_reader.data_generator_reader import DataGeneratorTaskReader def run (): config = TaskReaderConfig ( data_generation = DataGenerationConfig ( document_reader = DocumentReaderConfig ( document_path = [ 'dataset/document/your-document1.pdf' , 'dataset/document/your-document2.pdf' ], languages = [ \"eng\" ], chunk_size = 5120 , split_by = \"sentence\" , ), query_reader = QueryReaderConfig ( type = \"jsonl_dataset_file\" , jsonl_dataset_file = DatasetFileConfig ( training = TrainingDatasetConfig ( file_path = 'dataset/jsonl/your-queries.jsonl' ) ), ), task_num = 50 , llm_model = \"qwen-long\" , num_workers = 16 , sampling_params = SamplingParamsConfig ( temperature = 0.0 ), deduplication_filter = DeduplicationFilterConfig ( enabled = True , params = DeduplicationFilterParamsConfig ( similarity_threshold = 0.8 , model = \"text-embedding-v4\" , ), ), ) ) reader = DataGeneratorTaskReader ( reader_config = config ) run ()","title":"Method 2: Run the Generation Script"},{"location":"en/data_generation/#generated-task-examples","text":"Based on user-provided documents (optional) and a few task examples, the Data Generation Reader can batch-generate training tasks: [ { \"main_query\" : \"Are financial institutions required to verify the source of funds for corporate clients during account opening?\" }, { \"main_query\" : \"What are the requirements for banks to verify customer identities under anti-money laundering regulations?\" } ... ]","title":"Generated Task Examples"},{"location":"en/data_generation/#detailed-config-options","text":"Parameter Path Type Default Required Description document_reader.document_path list[str] - No List of document file paths. Supports PDF, Word, TXT, and more. document_reader.languages list[str] ['eng'] No List of document languages for OCR and text parsing, e.g., eng (English), chs (Simplified Chinese). query_reader.type str jsonl_dataset_file Yes Reader type. Options: jsonl_dataset_file , env_service , huggingface_dat_repo . query_reader.jsonl_dataset_file.training.file_path str - Yes Path to the training tasks JSONL file (when type: jsonl_dataset_file ). task_num int 10 Yes Number of tasks to generate. The actual number may be reduced by filtering. llm_model str qwen-long Yes LLM model name used for task generation. llm_response_length int 8192 No Maximum number of tokens in the LLM response. num_workers int 32 No Number of parallel worker threads for speeding up task generation. sampling_params.temperature float 0 No Sampling temperature. 0 means greedy decoding (deterministic output); higher values make outputs more random. deduplication_filter.enabled bool true No Whether to enable the deduplication filter. deduplication_filter.params.similarity_threshold float 0.8 Yes Similarity threshold (0\u20131). Tasks above this threshold will be filtered out. deduplication_filter.params.db_path str ./.similarity_db No Path to the similarity database used to cache embeddings. deduplication_filter.params.model str text-embedding-v4 Yes Embedding model used to compute similarity. deduplication_filter.params.api_key str null No API key. If null , it will be loaded from the DASHSCOPE_API_KEY environment variable. deduplication_filter.params.base_url str https://dashscope.aliyuncs.com/compatible-mode/v1 No Base URL for the embedding API.","title":"Detailed Config Options"},{"location":"en/data_pipeline/","text":"AgentJet loads training tasks from various data sources through Task Reader. This page covers the Task schema definition and different built-in Task Readers for common scenarios. Overview In agent training, all training data must be represented as tasks following a unified schema. Key Concepts Unified Schema : All tasks conform to the Task structure regardless of source Multiple Sources : Load from local files, HuggingFace datasets, interactive environments, or auto-generate new tasks Automatic Routing : The framework selects the appropriate reader based on ajet.task_reader.type Data Source \u2192 Task Reader \u2192 Unified Task Schema \u2192 Training Pipeline Task Schema All training tasks must be defined according to the following structure: class Task ( BaseModel ): main_query : str = Field ( default = \"\" ) init_messages : List [ dict ] = Field ( default = []) task_id : str = Field ( default = \"\" ) env_type : str = Field ( default = \"\" ) metadata : dict = Field ( default_factory = dict ) Field Descriptions Field Type Description main_query str The main instruction or question for the agent to solve init_messages List[dict] Initial conversation messages (e.g., system prompts). Each must have role and content fields task_id str Unique identifier for the task env_type str Environment type (e.g., \"math\", \"appworld\") metadata dict Additional context information (e.g., reference answers for reward calculation) Example Task example_task.json { \"main_query\" : \"What is 15 * 23?\" , \"init_messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful math assistant.\" } ], \"task_id\" : \"math_001\" , \"env_type\" : \"math\" , \"metadata\" : { \"answer\" : \"345\" , \"difficulty\" : \"easy\" } } Best Practices Use metadata to store information needed for reward computation (e.g., reference answers, scoring rubrics) Keep main_query clear and concise Use init_messages for system prompts or few-shot examples Built-in Task Readers AgentJet provides multiple built-in Task Readers for different scenarios. The framework automatically routes to the correct reader based on ajet.task_reader.type . Quick Selection Guide JSONL File You have prepared task data in JSONL format locally. HuggingFace Load tasks from HuggingFace Hub (e.g., GSM8K, MATH). EnvService Tasks come from a running environment service. 1. JSONL File Reader When to use: You have prepared training tasks in JSONL format locally. Configuration JSONL Format config.yaml ajet : task_reader : type : jsonl_dataset_file jsonl_dataset_file : training : file_path : \"data/train.jsonl\" validation : file_path : \"data/val.jsonl\" Each line should be a JSON object conforming to the Task schema: data/train.jsonl { \"main_query\" : \"Solve: x + 5 = 12\" , \"task_id\" : \"algebra_01\" , \"env_type\" : \"math\" , \"metadata\" : { \"answer\" : \"7\" }} { \"main_query\" : \"What is the capital of France?\" , \"task_id\" : \"geo_01\" , \"env_type\" : \"qa\" , \"metadata\" : { \"answer\" : \"Paris\" }} How it works Reads tasks line-by-line from specified JSONL files Automatically validates against Task schema Supports separate training and validation splits 2. HuggingFace Dataset Reader When to use: Load tasks from HuggingFace Hub datasets (e.g., GSM8K, MATH). config.yaml ajet : task_reader : type : huggingface_dat_repo huggingface_dat_repo : dataset_path : \"gsm8k\" # HF dataset repo name dataset_name : \"main\" # Optional: dataset subset name training_split : \"train\" # Training split name validation_split : \"test\" # Validation split name How it works Downloads dataset from HuggingFace Hub using datasets library Automatically maps dataset fields to Task schema Caches downloaded data locally for faster subsequent runs 3. EnvService Reader When to use: Tasks are provided by an interactive environment service (e.g., AppWorld, RL gym environments). config.yaml ajet : task_reader : type : env_service env_service : env_type : \"appworld\" # Environment type env_url : \"http://127.0.0.1:8080\" # Service URL env_action_preference : code # Action format: code/text/box training_split : train validation_split : dev How it works Connects to a running environment service via HTTP Pulls task instances from the environment Supports dynamic task generation from interactive environments Use Cases Training agents in simulated environments (e.g., FrozenLake, game environments) Complex interactive scenarios where tasks are generated dynamically Next Steps Task Judger Set up reward functions to evaluate agent outputs. Configuration Complete reference for all configuration options.","title":"Data Pipeline"},{"location":"en/data_pipeline/#overview","text":"In agent training, all training data must be represented as tasks following a unified schema. Key Concepts Unified Schema : All tasks conform to the Task structure regardless of source Multiple Sources : Load from local files, HuggingFace datasets, interactive environments, or auto-generate new tasks Automatic Routing : The framework selects the appropriate reader based on ajet.task_reader.type Data Source \u2192 Task Reader \u2192 Unified Task Schema \u2192 Training Pipeline","title":"Overview"},{"location":"en/data_pipeline/#task-schema","text":"All training tasks must be defined according to the following structure: class Task ( BaseModel ): main_query : str = Field ( default = \"\" ) init_messages : List [ dict ] = Field ( default = []) task_id : str = Field ( default = \"\" ) env_type : str = Field ( default = \"\" ) metadata : dict = Field ( default_factory = dict )","title":"Task Schema"},{"location":"en/data_pipeline/#field-descriptions","text":"Field Type Description main_query str The main instruction or question for the agent to solve init_messages List[dict] Initial conversation messages (e.g., system prompts). Each must have role and content fields task_id str Unique identifier for the task env_type str Environment type (e.g., \"math\", \"appworld\") metadata dict Additional context information (e.g., reference answers for reward calculation)","title":"Field Descriptions"},{"location":"en/data_pipeline/#example-task","text":"example_task.json { \"main_query\" : \"What is 15 * 23?\" , \"init_messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful math assistant.\" } ], \"task_id\" : \"math_001\" , \"env_type\" : \"math\" , \"metadata\" : { \"answer\" : \"345\" , \"difficulty\" : \"easy\" } } Best Practices Use metadata to store information needed for reward computation (e.g., reference answers, scoring rubrics) Keep main_query clear and concise Use init_messages for system prompts or few-shot examples","title":"Example Task"},{"location":"en/data_pipeline/#built-in-task-readers","text":"AgentJet provides multiple built-in Task Readers for different scenarios. The framework automatically routes to the correct reader based on ajet.task_reader.type .","title":"Built-in Task Readers"},{"location":"en/data_pipeline/#quick-selection-guide","text":"","title":"Quick Selection Guide"},{"location":"en/data_pipeline/#1-jsonl-file-reader","text":"When to use: You have prepared training tasks in JSONL format locally. Configuration JSONL Format config.yaml ajet : task_reader : type : jsonl_dataset_file jsonl_dataset_file : training : file_path : \"data/train.jsonl\" validation : file_path : \"data/val.jsonl\" Each line should be a JSON object conforming to the Task schema: data/train.jsonl { \"main_query\" : \"Solve: x + 5 = 12\" , \"task_id\" : \"algebra_01\" , \"env_type\" : \"math\" , \"metadata\" : { \"answer\" : \"7\" }} { \"main_query\" : \"What is the capital of France?\" , \"task_id\" : \"geo_01\" , \"env_type\" : \"qa\" , \"metadata\" : { \"answer\" : \"Paris\" }} How it works Reads tasks line-by-line from specified JSONL files Automatically validates against Task schema Supports separate training and validation splits","title":"1. JSONL File Reader"},{"location":"en/data_pipeline/#2-huggingface-dataset-reader","text":"When to use: Load tasks from HuggingFace Hub datasets (e.g., GSM8K, MATH). config.yaml ajet : task_reader : type : huggingface_dat_repo huggingface_dat_repo : dataset_path : \"gsm8k\" # HF dataset repo name dataset_name : \"main\" # Optional: dataset subset name training_split : \"train\" # Training split name validation_split : \"test\" # Validation split name How it works Downloads dataset from HuggingFace Hub using datasets library Automatically maps dataset fields to Task schema Caches downloaded data locally for faster subsequent runs","title":"2. HuggingFace Dataset Reader"},{"location":"en/data_pipeline/#3-envservice-reader","text":"When to use: Tasks are provided by an interactive environment service (e.g., AppWorld, RL gym environments). config.yaml ajet : task_reader : type : env_service env_service : env_type : \"appworld\" # Environment type env_url : \"http://127.0.0.1:8080\" # Service URL env_action_preference : code # Action format: code/text/box training_split : train validation_split : dev How it works Connects to a running environment service via HTTP Pulls task instances from the environment Supports dynamic task generation from interactive environments Use Cases Training agents in simulated environments (e.g., FrozenLake, game environments) Complex interactive scenarios where tasks are generated dynamically","title":"3. EnvService Reader"},{"location":"en/data_pipeline/#next-steps","text":"","title":"Next Steps"},{"location":"en/debugging_guide/","text":"In this tutorial, we introduce the way to debug the workflow and the training algorithms. Workflow Debugging (--backbone=debug) Install VSCode and connect to GPU server. VSCode is a open-source software and provides all debugging plugins for free. Therefore, we choose VSCode as our debugging platform. VSCode can connect to remote ssh server and operate it as if it is your local machine. For more details, please refer to VSCode official documents. Install VSCode Python Extension Bundle Create .vscode/launch.json . If .vscode does not exists yet, create it. Copy and paste the following configuration into launch.json { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] } Modify ./path/to/yaml.yaml field to your task yaml. For more sophisticated task with additional external service, add env variables or more args. For example, if your original training command is: export DASHSCOPE_API_KEY = \"sk-abcdefg\" ajet --conf tutorial/example_appworld/appworld.yaml --with-appworld --backbone = 'verl' Then, the modified launch.json will be { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , // verl -> debug \"--conf\" , \"tutorial/example_appworld/appworld.yaml\" , \"--with-appworld\" , ], \"env\" : { \"DASHSCOPE_API_KEY\" : \"sk-abcdefg\" } } ] } Press F5 to start debugging. You can set breakpoint inside the workflow to observe program execution now. General Debugging (Ray Distributed Debugger) Install the Ray Distributed Debugger extension in VSCode. In AgentJet project: 2-1. In the place your want to set a conditional breakpoint, write from ajet import bp; bp(\"TAG_1\") 2-2. When launching the training process, add --debug as commandline argument ajet --conf your_config.yaml --debug=\"TAG_1\" 2-3. Open Tab \"Ray Distributed Debugger\" in VSCode, and just wait until the breakpoint is hit. Comparison Feature Workflow Debugging General Debugging (Ray) Backend debug , tinker verl , trinity Reboot Speed Very Fast Slow Debug Target Workflow Everything VSCode Extension Python Python + Ray Distributed Debugger Launch Mode F5 standard launch (via launch.json ) Command line execution with ajet ... --debug=\"TAG\" Commandline --backbone=debug --debug=\"TAG1\\|TAG2\\|TAG3\"","title":"Debugging guide"},{"location":"en/debugging_guide/#workflow-debugging-backbonedebug","text":"Install VSCode and connect to GPU server. VSCode is a open-source software and provides all debugging plugins for free. Therefore, we choose VSCode as our debugging platform. VSCode can connect to remote ssh server and operate it as if it is your local machine. For more details, please refer to VSCode official documents. Install VSCode Python Extension Bundle Create .vscode/launch.json . If .vscode does not exists yet, create it. Copy and paste the following configuration into launch.json { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] } Modify ./path/to/yaml.yaml field to your task yaml. For more sophisticated task with additional external service, add env variables or more args. For example, if your original training command is: export DASHSCOPE_API_KEY = \"sk-abcdefg\" ajet --conf tutorial/example_appworld/appworld.yaml --with-appworld --backbone = 'verl' Then, the modified launch.json will be { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , // verl -> debug \"--conf\" , \"tutorial/example_appworld/appworld.yaml\" , \"--with-appworld\" , ], \"env\" : { \"DASHSCOPE_API_KEY\" : \"sk-abcdefg\" } } ] } Press F5 to start debugging. You can set breakpoint inside the workflow to observe program execution now.","title":"Workflow Debugging (--backbone=debug)"},{"location":"en/debugging_guide/#general-debugging-ray-distributed-debugger","text":"Install the Ray Distributed Debugger extension in VSCode. In AgentJet project: 2-1. In the place your want to set a conditional breakpoint, write from ajet import bp; bp(\"TAG_1\") 2-2. When launching the training process, add --debug as commandline argument ajet --conf your_config.yaml --debug=\"TAG_1\" 2-3. Open Tab \"Ray Distributed Debugger\" in VSCode, and just wait until the breakpoint is hit.","title":"General Debugging (Ray Distributed Debugger)"},{"location":"en/debugging_guide/#comparison","text":"Feature Workflow Debugging General Debugging (Ray) Backend debug , tinker verl , trinity Reboot Speed Very Fast Slow Debug Target Workflow Everything VSCode Extension Python Python + Ray Distributed Debugger Launch Mode F5 standard launch (via launch.json ) Command line execution with ajet ... --debug=\"TAG\" Commandline --backbone=debug --debug=\"TAG1\\|TAG2\\|TAG3\"","title":"Comparison"},{"location":"en/example/","text":"This section provides practical examples of how to use AgentJet in various scenarios.","title":"Example"},{"location":"en/example_app_world/","text":"This tutorial demonstrates how to train an agent to interact with AppWorld and solve complex tasks through tool usage. 1. Overview AppWorld is a high-fidelity execution environment of 9 day-to-day apps, operable via 457 APIs, populated with digital activities of 106 people living in a simulated world. The goal is to tune an agent that can effectively navigate and utilize these apps to complete complex tasks. This document is organized as follows: Quick Start: run the example with minimal setup Understand: workflow loop, configuration, code locations, and reward Results: training curve and qualitative cases 2. Quick Start 2.1 Preparation First, download and unpack the Appworld services. The script below is idempotent: it clears any existing folder and re-downloads the archive. base_path = \"/tmp\" export APPWORLD_PATH = \" ${ base_path } /pack_all_in_one\" export APPWORLD_SCRIPT = \"bash EnvService/env_sandbox/appworld.sh\" rm -rf \" ${ APPWORLD_PATH } \" rm -f ./appworld_pack_v2.tar.gz wget -q \"https://dail-wlcb.oss-cn-wulanchabu.aliyuncs.com/astuner_archive/appworld_pack_v2.tar.gz\" -O appworld_pack_v2.tar.gz tar -xzf ./appworld_pack_v2.tar.gz -C \" ${ base_path } \" Then export the environment variables (re-run in every new shell): export BASE_PATH = /tmp export APPWORLD_PATH = \" ${ BASE_PATH } /pack_all_in_one\" export APPWORLD_SCRIPT = \"bash EnvService/env_sandbox/appworld.sh\" 2.2 Start Training Run the training script: ajet --conf tutorial/example_appworld/appworld.yaml --with-appworld Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_appworld/math_agent.yaml --backbone = 'debug' --with-logview When `--backbone=debug`, Ray is disabled. You can use a VSCode `.vscode/launch.json` like below: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] } 3. Understand This section explains how the AppWorld example is assembled: workflow, reward, configuration, and code locations. 3.1 Core Process The AgentScope workflow code for the AppWorld example is located at tutorial/example_appworld/appworld.py . The code first defines the AgentScope workflow (set the agent's model to tuner.as_agentscope_model() ): agent = ReActAgent ( name = \"Qwen\" , sys_prompt = first_msg [ \"content\" ], model = tuner . as_agentscope_model (), formatter = DashScopeChatFormatter (), memory = InMemoryMemory (), toolkit = None , print_hint_msg = False , ) env = workflow_task . gym_env for step in range ( tuner . config . ajet . rollout . multi_turn . max_steps ): # agentscope deal with interaction message reply_message = await agent ( interaction_message ) # env service protocol obs , _ , terminate , _ = env . step ( action = { \"content\" : reply_message . content , \"role\" : \"assistant\" } ) # generate new message from env output interaction_message = Msg ( name = \"env\" , content = obs , role = \"user\" ) # is terminated? if terminate : break if tuner . get_context_tracker () . context_overflow : break In the above code: env.step : simulates the gym interface. It takes an action as input and returns a four-tuple (observation, reward, terminate_flag, info) . tuner.get_context_tracker().context_overflow : checks whether the current context window has exceeded the token limit. 3.2 Reward In ajet/task_judge/env_service_as_judge.py , we read the reward signal from the environment via env.evaluate(...) . You can also refer to this file to implement your own Judge for your specific task. 3.3 Configuration Details Copy and modify the key parameters in tutorial/example_appworld/appworld.yaml . The parts most relevant to this document are marked with in the yaml file: Read tasks (corresponding config field: ajet.task_reader ) Define the workflow (corresponding config field: ajet.rollout.user_workflow ) Example: if the AgentScope workflow is defined in the ExampleAgentScopeWorkflow class in tutorial/example_appworld/appworld.py Then set ajet.rollout.user_workflow = \"tutorial.example_appworld.appworld->ExampleAgentScopeWorkflow\" Define the scoring function (corresponding config field: ajet.task_judge.judge_protocol ) Example: ajet.task_judge.judge_protocol = \"ajet.task_judge.env_service_as_judge->EnvServiceJudge\" Specify the model (corresponding config field: ajet.model.path ) ajet : project_name : example_appworld experiment_name : \"read_yaml_name\" task_judge : # [key] Implement and select the evaluation function judge_protocol : ajet.task_judge.env_service_as_judge->EnvServiceJudge model : # [key] Set the model to be trained path : YOUR_MODEL_PATH rollout : # [key] Implement and select the Agent user_workflow : tutorial.example_appworld.appworld->ExampleAgentScopeWorkflow force_disable_toolcalls : True debug : debug_max_parallel : 1 debug_first_n_tasks : 1 4. Results 4.1 Training Curve Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. As training progresses, reward increases. This usually means the agent becomes more stable on two things : Following correct API protocols : it learns to look up API documentation before calling, and uses valid API endpoints instead of hallucinating non-existent ones. Completing multi-step workflows : it can properly obtain access tokens and chain multiple API calls to accomplish complex tasks. 4.2 Case Study Before tuning: Frequently call non-existent APIs The agent hallucinates API names without checking whether they exist, leading to repeated failures. Fail to follow the instructions to obtain an access token The agent attempts to call protected APIs without first obtaining the required access token, resulting in authentication errors. After tuning: Look up the API documentation first, and learn to use valid APIs The agent now checks available APIs before making calls, avoiding hallucinated endpoints. Learn to obtain an access token correctly The agent properly handles the authentication step before accessing protected APIs. Token-level Visualization: These detailed logs are generated by Beast-Logger. See Beast-Logger Usage for more details.","title":"AppWorld Agent"},{"location":"en/example_app_world/#1-overview","text":"AppWorld is a high-fidelity execution environment of 9 day-to-day apps, operable via 457 APIs, populated with digital activities of 106 people living in a simulated world. The goal is to tune an agent that can effectively navigate and utilize these apps to complete complex tasks. This document is organized as follows: Quick Start: run the example with minimal setup Understand: workflow loop, configuration, code locations, and reward Results: training curve and qualitative cases","title":"1. Overview"},{"location":"en/example_app_world/#2-quick-start","text":"","title":"2. Quick Start"},{"location":"en/example_app_world/#21-preparation","text":"First, download and unpack the Appworld services. The script below is idempotent: it clears any existing folder and re-downloads the archive. base_path = \"/tmp\" export APPWORLD_PATH = \" ${ base_path } /pack_all_in_one\" export APPWORLD_SCRIPT = \"bash EnvService/env_sandbox/appworld.sh\" rm -rf \" ${ APPWORLD_PATH } \" rm -f ./appworld_pack_v2.tar.gz wget -q \"https://dail-wlcb.oss-cn-wulanchabu.aliyuncs.com/astuner_archive/appworld_pack_v2.tar.gz\" -O appworld_pack_v2.tar.gz tar -xzf ./appworld_pack_v2.tar.gz -C \" ${ base_path } \" Then export the environment variables (re-run in every new shell): export BASE_PATH = /tmp export APPWORLD_PATH = \" ${ BASE_PATH } /pack_all_in_one\" export APPWORLD_SCRIPT = \"bash EnvService/env_sandbox/appworld.sh\"","title":"2.1 Preparation"},{"location":"en/example_app_world/#22-start-training","text":"Run the training script: ajet --conf tutorial/example_appworld/appworld.yaml --with-appworld Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_appworld/math_agent.yaml --backbone = 'debug' --with-logview When `--backbone=debug`, Ray is disabled. You can use a VSCode `.vscode/launch.json` like below: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] }","title":"2.2 Start Training"},{"location":"en/example_app_world/#3-understand","text":"This section explains how the AppWorld example is assembled: workflow, reward, configuration, and code locations.","title":"3. Understand"},{"location":"en/example_app_world/#31-core-process","text":"The AgentScope workflow code for the AppWorld example is located at tutorial/example_appworld/appworld.py . The code first defines the AgentScope workflow (set the agent's model to tuner.as_agentscope_model() ): agent = ReActAgent ( name = \"Qwen\" , sys_prompt = first_msg [ \"content\" ], model = tuner . as_agentscope_model (), formatter = DashScopeChatFormatter (), memory = InMemoryMemory (), toolkit = None , print_hint_msg = False , ) env = workflow_task . gym_env for step in range ( tuner . config . ajet . rollout . multi_turn . max_steps ): # agentscope deal with interaction message reply_message = await agent ( interaction_message ) # env service protocol obs , _ , terminate , _ = env . step ( action = { \"content\" : reply_message . content , \"role\" : \"assistant\" } ) # generate new message from env output interaction_message = Msg ( name = \"env\" , content = obs , role = \"user\" ) # is terminated? if terminate : break if tuner . get_context_tracker () . context_overflow : break In the above code: env.step : simulates the gym interface. It takes an action as input and returns a four-tuple (observation, reward, terminate_flag, info) . tuner.get_context_tracker().context_overflow : checks whether the current context window has exceeded the token limit.","title":"3.1 Core Process"},{"location":"en/example_app_world/#32-reward","text":"In ajet/task_judge/env_service_as_judge.py , we read the reward signal from the environment via env.evaluate(...) . You can also refer to this file to implement your own Judge for your specific task.","title":"3.2 Reward"},{"location":"en/example_app_world/#33-configuration-details","text":"Copy and modify the key parameters in tutorial/example_appworld/appworld.yaml . The parts most relevant to this document are marked with in the yaml file: Read tasks (corresponding config field: ajet.task_reader ) Define the workflow (corresponding config field: ajet.rollout.user_workflow ) Example: if the AgentScope workflow is defined in the ExampleAgentScopeWorkflow class in tutorial/example_appworld/appworld.py Then set ajet.rollout.user_workflow = \"tutorial.example_appworld.appworld->ExampleAgentScopeWorkflow\" Define the scoring function (corresponding config field: ajet.task_judge.judge_protocol ) Example: ajet.task_judge.judge_protocol = \"ajet.task_judge.env_service_as_judge->EnvServiceJudge\" Specify the model (corresponding config field: ajet.model.path ) ajet : project_name : example_appworld experiment_name : \"read_yaml_name\" task_judge : # [key] Implement and select the evaluation function judge_protocol : ajet.task_judge.env_service_as_judge->EnvServiceJudge model : # [key] Set the model to be trained path : YOUR_MODEL_PATH rollout : # [key] Implement and select the Agent user_workflow : tutorial.example_appworld.appworld->ExampleAgentScopeWorkflow force_disable_toolcalls : True debug : debug_max_parallel : 1 debug_first_n_tasks : 1","title":"3.3 Configuration Details"},{"location":"en/example_app_world/#4-results","text":"","title":"4. Results"},{"location":"en/example_app_world/#41-training-curve","text":"Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. As training progresses, reward increases. This usually means the agent becomes more stable on two things : Following correct API protocols : it learns to look up API documentation before calling, and uses valid API endpoints instead of hallucinating non-existent ones. Completing multi-step workflows : it can properly obtain access tokens and chain multiple API calls to accomplish complex tasks.","title":"4.1 Training Curve"},{"location":"en/example_app_world/#42-case-study","text":"","title":"4.2 Case Study"},{"location":"en/example_app_world/#before-tuning","text":"Frequently call non-existent APIs The agent hallucinates API names without checking whether they exist, leading to repeated failures. Fail to follow the instructions to obtain an access token The agent attempts to call protected APIs without first obtaining the required access token, resulting in authentication errors.","title":"Before tuning:"},{"location":"en/example_app_world/#after-tuning","text":"Look up the API documentation first, and learn to use valid APIs The agent now checks available APIs before making calls, avoiding hallucinated endpoints. Learn to obtain an access token correctly The agent properly handles the authentication step before accessing protected APIs. Token-level Visualization: These detailed logs are generated by Beast-Logger. See Beast-Logger Usage for more details.","title":"After tuning:"},{"location":"en/example_countdown/","text":"1. Overview Countdown is a math puzzle game. Given a list of numbers and a target number, the player needs to use the numbers and the four basic arithmetic operations (addition, subtraction, multiplication, and division) to form an expression that evaluates to the target number. Each number can be used only once, but parentheses can be used freely to change the order of operations. 2. Quick Start 2.1 Preparation Download the Jiayi-Pan/Countdown-Tasks-3to4 dataset and split it into training and test sets: python tutorial/example_countdown/prepare_data.py --target = Jiayi-Pan/Countdown-Tasks-3to4 --path = /the/path/to/store/dataset The Countdown dataset contains the target and nums fields and requires custom data formatting logic. For example, when using the huggingface_dat_repo task reader method, you need to modify the _load_dataset_split method in ajet/task_reader/hf_dataset_reader.py : task = Task ( main_query = json . dumps ({ 'target' : example [ \"target\" ], 'nums' : example [ \"nums\" ]}), init_messages = [], task_id = str ( idx ), env_type = \"no_env\" , metadata = example , ) 2.2 Start Training Simply run the following command: # It is recommended to kill all ray, vllm, and env_service processes before starting ( python launcher.py --kill=\"python|ray|vllm\" ) ajet --conf tutorial/example_countdown/countdown.yaml --backbone = 'verl' Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_countdown/countdown.yaml --backbone = 'debug' --with-logview When `--backbone=debug`, Ray is disabled. You can use a VSCode `.vscode/launch.json` like below: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] } 3. Understand In this section, we will cover the details of this tutorial. 3.1 Core Process See details in tutorial/example_countdown/countdown.py . You can create new AgentScope Workflow code anywhere in the project. Define the AgentScope workflow self . agent = ReActAgent ( name = \"countdown_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), formatter = DashScopeChatFormatter (), memory = InMemoryMemory (), max_iters = 2 , ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) In the AgentScope Workflow\uff0cyou need to write the key information required by the judge into: WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer , \"target\" : target , \"nums\" : nums , } ) 3.2 Configuration Details Copy and modify key parameters in tutorial/example_countdown/countdown.yaml . The most relevant parts in the yaml file are marked with symbols. Read task (corresponds to configuration field ajet.task_reader ) Define Workflow (corresponds to configuration field ajet.rollout.user_workflow ) Example: If agentscope workflow is defined in ExampleCountdownLearn class of tutorial/example_countdown/countdown.py Then set ajet.rollout.user_workflow = tutorial.example_countdown.countdown->ExampleCountdownLearn Define scoring function (corresponds to configuration field ajet.task_judge.judge_protocol ) Example: If agentscope workflow is defined in CountdownAnswerAsJudge class of tutorial/example_countdown/countdown_answer_as_judge.py Then set ajet.task_judge.judge_protocol = tutorial.example_countdown.countdown_answer_as_judge->CountdownAnswerAsJudge Specify model (corresponds to configuration field ajet.model.path ) ajet : task_reader : type : huggingface_dat_repo # [key] `env_service` or `dataset_file` or `huggingface_dat_repo` or `data_generation` rollout : user_workflow : tutorial.example_countdown.countdown->ExampleCountdownLearn # [key] Write and select Agent task_judge : # [key] Write and select evaluation function judge_protocol : tutorial.example_countdown.countdown_answer_as_judge->CountdownAnswerAsJudge model : # [key] Set the model to be trained path : YOUR_MODEL_PATH 3.3 Code Map tutorial/example_countdown/countdown.py : defines the AgentScope workflow (e.g., ExampleCountdownLearn ). tutorial/example_countdown/countdown.yaml : wires together task reader, workflow, judge, and model. 3.4 Reward/Evaluation Mechanism A simple Judge is provided in tutorial/example_countdown/countdown_answer_as_judge.py . You can create new Judge code anywhere in the project. Judge input parameters include: workflow_task: Task information\uff08if reference answer is included, it can be retrieved from here\uff09 workflow_output: Task information output (final_answer needs to be added manually) Judge return values: raw_reward is_success 4. Results 4.1 Training Curves/Metrics Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. The upward trend in reward reflects improvement in two key areas: Format compliance : the agent learns to emit well-formed <tool_call> JSON and wrap answers in \\boxed{...} . Accuracy improvement : the agent achieves higher correctness rates in solving problems. 4.2 Case Study Agents are already able to deal with some simple problems at the beginning of the training process; however, small models inevitably produce many answers that fail to fully meet the requirements. In some problems, the agent does not strictly follow the instructions, such as invalid output format, reusing numbers, or missing expression outputs. bad case 1: Invalid output format. <|im_start|>assistant <tool_call> {\"name\": \"generate_response\", \"arguments\": \"Let's start by subtracting 28 from 76, which gives us 48. Now we need to get to 55 from 48. We can achieve this by adding 7 from the remaining numbers. However, 7 is not available. The next best number is 69. If we subtract 69 from 78, we get -11. If we subtract -11 from 48, we actually end up adding 11, which isn't one of our numbers. The correct approach would be to add 7 from the remaining numbers, but since 7 isn't available, we can use 69 and adjust our strategy. We can add 7 from the remaining numbers in another way. Let's try subtracting 69 from 78 to get -11, and then add 11 to 48. 11 can be obtained by subtracting 69 from 80, but since 80 isn't available, we'll use the next best approach. We'll subtract 69 from 78 to get -11, and then add 16 to 48 to reach 55. We can get 16 by subtracting 7 from 23, but since 23 isn't available, we need to find an alternative. The best way would be to use 78 - 69 = -11, and then 48 + 11 = 59 which is close but not 55. However, we can achieve 55 by using 78 - 69 + 7 = 55. So, the correct calculation is 78 - 69 + 7 = 55.\"} </tool_call><|im_end|> bad case 2: Reusing numbers in expressions. <|im_start|>assistant NavParams = {\"response\": \"Let's solve this step by step. We have the numbers 72, 20, and 4, and we need to reach the target number 38 using arithmetic operations.\\n\\nFirst, let's try using subtraction and addition since 72 is quite large and can help us get closer to our target if we subtract something. We can subtract 20 from 72 to get 52, which is close but still far from 38. Now, we can subtract 4 from 52 to get 48, and then subtract 10 to reach 38. Since we don't have a 10 directly, we can use 20 - 4 = 16, and then subtract 16 from 48 to get 32, and finally add 6 (which is 4 + 2) to reach 38.\\n\\nLet's try a more efficient approach: 72 - 20 = 52, and 52 - 4 = 48, now we need to reduce it by 10, which is 20 - 14, and 14 can be obtained by 20 - 4 - 2. So, 48 - (20 - 4 - 2) = 48 - 14 = 34, then we need 4 more to reach 38, which is simply 4. Thus, the solution is (72 - 20) - 4 - (20 - 4 - 2) + 4 = 38.\"} <tool_call> {\"name\": \"generate_response\", \"arguments\": {\"response\": \"The solution is \\boxed{(72 - 20) - 4 - (20 - 4 - 2) + 4}. Let's break it down: \\n1. 72 - 20 = 52\\n2. 52 - 4 = 48\\n3. 20 - 4 - 2 = 14\\n4. 48 - 14 = 34\\n5. 34 + 4 = 38\\nTherefore, we reach the target number 38.\"}} </tool_call><|im_end|> bad case 3: Missing expression outputs. <|im_start|>assistant <tool_call> {\"name\": \"generate_response\", \"arguments\": {\"response\": \"In this case, one straightforward way to reach the target number 64 is by directly using the number 64 itself, since it matches the target exactly. No other calculations are needed.\"}} boxing the answer as \\boxed{{64}}.<|im_end|> However, tuning resolves these issues, as shown in the example below: Message 3 : decomposes the problem + emits a <tool_call> , and uses \\boxed{} to output key values Message 4 : decomposes the problem further, and reuses numbers in expressions. Token-level Visualization: These detailed logs are generated by Beast-Logger. See Beast-Logger Usage for more details.","title":"Countdown Game"},{"location":"en/example_countdown/#1-overview","text":"Countdown is a math puzzle game. Given a list of numbers and a target number, the player needs to use the numbers and the four basic arithmetic operations (addition, subtraction, multiplication, and division) to form an expression that evaluates to the target number. Each number can be used only once, but parentheses can be used freely to change the order of operations.","title":"1. Overview"},{"location":"en/example_countdown/#2-quick-start","text":"","title":"2. Quick Start"},{"location":"en/example_countdown/#21-preparation","text":"Download the Jiayi-Pan/Countdown-Tasks-3to4 dataset and split it into training and test sets: python tutorial/example_countdown/prepare_data.py --target = Jiayi-Pan/Countdown-Tasks-3to4 --path = /the/path/to/store/dataset The Countdown dataset contains the target and nums fields and requires custom data formatting logic. For example, when using the huggingface_dat_repo task reader method, you need to modify the _load_dataset_split method in ajet/task_reader/hf_dataset_reader.py : task = Task ( main_query = json . dumps ({ 'target' : example [ \"target\" ], 'nums' : example [ \"nums\" ]}), init_messages = [], task_id = str ( idx ), env_type = \"no_env\" , metadata = example , )","title":"2.1 Preparation"},{"location":"en/example_countdown/#22-start-training","text":"Simply run the following command: # It is recommended to kill all ray, vllm, and env_service processes before starting ( python launcher.py --kill=\"python|ray|vllm\" ) ajet --conf tutorial/example_countdown/countdown.yaml --backbone = 'verl' Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_countdown/countdown.yaml --backbone = 'debug' --with-logview When `--backbone=debug`, Ray is disabled. You can use a VSCode `.vscode/launch.json` like below: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] }","title":"2.2 Start Training"},{"location":"en/example_countdown/#3-understand","text":"In this section, we will cover the details of this tutorial.","title":"3. Understand"},{"location":"en/example_countdown/#31-core-process","text":"See details in tutorial/example_countdown/countdown.py . You can create new AgentScope Workflow code anywhere in the project. Define the AgentScope workflow self . agent = ReActAgent ( name = \"countdown_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), formatter = DashScopeChatFormatter (), memory = InMemoryMemory (), max_iters = 2 , ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) In the AgentScope Workflow\uff0cyou need to write the key information required by the judge into: WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer , \"target\" : target , \"nums\" : nums , } )","title":"3.1 Core Process"},{"location":"en/example_countdown/#32-configuration-details","text":"Copy and modify key parameters in tutorial/example_countdown/countdown.yaml . The most relevant parts in the yaml file are marked with symbols. Read task (corresponds to configuration field ajet.task_reader ) Define Workflow (corresponds to configuration field ajet.rollout.user_workflow ) Example: If agentscope workflow is defined in ExampleCountdownLearn class of tutorial/example_countdown/countdown.py Then set ajet.rollout.user_workflow = tutorial.example_countdown.countdown->ExampleCountdownLearn Define scoring function (corresponds to configuration field ajet.task_judge.judge_protocol ) Example: If agentscope workflow is defined in CountdownAnswerAsJudge class of tutorial/example_countdown/countdown_answer_as_judge.py Then set ajet.task_judge.judge_protocol = tutorial.example_countdown.countdown_answer_as_judge->CountdownAnswerAsJudge Specify model (corresponds to configuration field ajet.model.path ) ajet : task_reader : type : huggingface_dat_repo # [key] `env_service` or `dataset_file` or `huggingface_dat_repo` or `data_generation` rollout : user_workflow : tutorial.example_countdown.countdown->ExampleCountdownLearn # [key] Write and select Agent task_judge : # [key] Write and select evaluation function judge_protocol : tutorial.example_countdown.countdown_answer_as_judge->CountdownAnswerAsJudge model : # [key] Set the model to be trained path : YOUR_MODEL_PATH","title":"3.2 Configuration Details"},{"location":"en/example_countdown/#33-code-map","text":"tutorial/example_countdown/countdown.py : defines the AgentScope workflow (e.g., ExampleCountdownLearn ). tutorial/example_countdown/countdown.yaml : wires together task reader, workflow, judge, and model.","title":"3.3 Code Map"},{"location":"en/example_countdown/#34-rewardevaluation-mechanism","text":"A simple Judge is provided in tutorial/example_countdown/countdown_answer_as_judge.py . You can create new Judge code anywhere in the project. Judge input parameters include: workflow_task: Task information\uff08if reference answer is included, it can be retrieved from here\uff09 workflow_output: Task information output (final_answer needs to be added manually) Judge return values: raw_reward is_success","title":"3.4 Reward/Evaluation Mechanism"},{"location":"en/example_countdown/#4-results","text":"","title":"4. Results"},{"location":"en/example_countdown/#41-training-curvesmetrics","text":"Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. The upward trend in reward reflects improvement in two key areas: Format compliance : the agent learns to emit well-formed <tool_call> JSON and wrap answers in \\boxed{...} . Accuracy improvement : the agent achieves higher correctness rates in solving problems.","title":"4.1 Training Curves/Metrics"},{"location":"en/example_countdown/#42-case-study","text":"Agents are already able to deal with some simple problems at the beginning of the training process; however, small models inevitably produce many answers that fail to fully meet the requirements. In some problems, the agent does not strictly follow the instructions, such as invalid output format, reusing numbers, or missing expression outputs. bad case 1: Invalid output format. <|im_start|>assistant <tool_call> {\"name\": \"generate_response\", \"arguments\": \"Let's start by subtracting 28 from 76, which gives us 48. Now we need to get to 55 from 48. We can achieve this by adding 7 from the remaining numbers. However, 7 is not available. The next best number is 69. If we subtract 69 from 78, we get -11. If we subtract -11 from 48, we actually end up adding 11, which isn't one of our numbers. The correct approach would be to add 7 from the remaining numbers, but since 7 isn't available, we can use 69 and adjust our strategy. We can add 7 from the remaining numbers in another way. Let's try subtracting 69 from 78 to get -11, and then add 11 to 48. 11 can be obtained by subtracting 69 from 80, but since 80 isn't available, we'll use the next best approach. We'll subtract 69 from 78 to get -11, and then add 16 to 48 to reach 55. We can get 16 by subtracting 7 from 23, but since 23 isn't available, we need to find an alternative. The best way would be to use 78 - 69 = -11, and then 48 + 11 = 59 which is close but not 55. However, we can achieve 55 by using 78 - 69 + 7 = 55. So, the correct calculation is 78 - 69 + 7 = 55.\"} </tool_call><|im_end|> bad case 2: Reusing numbers in expressions. <|im_start|>assistant NavParams = {\"response\": \"Let's solve this step by step. We have the numbers 72, 20, and 4, and we need to reach the target number 38 using arithmetic operations.\\n\\nFirst, let's try using subtraction and addition since 72 is quite large and can help us get closer to our target if we subtract something. We can subtract 20 from 72 to get 52, which is close but still far from 38. Now, we can subtract 4 from 52 to get 48, and then subtract 10 to reach 38. Since we don't have a 10 directly, we can use 20 - 4 = 16, and then subtract 16 from 48 to get 32, and finally add 6 (which is 4 + 2) to reach 38.\\n\\nLet's try a more efficient approach: 72 - 20 = 52, and 52 - 4 = 48, now we need to reduce it by 10, which is 20 - 14, and 14 can be obtained by 20 - 4 - 2. So, 48 - (20 - 4 - 2) = 48 - 14 = 34, then we need 4 more to reach 38, which is simply 4. Thus, the solution is (72 - 20) - 4 - (20 - 4 - 2) + 4 = 38.\"} <tool_call> {\"name\": \"generate_response\", \"arguments\": {\"response\": \"The solution is \\boxed{(72 - 20) - 4 - (20 - 4 - 2) + 4}. Let's break it down: \\n1. 72 - 20 = 52\\n2. 52 - 4 = 48\\n3. 20 - 4 - 2 = 14\\n4. 48 - 14 = 34\\n5. 34 + 4 = 38\\nTherefore, we reach the target number 38.\"}} </tool_call><|im_end|> bad case 3: Missing expression outputs. <|im_start|>assistant <tool_call> {\"name\": \"generate_response\", \"arguments\": {\"response\": \"In this case, one straightforward way to reach the target number 64 is by directly using the number 64 itself, since it matches the target exactly. No other calculations are needed.\"}} boxing the answer as \\boxed{{64}}.<|im_end|> However, tuning resolves these issues, as shown in the example below: Message 3 : decomposes the problem + emits a <tool_call> , and uses \\boxed{} to output key values Message 4 : decomposes the problem further, and reuses numbers in expressions. Token-level Visualization: These detailed logs are generated by Beast-Logger. See Beast-Logger Usage for more details.","title":"4.2 Case Study"},{"location":"en/example_frozenlake/","text":"1. Overview Frozen Lake is a classic reinforcement learning task from Gymnasium . In this environment, the agent is placed on a randomly generated frozen lake, which consists of safe ice ( _ ), dangerous holes (O), and a goal (G). The agent's position is marked as P. The goal is to navigate from the starting position P to the goal G while avoiding the holes. The agent can move up, down, left, or right, but due to the slippery nature of the ice, there is a probability of moving in an unintended direction. This example demonstrates how to create a trainable agent workflow to solve this navigation challenge. 2. Quick Start 2.1 Preparation Install the dependencies required for the Frozen Lake: pip install gymnasium [ toy_text ] 2.2 Start Training Use the provided configuration file to quickly start training: ajet --conf tutorial/example_frozenlake/frozenlake_easy.yaml --backbone = 'verl' To try a harder setting: ajet --conf tutorial/example_frozenlake/frozenlake_hard.yaml --backbone = 'verl' Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_frozenlake/frozenlake_easy.yaml --backbone = 'debug' --with-logview When `--backbone=debug`, Ray is disabled. You can use a VSCode `.vscode/launch.json` like below: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] } 3. Understand 3.1 Core Process This example packages a multi-step environment interaction loop into a trainable Workflow : The workflow resets the environment and renders the current grid as a text observation for the agent. The agent reads the observation and outputs one of Up | Down | Left | Right . The environment executes the action, returns the next observation and reward. The loop stops on success or when the max step limit is reached. 3.2 Configuration Details The key fields in tutorial/example_frozenlake/frozenlake_easy.yaml / frozenlake_hard.yaml are: ajet.rollout.user_workflow : entry point of the workflow class, set to tutorial.example_frozenlake.frozenlake->FrozenLakeWorkflow . ajet.rollout.multi_turn.max_steps : maximum steps per episode (also used by the agent). frozen_lake.frozen_lake_size : grid size (e.g. 4 for easy, 6 for hard). frozen_lake.is_slippery : whether the action may slip to unintended directions. 3.3 Code Map The FrozenLakeEnv class in tutorial/example_frozenlake/frozenlake.py wraps the Gymnasium Frozen Lake environment, mainly exposing the step and reset methods. The step method returns the next state (observation), reward, done flag, and additional info based on the agent's action. observation: The state of the lake after the agent moves, represented as a string, e.g.: _ _ G _ _ _ P O O reward: The reward received after each move. The agent receives 1 for reaching the goal G, otherwise 0. done: Boolean value. True if the agent reaches the goal or falls into a hole, otherwise False. info: Additional information. The reset method regenerates the lake environment based on user parameters. The FrozenLakeAgent class in tutorial/example_frozenlake/frozenlake.py implements the agent's decision logic, mainly through the step method, which takes the current environment observation as input and returns the chosen action. The core is a ReActAgent. class FrozenLakeAgent : def __init__ ( self , model : ModelTuner , max_steps : int = 20 ): self . agent = ReActAgent ( name = \"frozenlake_agent\" , sys_prompt = SYSTEM_PROMPT , model = model , formatter = DashScopeChatFormatter (), max_iters = 2 , ) # other initialization code async def step ( self , current_observation : str ) -> str : # Step 1: Build user prompt based on current_observation # Step 2: Call ReActAgent to get raw response # Step 3: Parse response and return action The FrozenLakeWorkflow class in tutorial/example_frozenlake/frozenlake.py integrates the environment and agent, mainly exposing the execute method. class FrozenLakeWorkflow ( Workflow ): async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # init agent and env # reset environment and get initial `observation_str` rewards = [] for _ in range ( self . max_steps ): action = await self . agent . step ( observation_str ) observation_str , reward , done , info = self . env . step ( action ) rewards . append ( reward ) if done : break return WorkflowOutput ( reward = sum ( rewards ), ) 3.4 Reward The per-episode reward is the sum of step rewards. In this FrozenLake setup, the agent gets +1 when reaching the goal, otherwise 0 . The workflow also returns metadata such as terminate_reason ( success , agent_error , max_steps_reached ) and step_count . 4. Results 4.1 Training Curve Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. Since the reward is sparse (only +1 when reaching the goal, otherwise 0 ), the rising reward curve directly reflects an increasing success rate \u2014the agent reaches the goal G more often. This improvement typically comes from two aspects: Better spatial reasoning : the agent learns to parse the grid and identify the relative positions. Safer path planning : it avoids falling into holes and takes more reliable routes toward the goal.","title":"Frozen Lake"},{"location":"en/example_frozenlake/#1-overview","text":"Frozen Lake is a classic reinforcement learning task from Gymnasium . In this environment, the agent is placed on a randomly generated frozen lake, which consists of safe ice ( _ ), dangerous holes (O), and a goal (G). The agent's position is marked as P. The goal is to navigate from the starting position P to the goal G while avoiding the holes. The agent can move up, down, left, or right, but due to the slippery nature of the ice, there is a probability of moving in an unintended direction. This example demonstrates how to create a trainable agent workflow to solve this navigation challenge.","title":"1. Overview"},{"location":"en/example_frozenlake/#2-quick-start","text":"","title":"2. Quick Start"},{"location":"en/example_frozenlake/#21-preparation","text":"Install the dependencies required for the Frozen Lake: pip install gymnasium [ toy_text ]","title":"2.1 Preparation"},{"location":"en/example_frozenlake/#22-start-training","text":"Use the provided configuration file to quickly start training: ajet --conf tutorial/example_frozenlake/frozenlake_easy.yaml --backbone = 'verl' To try a harder setting: ajet --conf tutorial/example_frozenlake/frozenlake_hard.yaml --backbone = 'verl' Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_frozenlake/frozenlake_easy.yaml --backbone = 'debug' --with-logview When `--backbone=debug`, Ray is disabled. You can use a VSCode `.vscode/launch.json` like below: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] }","title":"2.2 Start Training"},{"location":"en/example_frozenlake/#3-understand","text":"","title":"3. Understand"},{"location":"en/example_frozenlake/#31-core-process","text":"This example packages a multi-step environment interaction loop into a trainable Workflow : The workflow resets the environment and renders the current grid as a text observation for the agent. The agent reads the observation and outputs one of Up | Down | Left | Right . The environment executes the action, returns the next observation and reward. The loop stops on success or when the max step limit is reached.","title":"3.1 Core Process"},{"location":"en/example_frozenlake/#32-configuration-details","text":"The key fields in tutorial/example_frozenlake/frozenlake_easy.yaml / frozenlake_hard.yaml are: ajet.rollout.user_workflow : entry point of the workflow class, set to tutorial.example_frozenlake.frozenlake->FrozenLakeWorkflow . ajet.rollout.multi_turn.max_steps : maximum steps per episode (also used by the agent). frozen_lake.frozen_lake_size : grid size (e.g. 4 for easy, 6 for hard). frozen_lake.is_slippery : whether the action may slip to unintended directions.","title":"3.2 Configuration Details"},{"location":"en/example_frozenlake/#33-code-map","text":"The FrozenLakeEnv class in tutorial/example_frozenlake/frozenlake.py wraps the Gymnasium Frozen Lake environment, mainly exposing the step and reset methods. The step method returns the next state (observation), reward, done flag, and additional info based on the agent's action. observation: The state of the lake after the agent moves, represented as a string, e.g.: _ _ G _ _ _ P O O reward: The reward received after each move. The agent receives 1 for reaching the goal G, otherwise 0. done: Boolean value. True if the agent reaches the goal or falls into a hole, otherwise False. info: Additional information. The reset method regenerates the lake environment based on user parameters. The FrozenLakeAgent class in tutorial/example_frozenlake/frozenlake.py implements the agent's decision logic, mainly through the step method, which takes the current environment observation as input and returns the chosen action. The core is a ReActAgent. class FrozenLakeAgent : def __init__ ( self , model : ModelTuner , max_steps : int = 20 ): self . agent = ReActAgent ( name = \"frozenlake_agent\" , sys_prompt = SYSTEM_PROMPT , model = model , formatter = DashScopeChatFormatter (), max_iters = 2 , ) # other initialization code async def step ( self , current_observation : str ) -> str : # Step 1: Build user prompt based on current_observation # Step 2: Call ReActAgent to get raw response # Step 3: Parse response and return action The FrozenLakeWorkflow class in tutorial/example_frozenlake/frozenlake.py integrates the environment and agent, mainly exposing the execute method. class FrozenLakeWorkflow ( Workflow ): async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # init agent and env # reset environment and get initial `observation_str` rewards = [] for _ in range ( self . max_steps ): action = await self . agent . step ( observation_str ) observation_str , reward , done , info = self . env . step ( action ) rewards . append ( reward ) if done : break return WorkflowOutput ( reward = sum ( rewards ), )","title":"3.3 Code Map"},{"location":"en/example_frozenlake/#34-reward","text":"The per-episode reward is the sum of step rewards. In this FrozenLake setup, the agent gets +1 when reaching the goal, otherwise 0 . The workflow also returns metadata such as terminate_reason ( success , agent_error , max_steps_reached ) and step_count .","title":"3.4 Reward"},{"location":"en/example_frozenlake/#4-results","text":"","title":"4. Results"},{"location":"en/example_frozenlake/#41-training-curve","text":"Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. Since the reward is sparse (only +1 when reaching the goal, otherwise 0 ), the rising reward curve directly reflects an increasing success rate \u2014the agent reaches the goal G more often. This improvement typically comes from two aspects: Better spatial reasoning : the agent learns to parse the grid and identify the relative positions. Safer path planning : it avoids falling into holes and takes more reliable routes toward the goal.","title":"4.1 Training Curve"},{"location":"en/example_learning_to_ask/","text":"Train an agent to ask the next best question (instead of answering directly). Rewards come from an LLM-as-a-judge that scores whether the question is helpful and relevant. 1. Overview In Learning to Ask , each training sample is a short doctor\u2013patient chat history . The agent outputs one next question the doctor should ask next (optionally with multiple-choice answers), rather than giving diagnosis or treatment. Why \"Learning to Ask\" matters. Left: LLM gives a diagnosis with too little information. Right: LLM asks clear follow-up questions before concluding, which feels more reassuring. This tutorial is organized in two steps: 1) Run it : start training with the default YAML config. 2) Understand & customize : dataset preprocessing, workflow (ExampleLearn2Ask), and reward (reward_fn + llm_reward). 2. Quick Start 2.1 Preparation Download the RealMedConv dataset from HuggingFace and place files in: data/realmedconv Then preprocess it: export DASHSCOPE_API_KEY = your_api_key cd tutorial/example_learn2ask/data_preprocess ./run_process.sh data/realmedconv After preprocessing, you should have: train.jsonl and test.jsonl \u3002 2.2 Start Training ajet --conf tutorial/example_learn2ask/learn2ask.yaml --backbone = 'verl' # or ajet --conf tutorial/example_learn2ask/learn2ask.yaml --backbone = 'trinity' --with-ray Quick Debugging (Optional) Run Ray locally without enabling it for faster iteration: bash ajet --conf tutorial/example_learn2ask/learn2ask.yaml --backbone='debug' --with-logview If the results are incorrect, the quickest troubleshooting points include: whether the data path exists, whether an API key has been set if judge requires it, and whether the workflow classpath in `user_workflow` matches the location of your code. 3. Understand 3.1 What happens each step This tutorial trains a model to ask the next best question from a short doctor\u2013patient chat history. Concretely, each training step takes one conversation context from train.jsonl , asks the agent to generate exactly one follow-up question (optionally with answer options), and then uses an LLM judge to score whether that question is useful and relevant. AgentJet uses this score as the reward signal to update the policy, so the model gradually learns to ask better questions instead of answering directly. 3.2 YAML Configuration The whole example is \u201cwired\u201d in the YAML and implemented in one file. In the YAML, task_reader provides the dataset split, rollout.user_workflow tells AgentJet which workflow to run for each sample, and task_judge provides the reward entry that wraps the LLM judge. The model section decides which pretrained backbone you start from. ajet : task_reader : type : dataset_file # train_path: data/realmedconv/train.jsonl # test_path: data/realmedconv/test.jsonl rollout : # For each sample: conversation context -> one next question user_workflow : tutorial.example_learn2ask.learn2ask->ExampleLearn2Ask task_judge : # Reward function used by the trainer (internally calls the LLM judge) # judge_protocol: tutorial.example_learn2ask.learn2ask->reward_fn model : # pretrained backbone to start from # path: /path/to/your/model 3.3 Code Map At the code level, everything is implemented in tutorial/example_learn2ask/learn2ask.py : ExampleLearn2Ask defines the workflow: how the dialogue context is converted into the agent\u2019s prompt/input, and what output format is expected (one follow-up question, optionally with choices). reward_fn defines how to convert the judge\u2019s feedback into a scalar reward used for training. We provide two implmentations of the agent based on AgentScope and langchain: AgentScope Langchain # create the agent self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), formatter = DashScopeChatFormatter (), toolkit = None , memory = InMemoryMemory (), max_iters = 1 , ) self . agent . set_console_output_enabled ( False ) # convert the messages to agent scope format and send to the agent msg = [ # Msg(\"system\", system_prompt, role=\"system\"), * [ Msg ( name = x [ \"role\" ], content = x [ \"content\" ], role = x [ \"role\" ]) for x in messages ] ] result = await self . agent . reply ( msg ) if isinstance ( result . content , str ): response = result . content elif isinstance ( result . content , list ): response = result . content [ 0 ][ \"text\" ] # type: ignore else : raise NotImplementedError ( f \"do not know how to handle { type ( result . content ) } \" ) reward = await reward_fn_with_semaphore ( msg , response , truth_action , truth_info ) return WorkflowOutput ( reward = reward ) # get the trainable llm llm_info = tuner . as_oai_baseurl_apikey () # create the langchain agent llm = ChatOpenAI ( base_url = llm_info . base_url , api_key = lambda : llm_info . api_key , ) agent = create_agent ( model = llm , system_prompt = system_prompt , ) # build messages and send to the agent msg = [ { \"role\" : x [ \"role\" ], \"content\" : x [ \"content\" ]} for x in messages ] result = agent . invoke ({ \"messages\" : msg , # type: ignore }) response = result [ \"messages\" ][ - 1 ] . content reward = await reward_fn_with_semaphore ( msg , response , truth_action , truth_info ) return WorkflowOutput ( reward = reward ) 3.4 Reward llm_reward is the LLM-as-a-judge called inside reward_fn to score the model output. The evaluation follows these rules: It only evaluates the doctor\u2019s last message (doctor\u2019s last message), and does not consider earlier doctor turns. It outputs two scores: Format Score + Content Score (scored separately, then combined by reward_fn into the training reward). Format Score : scored by the number of questions in the doctor\u2019s last message - 1.0: exactly one question , or correctly output <stop /> when no question is needed - 0.5: two questions - 0.0: three or more questions Content Score : scored by whether the question targets the \u201cmissing information\u201d in Reference Information (i.e., information the doctor does not yet know) - 1.0: the question directly asks about an item in Reference Information , or correctly end the conversation when no more information is needed - 0.1: the question is too generic (a general question that could apply to almost any symptom) - 0.0: the question is irrelevant to the missing items in Reference Information - Additionally: ambiguous or uninformative questions are treated as low-quality (e.g., unclear references), and will typically receive a score of 0 or close to 0 4. Results 4.1 Training Curve Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. This training curve reflects the trend of the reward over time. The reward includes: Format Score (whether the model asks exactly one question) and Content Score (whether it directly asks about the missing key information in Reference Information ). Therefore, a rising curve indicates the model gradually learns to produce one clear question and to hit missing information more often, rather than asking generic questions or producing ambiguous/uninformative questions. 4.2 Case Study Before training User: Bitten by a midge Agent: Has irritation subsided past initial acute swelling phase afterwards unnoticed scratched overly causing secondary bacterial dermatitis chance affecting skin rendering needed topical antibiotic application options among azelaic acid cream!? The agent provided a slightly related but completely unreasonable answer and failed to form any effective question. During training (~50%) User: Bitten by a midge Agent: Have these itchy bumps appeared soon after getting bitten?(A.) Occasionally immediately afterward ; B fkknore \u00d7later throughout day )/) The agent asked a more reasonable follow-up, attempting to gather symptoms and offering options. After training User: Bitten by a midge Agent: Has itching or reddening appeared around this bite site recently without healing despite scratching frequently?(A,Yes indeed worse every evening B,No pain but small swollen spots persist day after incident/) The question becomes more precise and informative, guiding the user to provide clinically relevant details. To learn more about the task and results on larger models, refer to Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs .","title":"Learning to Ask"},{"location":"en/example_learning_to_ask/#1-overview","text":"In Learning to Ask , each training sample is a short doctor\u2013patient chat history . The agent outputs one next question the doctor should ask next (optionally with multiple-choice answers), rather than giving diagnosis or treatment. Why \"Learning to Ask\" matters. Left: LLM gives a diagnosis with too little information. Right: LLM asks clear follow-up questions before concluding, which feels more reassuring. This tutorial is organized in two steps: 1) Run it : start training with the default YAML config. 2) Understand & customize : dataset preprocessing, workflow (ExampleLearn2Ask), and reward (reward_fn + llm_reward).","title":"1. Overview"},{"location":"en/example_learning_to_ask/#2-quick-start","text":"","title":"2. Quick Start"},{"location":"en/example_learning_to_ask/#21-preparation","text":"Download the RealMedConv dataset from HuggingFace and place files in: data/realmedconv Then preprocess it: export DASHSCOPE_API_KEY = your_api_key cd tutorial/example_learn2ask/data_preprocess ./run_process.sh data/realmedconv After preprocessing, you should have: train.jsonl and test.jsonl \u3002","title":"2.1 Preparation"},{"location":"en/example_learning_to_ask/#22-start-training","text":"ajet --conf tutorial/example_learn2ask/learn2ask.yaml --backbone = 'verl' # or ajet --conf tutorial/example_learn2ask/learn2ask.yaml --backbone = 'trinity' --with-ray Quick Debugging (Optional) Run Ray locally without enabling it for faster iteration: bash ajet --conf tutorial/example_learn2ask/learn2ask.yaml --backbone='debug' --with-logview If the results are incorrect, the quickest troubleshooting points include: whether the data path exists, whether an API key has been set if judge requires it, and whether the workflow classpath in `user_workflow` matches the location of your code.","title":"2.2 Start Training"},{"location":"en/example_learning_to_ask/#3-understand","text":"","title":"3. Understand"},{"location":"en/example_learning_to_ask/#31-what-happens-each-step","text":"This tutorial trains a model to ask the next best question from a short doctor\u2013patient chat history. Concretely, each training step takes one conversation context from train.jsonl , asks the agent to generate exactly one follow-up question (optionally with answer options), and then uses an LLM judge to score whether that question is useful and relevant. AgentJet uses this score as the reward signal to update the policy, so the model gradually learns to ask better questions instead of answering directly.","title":"3.1 What happens each step"},{"location":"en/example_learning_to_ask/#32-yaml-configuration","text":"The whole example is \u201cwired\u201d in the YAML and implemented in one file. In the YAML, task_reader provides the dataset split, rollout.user_workflow tells AgentJet which workflow to run for each sample, and task_judge provides the reward entry that wraps the LLM judge. The model section decides which pretrained backbone you start from. ajet : task_reader : type : dataset_file # train_path: data/realmedconv/train.jsonl # test_path: data/realmedconv/test.jsonl rollout : # For each sample: conversation context -> one next question user_workflow : tutorial.example_learn2ask.learn2ask->ExampleLearn2Ask task_judge : # Reward function used by the trainer (internally calls the LLM judge) # judge_protocol: tutorial.example_learn2ask.learn2ask->reward_fn model : # pretrained backbone to start from # path: /path/to/your/model","title":"3.2 YAML Configuration"},{"location":"en/example_learning_to_ask/#33-code-map","text":"At the code level, everything is implemented in tutorial/example_learn2ask/learn2ask.py : ExampleLearn2Ask defines the workflow: how the dialogue context is converted into the agent\u2019s prompt/input, and what output format is expected (one follow-up question, optionally with choices). reward_fn defines how to convert the judge\u2019s feedback into a scalar reward used for training. We provide two implmentations of the agent based on AgentScope and langchain: AgentScope Langchain # create the agent self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), formatter = DashScopeChatFormatter (), toolkit = None , memory = InMemoryMemory (), max_iters = 1 , ) self . agent . set_console_output_enabled ( False ) # convert the messages to agent scope format and send to the agent msg = [ # Msg(\"system\", system_prompt, role=\"system\"), * [ Msg ( name = x [ \"role\" ], content = x [ \"content\" ], role = x [ \"role\" ]) for x in messages ] ] result = await self . agent . reply ( msg ) if isinstance ( result . content , str ): response = result . content elif isinstance ( result . content , list ): response = result . content [ 0 ][ \"text\" ] # type: ignore else : raise NotImplementedError ( f \"do not know how to handle { type ( result . content ) } \" ) reward = await reward_fn_with_semaphore ( msg , response , truth_action , truth_info ) return WorkflowOutput ( reward = reward ) # get the trainable llm llm_info = tuner . as_oai_baseurl_apikey () # create the langchain agent llm = ChatOpenAI ( base_url = llm_info . base_url , api_key = lambda : llm_info . api_key , ) agent = create_agent ( model = llm , system_prompt = system_prompt , ) # build messages and send to the agent msg = [ { \"role\" : x [ \"role\" ], \"content\" : x [ \"content\" ]} for x in messages ] result = agent . invoke ({ \"messages\" : msg , # type: ignore }) response = result [ \"messages\" ][ - 1 ] . content reward = await reward_fn_with_semaphore ( msg , response , truth_action , truth_info ) return WorkflowOutput ( reward = reward )","title":"3.3 Code Map"},{"location":"en/example_learning_to_ask/#34-reward","text":"llm_reward is the LLM-as-a-judge called inside reward_fn to score the model output. The evaluation follows these rules: It only evaluates the doctor\u2019s last message (doctor\u2019s last message), and does not consider earlier doctor turns. It outputs two scores: Format Score + Content Score (scored separately, then combined by reward_fn into the training reward). Format Score : scored by the number of questions in the doctor\u2019s last message - 1.0: exactly one question , or correctly output <stop /> when no question is needed - 0.5: two questions - 0.0: three or more questions Content Score : scored by whether the question targets the \u201cmissing information\u201d in Reference Information (i.e., information the doctor does not yet know) - 1.0: the question directly asks about an item in Reference Information , or correctly end the conversation when no more information is needed - 0.1: the question is too generic (a general question that could apply to almost any symptom) - 0.0: the question is irrelevant to the missing items in Reference Information - Additionally: ambiguous or uninformative questions are treated as low-quality (e.g., unclear references), and will typically receive a score of 0 or close to 0","title":"3.4 Reward"},{"location":"en/example_learning_to_ask/#4-results","text":"","title":"4. Results"},{"location":"en/example_learning_to_ask/#41-training-curve","text":"Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. This training curve reflects the trend of the reward over time. The reward includes: Format Score (whether the model asks exactly one question) and Content Score (whether it directly asks about the missing key information in Reference Information ). Therefore, a rising curve indicates the model gradually learns to produce one clear question and to hit missing information more often, rather than asking generic questions or producing ambiguous/uninformative questions.","title":"4.1 Training Curve"},{"location":"en/example_learning_to_ask/#42-case-study","text":"Before training User: Bitten by a midge Agent: Has irritation subsided past initial acute swelling phase afterwards unnoticed scratched overly causing secondary bacterial dermatitis chance affecting skin rendering needed topical antibiotic application options among azelaic acid cream!? The agent provided a slightly related but completely unreasonable answer and failed to form any effective question. During training (~50%) User: Bitten by a midge Agent: Have these itchy bumps appeared soon after getting bitten?(A.) Occasionally immediately afterward ; B fkknore \u00d7later throughout day )/) The agent asked a more reasonable follow-up, attempting to gather symptoms and offering options. After training User: Bitten by a midge Agent: Has itching or reddening appeared around this bite site recently without healing despite scratching frequently?(A,Yes indeed worse every evening B,No pain but small swollen spots persist day after incident/) The question becomes more precise and informative, guiding the user to provide clinically relevant details. To learn more about the task and results on larger models, refer to Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs .","title":"4.2 Case Study"},{"location":"en/example_math_agent/","text":"Train a tool-using Math Agent (ReAct + Python executor) to solve GSM8K-style math problems. Rewards come from a judge that checks final-answer correctness. Overview In Math Agent , each training sample is a math word problem (e.g., GSM8K). The agent learns to reason step by step (ReAct-style), call a Python tool when computation is needed, and produce a final answer that matches the reference. This tutorial is organized into the following sections: Run this tutorial : Download the dataset and start training with the default YAML config. Understand & customize : Read the workflow and the judge/reward logic. Training Curve : Compare the training curlve. Quick Start Prepare Dataset Download the openai/gsm8k dataset: python scripts/download_dataset.py --target = openai/gsm8k --path = /the/path/to/store/dataset Start Training # (optional) recommended cleanup before training # ajet --kill=\"python|ray|vllm\" ajet --conf tutorial/example_math_agent/math_agent.yaml --backbone = 'verl' Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_math_agent/math_agent.yaml --backbone = 'debug' --with-logview When --backbone=debug , Ray is disabled. You can use a VSCode launch config: .vscode/launch.json { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] } Understanding the Training Pipeline Pipeline Abstraction Training Step Flow Load one problem Load a math problem from the dataset via `task_reader`. Run the Workflow Build the prompt, let the ReActAgent call Python tools, and extract the final answer. Return result as `WorkflowOutput` Return `WorkflowOutput(reward=None, metadata={\"final_answer\": final_answer})`. (reward=None because we want to compute reward outside the workflow) Run the judge Compare `final_answer` with reference, compute `raw_reward` and `is_success`. YAML Configuration Most wiring happens in tutorial/example_math_agent/math_agent.yaml : AgentScope OpenAI Raw HTTP langchain math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent_oai_sdk->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent_raw_http->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent_langchain->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH Field Description task_reader Where tasks come from user_workflow Which workflow runs per sample judge_protocol Which judge computes rewards model.path Pretrained model to fine-tune Code Walkthrough Workflow: tutorial/example_math_agent/math_agent.py AgentScope OpenAI Raw HTTP Langchain Workflow Sketch self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = model_tuner , # trainer-managed model wrapper formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), ) msg = Msg ( \"user\" , init_messages [ 0 ][ \"content\" ], role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # IMPORTANT: provide final answer to the judge via WorkflowOutput metadata return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer }) Workflow Sketch client = tuner . as_raw_openai_sdk_client () # call 1: get response with tool call messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] reply_message : ChatCompletion = await client . chat . completions . create ( messages = messages , tools = self . available_functions ) if ( reply_message . choices [ 0 ] . message . content ): messages . append ({ \"role\" : \"assistant\" , \"content\" : reply_message . choices [ 0 ] . message . content }) # If the model called a tool if ( reply_message . choices [ 0 ] . message ) and ( reply_message . choices [ 0 ] . message . tool_calls ): tool_calls : list [ ChatCompletionMessageToolCall ] = reply_message . choices [ 0 ] . message . tool_calls for tool_call in tool_calls : if tool_call . function . name == \"execute_python_code\" : arguments = json . loads ( tool_call . function . arguments ) def sync_wrapper (): import subprocess import sys process = subprocess . run ( [ sys . executable , \"-c\" , arguments [ \"code\" ]], timeout = arguments . get ( \"timeout\" , 300 ), capture_output = True , text = True ) return process . stdout result = await asyncio . to_thread ( sync_wrapper ) tool_result_message = { \"role\" : \"tool\" , \"tool_call_id\" : tool_call . id , \"name\" : tool_call . function . name , \"content\" : json . dumps ({ \"return_code\" : str ( result ), }) } messages . append ( tool_result_message ) # Step 3: Make a follow-up API call with the tool result final_response : ChatCompletion = await client . chat . completions . create ( messages = messages , ) final_stage_response = final_response . choices [ 0 ] . message . content else : final_stage_response = reply_message . choices [ 0 ] . message . content return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_stage_response }) raw http url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key # take out query query = workflow_task . task . main_query messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] # use raw http requests (non-streaming) to get response response = requests . post ( f \" { base_url } /chat/completions\" , json = { \"model\" : \"fill_whatever_model\" , # Of course, this `model` field will be ignored. \"messages\" : messages , }, headers = { \"Authorization\" : f \"Bearer { api_key } \" } ) final_answer = response . json ()[ 'choices' ][ 0 ][ 'message' ][ 'content' ] return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer }) langchain # tuner to api key url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key from langchain_openai import ChatOpenAI llm = ChatOpenAI ( base_url = base_url , api_key = lambda : api_key , ) agent = create_agent ( model = llm , system_prompt = self . system_prompt , ) # take out query query = workflow_task . task . main_query response = agent . invoke ({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : query } ], }) final_answer = response [ 'messages' ][ - 1 ] . content return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer }) Important User should put all elements necessary for reward computation in WorkflowOutput.metadata , so the judge can use them. In this specific case, final_answer is that key element. Reward Computation The judge receives: Object Contains workflow_task Task info; reference answer from metadata workflow_output Workflow result; final answer from metadata[\"final_answer\"] Extending the Judge If you observe issues like \"almost solved but messed up tool-call formatting\", you can extend the judge to add: Format penalty (invalid <tool_call> ) Behavior penalty (tool called but no print ) Keep answer correctness as the primary signal YAML Configuration Most wiring happens in tutorial/example_math_agent/math_agent.yaml : math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH Field Description task_reader Where tasks come from user_workflow Which workflow runs per sample judge_protocol Which judge computes rewards model.path Pretrained model to fine-tune Results Training Curve Visualization Training curves are generated by SwanLab. See Visualization Tools for setup. Interpretation: As training progresses, reward increases. This usually means the agent becomes more stable at: Using tools when needed : Correctly emitting <tool_call> and calling execute_python_code Producing reliable answers : Using tool output to produce final answers aligned with reference Case Study: Tool Discipline Improvement Before training, the agent may solve many problems but often fails at tool-call discipline : Bad Cases Good Case (After Tuning) # bad case 1: forgot to print the result in python code <tool_call> {\"name\": \"execute_python_code\", \"arguments\": {\"code\": \"... height_difference\"}} </tool_call> # bad case 2: too impatient \u2014 outputs final answer without waiting for tool result <tool_call> {\"name\": \"execute_python_code\", ...} </tool_call> <tool_call> {\"name\": \"generate_response\", \"arguments\": {\"response\": \"... \\\\boxed{48} ...\"}} </tool_call> These failures are not because the model \"can't do math\", but because it does not close the loop by incorporating the tool execution result. After tuning, the agent follows a clean 3-stage pattern: Message 3 (assistant) : Decomposes problem + emits <tool_call> with print(...) Message 4 (tool_response) : Tool returns execution results Message 5 (assistant) : Reads stdout and produces final answer Token-level Visualization The colored blocks show token-level sequence visualization from Beast-Logger : Yellow tokens : Excluded from loss computation Blue tokens : Participate in loss computation (light to dark = high to low logprob) Next Steps Werewolves Explore multi-agent collaborative training. AppWorld Train agents for real-world app interactions. Visualization Monitor and analyze your training progress.","title":"Math Agent"},{"location":"en/example_math_agent/#overview","text":"In Math Agent , each training sample is a math word problem (e.g., GSM8K). The agent learns to reason step by step (ReAct-style), call a Python tool when computation is needed, and produce a final answer that matches the reference. This tutorial is organized into the following sections: Run this tutorial : Download the dataset and start training with the default YAML config. Understand & customize : Read the workflow and the judge/reward logic. Training Curve : Compare the training curlve.","title":"Overview"},{"location":"en/example_math_agent/#quick-start","text":"","title":"Quick Start"},{"location":"en/example_math_agent/#prepare-dataset","text":"Download the openai/gsm8k dataset: python scripts/download_dataset.py --target = openai/gsm8k --path = /the/path/to/store/dataset","title":"Prepare Dataset"},{"location":"en/example_math_agent/#start-training","text":"# (optional) recommended cleanup before training # ajet --kill=\"python|ray|vllm\" ajet --conf tutorial/example_math_agent/math_agent.yaml --backbone = 'verl' Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_math_agent/math_agent.yaml --backbone = 'debug' --with-logview When --backbone=debug , Ray is disabled. You can use a VSCode launch config: .vscode/launch.json { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] }","title":"Start Training"},{"location":"en/example_math_agent/#explain","text":"","title":"Understanding the Training Pipeline"},{"location":"en/example_math_agent/#pipeline-abstraction","text":"Training Step Flow Load one problem Load a math problem from the dataset via `task_reader`. Run the Workflow Build the prompt, let the ReActAgent call Python tools, and extract the final answer. Return result as `WorkflowOutput` Return `WorkflowOutput(reward=None, metadata={\"final_answer\": final_answer})`. (reward=None because we want to compute reward outside the workflow) Run the judge Compare `final_answer` with reference, compute `raw_reward` and `is_success`.","title":"Pipeline Abstraction"},{"location":"en/example_math_agent/#yaml-configuration","text":"Most wiring happens in tutorial/example_math_agent/math_agent.yaml : AgentScope OpenAI Raw HTTP langchain math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent_oai_sdk->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent_raw_http->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent_langchain->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH Field Description task_reader Where tasks come from user_workflow Which workflow runs per sample judge_protocol Which judge computes rewards model.path Pretrained model to fine-tune","title":"YAML Configuration"},{"location":"en/example_math_agent/#code-walkthrough","text":"Workflow: tutorial/example_math_agent/math_agent.py AgentScope OpenAI Raw HTTP Langchain Workflow Sketch self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = model_tuner , # trainer-managed model wrapper formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), ) msg = Msg ( \"user\" , init_messages [ 0 ][ \"content\" ], role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # IMPORTANT: provide final answer to the judge via WorkflowOutput metadata return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer }) Workflow Sketch client = tuner . as_raw_openai_sdk_client () # call 1: get response with tool call messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] reply_message : ChatCompletion = await client . chat . completions . create ( messages = messages , tools = self . available_functions ) if ( reply_message . choices [ 0 ] . message . content ): messages . append ({ \"role\" : \"assistant\" , \"content\" : reply_message . choices [ 0 ] . message . content }) # If the model called a tool if ( reply_message . choices [ 0 ] . message ) and ( reply_message . choices [ 0 ] . message . tool_calls ): tool_calls : list [ ChatCompletionMessageToolCall ] = reply_message . choices [ 0 ] . message . tool_calls for tool_call in tool_calls : if tool_call . function . name == \"execute_python_code\" : arguments = json . loads ( tool_call . function . arguments ) def sync_wrapper (): import subprocess import sys process = subprocess . run ( [ sys . executable , \"-c\" , arguments [ \"code\" ]], timeout = arguments . get ( \"timeout\" , 300 ), capture_output = True , text = True ) return process . stdout result = await asyncio . to_thread ( sync_wrapper ) tool_result_message = { \"role\" : \"tool\" , \"tool_call_id\" : tool_call . id , \"name\" : tool_call . function . name , \"content\" : json . dumps ({ \"return_code\" : str ( result ), }) } messages . append ( tool_result_message ) # Step 3: Make a follow-up API call with the tool result final_response : ChatCompletion = await client . chat . completions . create ( messages = messages , ) final_stage_response = final_response . choices [ 0 ] . message . content else : final_stage_response = reply_message . choices [ 0 ] . message . content return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_stage_response }) raw http url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key # take out query query = workflow_task . task . main_query messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] # use raw http requests (non-streaming) to get response response = requests . post ( f \" { base_url } /chat/completions\" , json = { \"model\" : \"fill_whatever_model\" , # Of course, this `model` field will be ignored. \"messages\" : messages , }, headers = { \"Authorization\" : f \"Bearer { api_key } \" } ) final_answer = response . json ()[ 'choices' ][ 0 ][ 'message' ][ 'content' ] return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer }) langchain # tuner to api key url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key from langchain_openai import ChatOpenAI llm = ChatOpenAI ( base_url = base_url , api_key = lambda : api_key , ) agent = create_agent ( model = llm , system_prompt = self . system_prompt , ) # take out query query = workflow_task . task . main_query response = agent . invoke ({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : query } ], }) final_answer = response [ 'messages' ][ - 1 ] . content return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer }) Important User should put all elements necessary for reward computation in WorkflowOutput.metadata , so the judge can use them. In this specific case, final_answer is that key element.","title":"Code Walkthrough"},{"location":"en/example_math_agent/#reward-computation","text":"The judge receives: Object Contains workflow_task Task info; reference answer from metadata workflow_output Workflow result; final answer from metadata[\"final_answer\"] Extending the Judge If you observe issues like \"almost solved but messed up tool-call formatting\", you can extend the judge to add: Format penalty (invalid <tool_call> ) Behavior penalty (tool called but no print ) Keep answer correctness as the primary signal","title":"Reward Computation"},{"location":"en/example_math_agent/#yaml-configuration_1","text":"Most wiring happens in tutorial/example_math_agent/math_agent.yaml : math_agent.yaml ajet : task_reader : type : huggingface_dat_repo # also supports: dataset_file / env_service rollout : user_workflow : tutorial.example_math_agent.math_agent->ExampleMathLearn task_judge : judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge model : path : YOUR_MODEL_PATH Field Description task_reader Where tasks come from user_workflow Which workflow runs per sample judge_protocol Which judge computes rewards model.path Pretrained model to fine-tune","title":"YAML Configuration"},{"location":"en/example_math_agent/#culve","text":"","title":"Results"},{"location":"en/example_math_agent/#training-curve","text":"Visualization Training curves are generated by SwanLab. See Visualization Tools for setup. Interpretation: As training progresses, reward increases. This usually means the agent becomes more stable at: Using tools when needed : Correctly emitting <tool_call> and calling execute_python_code Producing reliable answers : Using tool output to produce final answers aligned with reference","title":"Training Curve"},{"location":"en/example_math_agent/#case-study-tool-discipline-improvement","text":"Before training, the agent may solve many problems but often fails at tool-call discipline : Bad Cases Good Case (After Tuning) # bad case 1: forgot to print the result in python code <tool_call> {\"name\": \"execute_python_code\", \"arguments\": {\"code\": \"... height_difference\"}} </tool_call> # bad case 2: too impatient \u2014 outputs final answer without waiting for tool result <tool_call> {\"name\": \"execute_python_code\", ...} </tool_call> <tool_call> {\"name\": \"generate_response\", \"arguments\": {\"response\": \"... \\\\boxed{48} ...\"}} </tool_call> These failures are not because the model \"can't do math\", but because it does not close the loop by incorporating the tool execution result. After tuning, the agent follows a clean 3-stage pattern: Message 3 (assistant) : Decomposes problem + emits <tool_call> with print(...) Message 4 (tool_response) : Tool returns execution results Message 5 (assistant) : Reads stdout and produces final answer Token-level Visualization The colored blocks show token-level sequence visualization from Beast-Logger : Yellow tokens : Excluded from loss computation Blue tokens : Participate in loss computation (light to dark = high to low logprob)","title":"Case Study: Tool Discipline Improvement"},{"location":"en/example_math_agent/#next-steps","text":"","title":"Next Steps"},{"location":"en/example_tracing_feedback_loop/","text":"AgentJet allows you to recycle the chat logs generated during an Agent's execution and continuously improve the Agent through iterative training, which we call Tracing-Feedback Training . It provides features Loading tracing log from agentscope studio database Converting log into formatted data Filtering high-quality samples with custom rubrics/filters Packing samples into datasets for iterative training In the next section, we will demonstrate how to improve an Agent using Tracing-Feedback Training. AgentScope & Studio Version Compatibility It is recommended to use matched versions: AgentScope (v1.0.7) Studio (23eb7c0b1185486d1baca36aea0ce8b85ea9de48) Setup To use tracing logs for training, you are expected to already have an agent built with agentscope running in agentscope-studio for some time (usually in production), which means you have Written your agent with agentscope . Enabled tracing module following the doc . Deployed your agent and collected the database. By default, agentscope-studio will store the tracing logs in ~/AgentScope-Studio/database.sqlite , containing all recorded dialogues between the user and the agent. We have prepared a demo agent in tutorials/example_feedback_tracing/agent_deployed.py . You can simulate the tracing log with it and get the database file. Start Tracing-Feedback Training Once we have the log ( database.sqlite ), we can train a new Agent with Tracing-Feedback Training. Set the ajet.task_reader.type parameter to tracing in the configuration file to enable tracing-feedback mode. Configure the ajet.task_reader.feedback_tracing section with the database path and filtering options. Configure other training parameters and Rewards as you would in a normal training workflow. ajet : # ... task_reader : # use tracing log as tasks type : tracing feedback_tracing : # path to the database base_url : ./tutorial/example_feedback_tracing/database.sqlite # path where the module will write cache train_output_path : ./tutorial/example_feedback_tracing/tasks.jsonl # the model used in filters alien_llm_model : qwen3-235b-a22b-instruct-2507 alien_llm_response_length : 2048 # filters filters : # the default filter llm_evaluate - type : llm_evaluate enabled : true params : # define your rubrics to drop any bad-quality tasks custom_rubrics : | 1. Check the answer and drop the task if it does not answer or answer is wrong. 2. Consider a response is invalid if it does not wrap the final answer in \\boxed{}. # LLM temperature temperature : 0.5 # print debug log print_reason : false max_thread : 16 When everything is ready, start the training with launcher.py . ajet --conf tutorial/example_feedback_tracing/example_feedback_tracing.yaml --backbone = 'verl' # or ajet --conf tutorial/example_feedback_tracing/example_feedback_tracing.yaml --backbone = 'trinity' --with-ray After training, we can now deploy the new Agent into production and collect new logs. This workflow enables continuous improvement through iterative tracing-feedback training. Customize Filter The module provides Filter to select high-quality samples from logs for training. Users are allowed to customize the specific rubrics of their own tasks. To write rubrics, edit the configuration file: ajet : # ... task_reader : # ... feedback_tracing : # ... filters : - type : llm_evaluate enabled : true # enable the filter params : # define your rubrics custom_rubrics : | 1. Check the answer and drop the task if it does not answer or answer is wrong. 2. Consider a response is invalid if it does not wrap the final answer in \\boxed{}. temperature : 0.5 print_reason : false max_thread : 16","title":"Tracing Feedback"},{"location":"en/example_tracing_feedback_loop/#setup","text":"To use tracing logs for training, you are expected to already have an agent built with agentscope running in agentscope-studio for some time (usually in production), which means you have Written your agent with agentscope . Enabled tracing module following the doc . Deployed your agent and collected the database. By default, agentscope-studio will store the tracing logs in ~/AgentScope-Studio/database.sqlite , containing all recorded dialogues between the user and the agent. We have prepared a demo agent in tutorials/example_feedback_tracing/agent_deployed.py . You can simulate the tracing log with it and get the database file.","title":"Setup"},{"location":"en/example_tracing_feedback_loop/#start-tracing-feedback-training","text":"Once we have the log ( database.sqlite ), we can train a new Agent with Tracing-Feedback Training. Set the ajet.task_reader.type parameter to tracing in the configuration file to enable tracing-feedback mode. Configure the ajet.task_reader.feedback_tracing section with the database path and filtering options. Configure other training parameters and Rewards as you would in a normal training workflow. ajet : # ... task_reader : # use tracing log as tasks type : tracing feedback_tracing : # path to the database base_url : ./tutorial/example_feedback_tracing/database.sqlite # path where the module will write cache train_output_path : ./tutorial/example_feedback_tracing/tasks.jsonl # the model used in filters alien_llm_model : qwen3-235b-a22b-instruct-2507 alien_llm_response_length : 2048 # filters filters : # the default filter llm_evaluate - type : llm_evaluate enabled : true params : # define your rubrics to drop any bad-quality tasks custom_rubrics : | 1. Check the answer and drop the task if it does not answer or answer is wrong. 2. Consider a response is invalid if it does not wrap the final answer in \\boxed{}. # LLM temperature temperature : 0.5 # print debug log print_reason : false max_thread : 16 When everything is ready, start the training with launcher.py . ajet --conf tutorial/example_feedback_tracing/example_feedback_tracing.yaml --backbone = 'verl' # or ajet --conf tutorial/example_feedback_tracing/example_feedback_tracing.yaml --backbone = 'trinity' --with-ray After training, we can now deploy the new Agent into production and collect new logs. This workflow enables continuous improvement through iterative tracing-feedback training.","title":"Start Tracing-Feedback Training"},{"location":"en/example_tracing_feedback_loop/#customize","text":"","title":"Customize"},{"location":"en/example_tracing_feedback_loop/#filter","text":"The module provides Filter to select high-quality samples from logs for training. Users are allowed to customize the specific rubrics of their own tasks. To write rubrics, edit the configuration file: ajet : # ... task_reader : # ... feedback_tracing : # ... filters : - type : llm_evaluate enabled : true # enable the filter params : # define your rubrics custom_rubrics : | 1. Check the answer and drop the task if it does not answer or answer is wrong. 2. Consider a response is invalid if it does not wrap the final answer in \\boxed{}. temperature : 0.5 print_reason : false max_thread : 16","title":"Filter"},{"location":"en/example_werewolves/","text":"This tutorial demonstrates how to train multiple agents to play the Werewolves game. 1. Overview The Werewolves role-playing game is a typical POMDP (Partially Observable Markov Decision Process) problem. We can train agents in this cooperative multi-agent problem using shared-parameter methods. Terms explained: Partially Observable : Agents are only able to receive local information . One agent cannot obtain others' perception, even if they are teammates. Markov Decision Process : Making decisions according to current situations. Shared-parameter : Using one model as policy for multiple agents. But notice agents share policy (model parameters) but do not share perception (model input). Cooperative multi-agent problem : Agents have aligned interests (reward). Environment : We use static Qwen3-235B-A22B as the brain of opponents. We use Qwen2-7B as the brain of trainable agents ( trainable_targets ). This page shows how to use the Werewolves social deduction game as a multi-agent environment to prepare data and environment, write an AgentScope Workflow, configure the reward module (Judge), and complete the full process from local debugging to formal training. Scenario Overview Scenario: Classic Werewolves game, including roles such as werewolf, villager, seer, witch, and hunter. Goal: Train a specific role (in this example, the werewolf ) to achieve a higher win rate in games. 2. Quick Start Start training with the following command: # ( ajet --kill=\"python|ray|vllm\" ) ajet --conf tutorial/example_werewolves/werewolves.yaml --backbone='verl' Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_werewolves/math_agent.yaml --backbone = 'debug' --with-logview When `--backbone=debug`, Ray is disabled. You can use a VSCode `.vscode/launch.json` like below: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] } 3. Understand 3.1 Core Process At a high level, each training iteration follows this flow: The task reader generates a new game setup (players, role assignments, initial state). The rollout runs the AgentScope workflow to simulate a full game. Agents in trainable_targets act by using the trainable model (via tuner.as_agentscope_model(...) ), while opponents use the fixed model. The environment produces rewards / outcomes for the episode. Trajectories are collected and passed to the backbone trainer ( verl or trinity ) to update the trainable model. 3.2 Configuration Details This section corresponds to tutorial/example_werewolves/werewolves.yaml . The key configuration items are as follows: ajet : task_reader : # random seed to shuffle players type : random_dummy task_judge : # Implement and select the evaluation function # (in this example you can first set it to null and rely purely on the rollout's internal reward) judge_protocol : null model : # Set the model to be trained path : YOUR_MODEL_PATH rollout : # Select the AgentScope Workflow entry user_workflow : tutorial.example_werewolves.start->ExampleWerewolves 3.3 Code Map tutorial/example_werewolves/werewolves.yaml : connects the task reader, judge, model, and workflow entry. tutorial/example_werewolves/start.py : the AgentScope workflow implementation ( ExampleWerewolves ). tutorial/example_werewolves/game.py : the Werewolves game logic implementation. tutorial/example_werewolves/prompt.py : prompt templates related to the game. tutorial/example_werewolves/structured_model.py : defines structured output formats for different roles. tutorial/example_werewolves/utils.py : game state management and helper functions. 3.4 Reward When judge_protocol: null , training relies on the reward (or win/loss outcome) produced inside the rollout / environment. In this example, the reward is produced in the workflow in tutorial/example_werewolves/start.py . In ExampleWerewolves.execute() , the workflow first runs a full game by calling werewolves_game(players, roles) , and obtains good_guy_win (whether the good-guy side wins). Then it uses a turn-level sparse win/loss reward : If good_guy_win == True and the training target is not werewolf (i.e., you are training a good-guy role), then raw_reward = 1 and is_success = True . If good_guy_win == False and the training target is werewolf (i.e., you are training a werewolf-side role), then raw_reward = 1 and is_success = True . Otherwise, the training side did not win: raw_reward = 0 and is_success = False . Exception / invalid-behavior penalty: If an exception is thrown during the game (e.g., the game cannot proceed), all trainable targets are penalized uniformly: raw_reward = -0.1 and is_success = False . If you need a more fine-grained evaluation (e.g., giving partial credit for key intermediate decisions instead of only win/loss), implement a custom Judge and enable it via ajet.task_judge.judge_protocol . 4. Results 4.1 Training Curves Qwen2-7B is able to reach about 60% win rate in about 20 steps. Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. As training progresses, win rate increases. This usually means the agent becomes more stable on two things : - Role-playing consistency : the agent learns to maintain its werewolf cover under pressure, avoiding self-exposure even when voted out. - Social deception skills : it develops strategies to mislead opponents, sow suspicion among villagers, and implicitly coordinate with teammates. 4.2 Case Study Behavior Shifts Significant role-playing improvement is observed during the experiment. For example, when voted out, the original model tends to reveal its identity as werewolf , but after fine-tuning, the agent will try to cheat its opponents and protect teammates. For example: Token-level Visualization: These detailed logs are generated by Beast-Logger. See Beast-Logger Usage for more details. The agent develops multiple strategies for winning. For example: Misleading opponents : \"Let's keep an eye on the seer and the witch. They could be werewolves trying to hide\". Appealing to reason : \"We need to be wary of fake seers and watch for inconsistencies in stories, Player-Y as hunter should act carefully\". Sometimes agents can take advantage of suspicion between non-werewolf players to eliminate opponents. Expanding Qwen2-7B to Qwen2-14B","title":"Werewolves Game"},{"location":"en/example_werewolves/#1-overview","text":"The Werewolves role-playing game is a typical POMDP (Partially Observable Markov Decision Process) problem. We can train agents in this cooperative multi-agent problem using shared-parameter methods. Terms explained: Partially Observable : Agents are only able to receive local information . One agent cannot obtain others' perception, even if they are teammates. Markov Decision Process : Making decisions according to current situations. Shared-parameter : Using one model as policy for multiple agents. But notice agents share policy (model parameters) but do not share perception (model input). Cooperative multi-agent problem : Agents have aligned interests (reward). Environment : We use static Qwen3-235B-A22B as the brain of opponents. We use Qwen2-7B as the brain of trainable agents ( trainable_targets ). This page shows how to use the Werewolves social deduction game as a multi-agent environment to prepare data and environment, write an AgentScope Workflow, configure the reward module (Judge), and complete the full process from local debugging to formal training. Scenario Overview Scenario: Classic Werewolves game, including roles such as werewolf, villager, seer, witch, and hunter. Goal: Train a specific role (in this example, the werewolf ) to achieve a higher win rate in games.","title":"1. Overview"},{"location":"en/example_werewolves/#2-quick-start","text":"Start training with the following command: # ( ajet --kill=\"python|ray|vllm\" ) ajet --conf tutorial/example_werewolves/werewolves.yaml --backbone='verl' Quick Debugging (Optional) If you want to breakpoint-debug the workflow/judge locally: # (optional) recommended cleanup before debug # ajet --kill=\"python|ray\" clear && \\ ajet --conf tutorial/example_werewolves/math_agent.yaml --backbone = 'debug' --with-logview When `--backbone=debug`, Ray is disabled. You can use a VSCode `.vscode/launch.json` like below: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"./path/to/yaml.yaml\" ], \"env\" : {} } ] }","title":"2. Quick Start"},{"location":"en/example_werewolves/#3-understand","text":"","title":"3. Understand"},{"location":"en/example_werewolves/#31-core-process","text":"At a high level, each training iteration follows this flow: The task reader generates a new game setup (players, role assignments, initial state). The rollout runs the AgentScope workflow to simulate a full game. Agents in trainable_targets act by using the trainable model (via tuner.as_agentscope_model(...) ), while opponents use the fixed model. The environment produces rewards / outcomes for the episode. Trajectories are collected and passed to the backbone trainer ( verl or trinity ) to update the trainable model.","title":"3.1 Core Process"},{"location":"en/example_werewolves/#32-configuration-details","text":"This section corresponds to tutorial/example_werewolves/werewolves.yaml . The key configuration items are as follows: ajet : task_reader : # random seed to shuffle players type : random_dummy task_judge : # Implement and select the evaluation function # (in this example you can first set it to null and rely purely on the rollout's internal reward) judge_protocol : null model : # Set the model to be trained path : YOUR_MODEL_PATH rollout : # Select the AgentScope Workflow entry user_workflow : tutorial.example_werewolves.start->ExampleWerewolves","title":"3.2 Configuration Details"},{"location":"en/example_werewolves/#33-code-map","text":"tutorial/example_werewolves/werewolves.yaml : connects the task reader, judge, model, and workflow entry. tutorial/example_werewolves/start.py : the AgentScope workflow implementation ( ExampleWerewolves ). tutorial/example_werewolves/game.py : the Werewolves game logic implementation. tutorial/example_werewolves/prompt.py : prompt templates related to the game. tutorial/example_werewolves/structured_model.py : defines structured output formats for different roles. tutorial/example_werewolves/utils.py : game state management and helper functions.","title":"3.3 Code Map"},{"location":"en/example_werewolves/#34-reward","text":"When judge_protocol: null , training relies on the reward (or win/loss outcome) produced inside the rollout / environment. In this example, the reward is produced in the workflow in tutorial/example_werewolves/start.py . In ExampleWerewolves.execute() , the workflow first runs a full game by calling werewolves_game(players, roles) , and obtains good_guy_win (whether the good-guy side wins). Then it uses a turn-level sparse win/loss reward : If good_guy_win == True and the training target is not werewolf (i.e., you are training a good-guy role), then raw_reward = 1 and is_success = True . If good_guy_win == False and the training target is werewolf (i.e., you are training a werewolf-side role), then raw_reward = 1 and is_success = True . Otherwise, the training side did not win: raw_reward = 0 and is_success = False . Exception / invalid-behavior penalty: If an exception is thrown during the game (e.g., the game cannot proceed), all trainable targets are penalized uniformly: raw_reward = -0.1 and is_success = False . If you need a more fine-grained evaluation (e.g., giving partial credit for key intermediate decisions instead of only win/loss), implement a custom Judge and enable it via ajet.task_judge.judge_protocol .","title":"3.4 Reward"},{"location":"en/example_werewolves/#4-results","text":"","title":"4. Results"},{"location":"en/example_werewolves/#41-training-curves","text":"Qwen2-7B is able to reach about 60% win rate in about 20 steps. Visualization: Training curves are generated by SwanLab. See Visualization Tools for setup and usage. As training progresses, win rate increases. This usually means the agent becomes more stable on two things : - Role-playing consistency : the agent learns to maintain its werewolf cover under pressure, avoiding self-exposure even when voted out. - Social deception skills : it develops strategies to mislead opponents, sow suspicion among villagers, and implicitly coordinate with teammates.","title":"4.1 Training Curves"},{"location":"en/example_werewolves/#42-case-study","text":"","title":"4.2 Case Study"},{"location":"en/example_werewolves/#behavior-shifts","text":"Significant role-playing improvement is observed during the experiment. For example, when voted out, the original model tends to reveal its identity as werewolf , but after fine-tuning, the agent will try to cheat its opponents and protect teammates. For example: Token-level Visualization: These detailed logs are generated by Beast-Logger. See Beast-Logger Usage for more details. The agent develops multiple strategies for winning. For example: Misleading opponents : \"Let's keep an eye on the seer and the witch. They could be werewolves trying to hide\". Appealing to reason : \"We need to be wary of fake seers and watch for inconsistencies in stories, Player-Y as hunter should act carefully\". Sometimes agents can take advantage of suspicion between non-werewolf players to eliminate opponents.","title":"Behavior Shifts"},{"location":"en/example_werewolves/#expanding-qwen2-7b-to-qwen2-14b","text":"","title":"Expanding Qwen2-7B to Qwen2-14B"},{"location":"en/hardware_related_solution/","text":"This document records a list of Hardware Related issues for future reference. 1. ncclUnhandledCudaError: Call to CUDA function failed. Problem: File \"/root/AgentJet/.venv/lib/python3.10/site-packages/torch/distributed/utils.py\" , line 322 , in _sync_params_and_buffers dist . _broadcast_coalesced ( torch . distributed . DistBackendError : NCCL error in : / pytorch / torch / csrc / distributed / c10d / NCCLUtils . cpp : 77 , unhandled cuda error ( run with NCCL_DEBUG = INFO for details ), NCCL version 2.26.2 ncclUnhandledCudaError : Call to CUDA function failed . Last error : Cuda failure 1 'invalid argument' Solution: export NCCL_NVLS_ENABLE = 0","title":"Hardware related solution"},{"location":"en/hardware_related_solution/#1-ncclunhandledcudaerror-call-to-cuda-function-failed","text":"Problem: File \"/root/AgentJet/.venv/lib/python3.10/site-packages/torch/distributed/utils.py\" , line 322 , in _sync_params_and_buffers dist . _broadcast_coalesced ( torch . distributed . DistBackendError : NCCL error in : / pytorch / torch / csrc / distributed / c10d / NCCLUtils . cpp : 77 , unhandled cuda error ( run with NCCL_DEBUG = INFO for details ), NCCL version 2.26.2 ncclUnhandledCudaError : Call to CUDA function failed . Last error : Cuda failure 1 'invalid argument' Solution: export NCCL_NVLS_ENABLE = 0","title":"1. ncclUnhandledCudaError: Call to CUDA function failed."},{"location":"en/installation/","text":"This document provides a step-by-step guide to installing AgentJet. Latest Version Recommended: AgentJet is under active development and iteration. We recommend installing from source to get the latest features and bug fixes. Prerequisites Requirement Detail Python 3.10 Package Management uv or conda Install from Source Step 1: Clone the Repository Clone the AgentJet repository from GitHub and navigate into the project directory: git clone https://github.com/modelscope/AgentJet.git cd AgentJet Step 2: Install Dependencies AgentJet supports multiple backbones. Currently we have verl and trinity (recommended). Package Manager We recommend using uv to manage your Python environment as it is incredibly fast. See also uv installation document . And of course, if you prefer conda , you can also install via conda and pip (simply change uv pip to pip ). VERL (uv) VERL (conda) VERL (aliyun) Trinity Trinity (aliyun) # Install with `verl` training backbone: uv venv --python = 3 .10 source .venv/bin/activate uv pip install -e . [ verl ] #`flash-attn` must be installed after other dependencies uv pip install --verbose flash-attn --no-deps --no-build-isolation --no-cache flash-attn Installation flash-attn must be installed after other dependencies. To build faster, export MAX_JOBS=${N_CPU} , or ensure a healthy connection to GitHub to install pre-compiled wheels. # Install with `verl` training backbone: conda create -n ajet-verl python = 3 .10 conda activate ajet-verl pip install -e . [ verl ] #`flash-attn` must be installed after other dependencies pip install --verbose flash-attn --no-deps --no-build-isolation --no-cache flash-attn Installation flash-attn must be installed after other dependencies. To build faster, export MAX_JOBS=${N_CPU} , or ensure a healthy connection to GitHub to install pre-compiled wheels. # Install with `verl` training backbone: uv venv --python = 3 .10 source .venv/bin/activate uv pip install -i https://mirrors.aliyun.com/pypi/simple/ -e . [ verl ] #`flash-attn` must be installed after other dependencies uv pip install -i https://mirrors.aliyun.com/pypi/simple/ --verbose flash-attn --no-deps --no-build-isolation --no-cache flash-attn Installation flash-attn must be installed after other dependencies. Ensure a healthy connection to GitHub to install pre-compiled wheels. If you find your machine spend a long time installing flash-attn, ensure a healthy connection to GitHub. To build faster, export MAX_JOBS=${N_CPU} . # Install with `trinity` training backbone for fully asynchronous RFT: uv venv --python = 3 .10 source .venv/bin/activate uv pip install -e . [ trinity ] uv pip install --verbose flash-attn --no-deps --no-build-isolation --no-cache # Install with `trinity` training backbone for fully asynchronous RFT: uv venv --python = 3 .10 source .venv/bin/activate uv pip install -i https://mirrors.aliyun.com/pypi/simple/ -e . [ trinity ] uv pip install -i https://mirrors.aliyun.com/pypi/simple/ --verbose flash-attn --no-deps --no-build-isolation --no-cache Backbone VeRL Trinity-RFT Core design Share-GPU actor-rollout engine (colocate) Async actor-rollout engine Speed \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Scalability \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Minimum Required GPU Resource 1 2 Training Stability \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 vLLM Version 0.10.0 0.10.0 Next Steps Quick Start Run your first training command and explore examples. Tune Your First Agent Step-by-step guide to build and train your own agent.","title":"Installation"},{"location":"en/installation/#prerequisites","text":"Requirement Detail Python 3.10 Package Management uv or conda","title":"Prerequisites"},{"location":"en/installation/#install-from-source","text":"","title":"Install from Source"},{"location":"en/installation/#step-1-clone-the-repository","text":"Clone the AgentJet repository from GitHub and navigate into the project directory: git clone https://github.com/modelscope/AgentJet.git cd AgentJet","title":"Step 1: Clone the Repository"},{"location":"en/installation/#step-2-install-dependencies","text":"AgentJet supports multiple backbones. Currently we have verl and trinity (recommended). Package Manager We recommend using uv to manage your Python environment as it is incredibly fast. See also uv installation document . And of course, if you prefer conda , you can also install via conda and pip (simply change uv pip to pip ). VERL (uv) VERL (conda) VERL (aliyun) Trinity Trinity (aliyun) # Install with `verl` training backbone: uv venv --python = 3 .10 source .venv/bin/activate uv pip install -e . [ verl ] #`flash-attn` must be installed after other dependencies uv pip install --verbose flash-attn --no-deps --no-build-isolation --no-cache flash-attn Installation flash-attn must be installed after other dependencies. To build faster, export MAX_JOBS=${N_CPU} , or ensure a healthy connection to GitHub to install pre-compiled wheels. # Install with `verl` training backbone: conda create -n ajet-verl python = 3 .10 conda activate ajet-verl pip install -e . [ verl ] #`flash-attn` must be installed after other dependencies pip install --verbose flash-attn --no-deps --no-build-isolation --no-cache flash-attn Installation flash-attn must be installed after other dependencies. To build faster, export MAX_JOBS=${N_CPU} , or ensure a healthy connection to GitHub to install pre-compiled wheels. # Install with `verl` training backbone: uv venv --python = 3 .10 source .venv/bin/activate uv pip install -i https://mirrors.aliyun.com/pypi/simple/ -e . [ verl ] #`flash-attn` must be installed after other dependencies uv pip install -i https://mirrors.aliyun.com/pypi/simple/ --verbose flash-attn --no-deps --no-build-isolation --no-cache flash-attn Installation flash-attn must be installed after other dependencies. Ensure a healthy connection to GitHub to install pre-compiled wheels. If you find your machine spend a long time installing flash-attn, ensure a healthy connection to GitHub. To build faster, export MAX_JOBS=${N_CPU} . # Install with `trinity` training backbone for fully asynchronous RFT: uv venv --python = 3 .10 source .venv/bin/activate uv pip install -e . [ trinity ] uv pip install --verbose flash-attn --no-deps --no-build-isolation --no-cache # Install with `trinity` training backbone for fully asynchronous RFT: uv venv --python = 3 .10 source .venv/bin/activate uv pip install -i https://mirrors.aliyun.com/pypi/simple/ -e . [ trinity ] uv pip install -i https://mirrors.aliyun.com/pypi/simple/ --verbose flash-attn --no-deps --no-build-isolation --no-cache Backbone VeRL Trinity-RFT Core design Share-GPU actor-rollout engine (colocate) Async actor-rollout engine Speed \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Scalability \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Minimum Required GPU Resource 1 2 Training Stability \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 vLLM Version 0.10.0 0.10.0","title":"Step 2: Install Dependencies"},{"location":"en/installation/#next-steps","text":"","title":"Next Steps"},{"location":"en/intro/","text":"AgentJet (AJet) is a cutting-edge, user-friendly agent tuning framework designed to optimize LLM models and agent workflows. Simply provide your workflow (built from AgentScope, OpenAI SDK, Langchain, raw HTTP requests, or hybrid of all of them), training data, and reward function, and we will be ready to enhance your agents to their optimal performance! Features AgentJet aims to build a state-of-the-art agent tuning platform for both developers and researchers Easy and Friendly . AgentJet helps you tune models behind your agent workflows easily, optimizing your agents for top performance with minimal effort. Rich Tutorial Library . AgentJet provides a rich library of examples as tutorials. Efficient and Scalable . AgentJet uses [verl] as the default backbone ( --backbone=verl ). However, we also support trinity as alternative backbone, accelerating your tuning process via fully asynchronous RFT. Flexible and Fast . AgentJet supports multi-agent workflows and adopts a context merging technique, accelerating training by 1.5x to 10x when the workflow involves multi-turn (or multi-agent) conversations. Reliability and Reproducibility . Our team keeps track of framework performance across multiple tasks + major-git-version + training-backbones (under construction, still gathering data, coming soon). For advanced researchers, AgentJet also provides high-resolution logging and debugging solutions: High-Resolution Logging : AgentJet allows users to save and inspect token-level rollout details, recording token IDs, token loss masks, and even token logprobs to facilitate workflow development and agent diagnostics. Fast Debugging : AgentJet also provides the --backbone=debug option for the best debugging experience, shortening your wait period from minutes to seconds after code changes and enabling breakpoint debugging in IDEs. Quick Start Installation We recommend using uv for dependency management. Step 1: Clone Repository Step 2: Setup Environment git clone https://github.com/modelscope/AgentJet.git cd AgentJet uv venv --python = 3 .10.16 && source .venv/bin/activate uv pip install -e . [ trinity ] # Note: flash-attn must be installed after other dependencies uv pip install flash_attn == 2 .8.3 --no-build-isolation --no-cache-dir Train the first agent You can start training your first agent with a single command using a pre-configured YAML file: ajet --conf tutorial/example_math_agent/math_agent.yaml Learn More See the Math Agent example for detailed explanation. Example Library Explore our rich library of examples to kickstart your journey: Math Agent Training a math agent that can write Python code to solve mathematical problems. AppWorld Agent Creating an AppWorld agent using AgentScope and training it for real-world tasks. Werewolves Game Developing Werewolves RPG agents and training them for strategic gameplay. Learning to Ask Learning to ask questions like a doctor for medical consultation scenarios. Countdown Game Writing a countdown game using AgentScope and solving it with RL. Frozen Lake Solving a frozen lake walking puzzle using AgentJet's reinforcement learning. Core Concepts AgentJet makes agent fine-tuning straightforward by separating the developer interface from the internal execution logic. 1. The User-Centric Interface To optimize an agent, you provide three core inputs: Trainable Workflow Define your agent logic by inheriting the Workflow class, supporting both simple and multi-agent setups. Task Reader Load training tasks from JSONL files, HuggingFace datasets, or auto-generate from documents. Task Judger Evaluates agent outputs and assigns rewards to guide the training process. 2. Internal System Architecture The internal system orchestrates several specialized modules to handle the complexities of RL training and agent interactions. Module Description Launcher Manages background service processes (Ray, vLLM) and routes the backbone Task Reader Handles data ingestion, augmentation, and filtering Task Rollout Bridges LLM engines and manages the Gym environment lifecycle Task Runner Executes the AgentScope workflow and calculates rewards Model Tuner Forwards inference requests from the workflow to the LLM engine Context Tracker Monitors LLM calls and automatically merges shared-history timelines ( 1.5x-10x efficiency boost) Next Steps Installation Set up AgentJet environment and dependencies. Quick Start Run your first training in minutes.","title":"Introduction"},{"location":"en/intro/#features","text":"AgentJet aims to build a state-of-the-art agent tuning platform for both developers and researchers Easy and Friendly . AgentJet helps you tune models behind your agent workflows easily, optimizing your agents for top performance with minimal effort. Rich Tutorial Library . AgentJet provides a rich library of examples as tutorials. Efficient and Scalable . AgentJet uses [verl] as the default backbone ( --backbone=verl ). However, we also support trinity as alternative backbone, accelerating your tuning process via fully asynchronous RFT. Flexible and Fast . AgentJet supports multi-agent workflows and adopts a context merging technique, accelerating training by 1.5x to 10x when the workflow involves multi-turn (or multi-agent) conversations. Reliability and Reproducibility . Our team keeps track of framework performance across multiple tasks + major-git-version + training-backbones (under construction, still gathering data, coming soon). For advanced researchers, AgentJet also provides high-resolution logging and debugging solutions: High-Resolution Logging : AgentJet allows users to save and inspect token-level rollout details, recording token IDs, token loss masks, and even token logprobs to facilitate workflow development and agent diagnostics. Fast Debugging : AgentJet also provides the --backbone=debug option for the best debugging experience, shortening your wait period from minutes to seconds after code changes and enabling breakpoint debugging in IDEs.","title":"Features"},{"location":"en/intro/#quick-start","text":"","title":"Quick Start"},{"location":"en/intro/#installation","text":"We recommend using uv for dependency management. Step 1: Clone Repository Step 2: Setup Environment git clone https://github.com/modelscope/AgentJet.git cd AgentJet uv venv --python = 3 .10.16 && source .venv/bin/activate uv pip install -e . [ trinity ] # Note: flash-attn must be installed after other dependencies uv pip install flash_attn == 2 .8.3 --no-build-isolation --no-cache-dir Train the first agent You can start training your first agent with a single command using a pre-configured YAML file: ajet --conf tutorial/example_math_agent/math_agent.yaml Learn More See the Math Agent example for detailed explanation.","title":"Installation"},{"location":"en/intro/#example-library","text":"Explore our rich library of examples to kickstart your journey:","title":"Example Library"},{"location":"en/intro/#core-concepts","text":"AgentJet makes agent fine-tuning straightforward by separating the developer interface from the internal execution logic.","title":"Core Concepts"},{"location":"en/intro/#1-the-user-centric-interface","text":"To optimize an agent, you provide three core inputs:","title":"1. The User-Centric Interface"},{"location":"en/intro/#2-internal-system-architecture","text":"The internal system orchestrates several specialized modules to handle the complexities of RL training and agent interactions. Module Description Launcher Manages background service processes (Ray, vLLM) and routes the backbone Task Reader Handles data ingestion, augmentation, and filtering Task Rollout Bridges LLM engines and manages the Gym environment lifecycle Task Runner Executes the AgentScope workflow and calculates rewards Model Tuner Forwards inference requests from the workflow to the LLM engine Context Tracker Monitors LLM calls and automatically merges shared-history timelines ( 1.5x-10x efficiency boost)","title":"2. Internal System Architecture"},{"location":"en/intro/#next-steps","text":"","title":"Next Steps"},{"location":"en/quickstart/","text":"1. Testing Pre-define Demo AgentJet provides a complete feature set for tuning agents. You can try starting training an agent right away by running a demo: ajet --conf tutorial/example_math_agent/math_agent.yaml 2. Minimum Example Let's begin with the simplest example: a math agent with a tool call. train_math_agent.py from ajet import AgentJetJob # refer to `https://doc.agentjet.top/AgentJet/en/tune_your_first_agent/` on how to write your own workflow from tutorial.example_math_agent.math_agent_simplify import MathToolWorkflow model_path = \"YOUR_MODEL_PATH\" job = AgentJetJob ( n_gpu = 8 , algorithm = 'grpo' , model = model_path ) job . set_workflow ( MathToolWorkflow ) job . set_data ( type = \"hf\" , dataset_path = 'openai/gsm8k' ) # [Optional] Save yaml file for manual adjustment # job.dump_job_as_yaml('saved_experiments/math.yaml') # [Optional] Load yaml file from manual adjustment # job.load_job_from_yaml('saved_experiments/math.yaml') # Start training tuned_model = job . tune () CLI Alternative The code above is equivalent to running in terminal: ajet --conf ./saved_experiments/math.yaml 3. Compare with Community Training Curves AgentJet Benchmark Tracking System (Developing-In-Progress) Compare training curves with community. Investigate the influence of versions, backbones, hyper-parameters, etc. 4. Explore Example Gallery Explore our rich library of examples to kickstart your journey: Math Agent Training a math agent that can write Python code to solve mathematical problems. AppWorld Agent Creating an AppWorld agent using AgentScope and training it. Werewolves Game Developing Werewolves RPG agents and training them. Learning to Ask Learning to ask questions like a doctor. Countdown Game Writing and solving a countdown game with RL. Frozen Lake Solving a frozen lake walking puzzle. 5. Next Steps Tune Your First Agent Complete step-by-step guide to building your own agent from scratch. Math Agent Example Detailed walkthrough of the Math Agent training example.","title":"Quick Start"},{"location":"en/quickstart/#1-testing-pre-define-demo","text":"AgentJet provides a complete feature set for tuning agents. You can try starting training an agent right away by running a demo: ajet --conf tutorial/example_math_agent/math_agent.yaml","title":"1. Testing Pre-define Demo"},{"location":"en/quickstart/#2-minimum-example","text":"Let's begin with the simplest example: a math agent with a tool call. train_math_agent.py from ajet import AgentJetJob # refer to `https://doc.agentjet.top/AgentJet/en/tune_your_first_agent/` on how to write your own workflow from tutorial.example_math_agent.math_agent_simplify import MathToolWorkflow model_path = \"YOUR_MODEL_PATH\" job = AgentJetJob ( n_gpu = 8 , algorithm = 'grpo' , model = model_path ) job . set_workflow ( MathToolWorkflow ) job . set_data ( type = \"hf\" , dataset_path = 'openai/gsm8k' ) # [Optional] Save yaml file for manual adjustment # job.dump_job_as_yaml('saved_experiments/math.yaml') # [Optional] Load yaml file from manual adjustment # job.load_job_from_yaml('saved_experiments/math.yaml') # Start training tuned_model = job . tune () CLI Alternative The code above is equivalent to running in terminal: ajet --conf ./saved_experiments/math.yaml","title":"2. Minimum Example"},{"location":"en/quickstart/#3-compare-with-community-training-curves","text":"","title":"3. Compare with Community Training Curves"},{"location":"en/quickstart/#4-explore-example-gallery","text":"Explore our rich library of examples to kickstart your journey:","title":"4. Explore Example Gallery"},{"location":"en/quickstart/#5-next-steps","text":"","title":"5. Next Steps"},{"location":"en/setup_ubuntu/","text":"sudo apt update sudo apt install docker docker.io curl 2. intsall nvidia-runtime # https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installing-on-ubuntu-and-debian # step 1 curl https://get.docker.com | sh \\ && sudo systemctl --now enable docker # step 2 distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) \\ && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ && curl -s -L https://nvidia.github.io/libnvidia-container/ $distribution /libnvidia-container.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list # step 3:Install the nvidia-docker2 package (and dependencies) after updating the package listing: sudo apt-get update sudo apt-get install -y nvidia-docker2 # Restart the Docker daemon to complete the installation after setting the default runtime: sudo systemctl restart docker","title":"Setup ubuntu"},{"location":"en/setup_ubuntu/#2-intsall-nvidia-runtime","text":"# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installing-on-ubuntu-and-debian # step 1 curl https://get.docker.com | sh \\ && sudo systemctl --now enable docker # step 2 distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) \\ && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ && curl -s -L https://nvidia.github.io/libnvidia-container/ $distribution /libnvidia-container.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list # step 3:Install the nvidia-docker2 package (and dependencies) after updating the package listing: sudo apt-get update sudo apt-get install -y nvidia-docker2 # Restart the Docker daemon to complete the installation after setting the default runtime: sudo systemctl restart docker","title":"2. intsall nvidia-runtime"},{"location":"en/support_agentscope/","text":"This article introduce the way to convert different types of ways to convert your existing workflows into AgentJet workflows. AgentScope use tuner.as_agentscope_model() to override ReActAgent's model argument use tuner.as_oai_baseurl_apikey() to override OpenAIChatModel's baseurl + apikey argument Explain with examples Before Convertion After Convertion ( as_agentscope_model() ) After Convertion ( as_oai_baseurl_apikey() ) model = DashScopeChatModel ( model_name = \"qwen-max\" , stream = False ) # \ud83d\udee9\ufe0f change here agent_instance = ReActAgent ( name = f \"Friday\" , sys_prompt = \"You are a helpful assistant\" , model = model , formatter = DashScopeChatFormatter (), ) model = tuner . as_agentscope_model () # \ud83d\udee9\ufe0f change here agent_instance = ReActAgent ( name = f \"Friday\" , sys_prompt = \"You are a helpful assistant\" , model = model , formatter = DashScopeChatFormatter (), ) url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key # the api key contain information, do not discard it model = OpenAIChatModel ( model_name = \"whatever\" , client_args = { \"base_url\" : base_url }, api_key = api_key , stream = False , ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = model , # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = OpenAIChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) when you are using the tuner.as_oai_baseurl_apikey() api, you must enable the following feature in the yaml configuration. ajet : ... enable_experimental_interchange_server : True ... Explain with examples (Full Workflow Code) Full Code After Convertion ( as_agentscope_model ) Full Code After Convertion ( as_agentscope_model ) import re from loguru import logger from agentscope.message import Msg from agentscope.agent import ReActAgent from agentscope.formatter import DashScopeChatFormatter from agentscope.memory import InMemoryMemory from agentscope.tool import Toolkit , execute_python_code from ajet import AjetTuner , Workflow , WorkflowOutput , WorkflowTask def extract_final_answer ( result ) -> str : \"\"\"Extract the final answer from the agent's response.\"\"\" try : if ( hasattr ( result , \"metadata\" ) and isinstance ( result . metadata , dict ) and \"result\" in result . metadata ): return result . metadata [ \"result\" ] if hasattr ( result , \"content\" ): if isinstance ( result . content , dict ) and \"result\" in result . content : return result . content [ \"result\" ] return str ( result . content ) return str ( result ) except Exception as e : logger . warning ( f \"Extract final answer error: { e } . Raw: { result } \" ) return str ( result ) system_prompt = \"\"\" You are an agent specialized in solving math problems with tools. Please solve the math problem given to you. You can write and execute Python code to perform calculation or verify your answer. You should return your final answer within \\\\ boxed{{}}. \"\"\" class MathToolWorkflow ( Workflow ): # \u2728\u2728 inherit `Workflow` class name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer }) import re from loguru import logger from agentscope.message import Msg from agentscope.agent import ReActAgent from agentscope.formatter import OpenAIChatFormatter from agentscope.model import OpenAIChatModel from agentscope.memory import InMemoryMemory from agentscope.tool import Toolkit , execute_python_code from ajet import AjetTuner , Workflow , WorkflowOutput , WorkflowTask def extract_final_answer ( result ) -> str : \"\"\"Extract the final answer from the agent's response.\"\"\" try : if ( hasattr ( result , \"metadata\" ) and isinstance ( result . metadata , dict ) and \"result\" in result . metadata ): return result . metadata [ \"result\" ] if hasattr ( result , \"content\" ): if isinstance ( result . content , dict ) and \"result\" in result . content : return result . content [ \"result\" ] return str ( result . content ) return str ( result ) except Exception as e : logger . warning ( f \"Extract final answer error: { e } . Raw: { result } \" ) return str ( result ) system_prompt = \"\"\" You are an agent specialized in solving math problems with tools. Please solve the math problem given to you. You can write and execute Python code to perform calculation or verify your answer. You should return your final answer within \\\\ boxed{{}}. \"\"\" class MathToolWorkflow ( Workflow ): # \u2728\u2728 inherit `Workflow` class name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key # the api key contain information, do not discard it model = OpenAIChatModel ( model_name = \"whatever\" , client_args = { \"base_url\" : base_url }, api_key = api_key , stream = False , ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = model , # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = OpenAIChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer })","title":"AgentScope"},{"location":"en/support_agentscope/#agentscope","text":"use tuner.as_agentscope_model() to override ReActAgent's model argument use tuner.as_oai_baseurl_apikey() to override OpenAIChatModel's baseurl + apikey argument","title":"AgentScope"},{"location":"en/support_agentscope/#explain-with-examples","text":"Before Convertion After Convertion ( as_agentscope_model() ) After Convertion ( as_oai_baseurl_apikey() ) model = DashScopeChatModel ( model_name = \"qwen-max\" , stream = False ) # \ud83d\udee9\ufe0f change here agent_instance = ReActAgent ( name = f \"Friday\" , sys_prompt = \"You are a helpful assistant\" , model = model , formatter = DashScopeChatFormatter (), ) model = tuner . as_agentscope_model () # \ud83d\udee9\ufe0f change here agent_instance = ReActAgent ( name = f \"Friday\" , sys_prompt = \"You are a helpful assistant\" , model = model , formatter = DashScopeChatFormatter (), ) url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key # the api key contain information, do not discard it model = OpenAIChatModel ( model_name = \"whatever\" , client_args = { \"base_url\" : base_url }, api_key = api_key , stream = False , ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = model , # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = OpenAIChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) when you are using the tuner.as_oai_baseurl_apikey() api, you must enable the following feature in the yaml configuration. ajet : ... enable_experimental_interchange_server : True ...","title":"Explain with examples"},{"location":"en/support_agentscope/#explain-with-examples-full-workflow-code","text":"Full Code After Convertion ( as_agentscope_model ) Full Code After Convertion ( as_agentscope_model ) import re from loguru import logger from agentscope.message import Msg from agentscope.agent import ReActAgent from agentscope.formatter import DashScopeChatFormatter from agentscope.memory import InMemoryMemory from agentscope.tool import Toolkit , execute_python_code from ajet import AjetTuner , Workflow , WorkflowOutput , WorkflowTask def extract_final_answer ( result ) -> str : \"\"\"Extract the final answer from the agent's response.\"\"\" try : if ( hasattr ( result , \"metadata\" ) and isinstance ( result . metadata , dict ) and \"result\" in result . metadata ): return result . metadata [ \"result\" ] if hasattr ( result , \"content\" ): if isinstance ( result . content , dict ) and \"result\" in result . content : return result . content [ \"result\" ] return str ( result . content ) return str ( result ) except Exception as e : logger . warning ( f \"Extract final answer error: { e } . Raw: { result } \" ) return str ( result ) system_prompt = \"\"\" You are an agent specialized in solving math problems with tools. Please solve the math problem given to you. You can write and execute Python code to perform calculation or verify your answer. You should return your final answer within \\\\ boxed{{}}. \"\"\" class MathToolWorkflow ( Workflow ): # \u2728\u2728 inherit `Workflow` class name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer }) import re from loguru import logger from agentscope.message import Msg from agentscope.agent import ReActAgent from agentscope.formatter import OpenAIChatFormatter from agentscope.model import OpenAIChatModel from agentscope.memory import InMemoryMemory from agentscope.tool import Toolkit , execute_python_code from ajet import AjetTuner , Workflow , WorkflowOutput , WorkflowTask def extract_final_answer ( result ) -> str : \"\"\"Extract the final answer from the agent's response.\"\"\" try : if ( hasattr ( result , \"metadata\" ) and isinstance ( result . metadata , dict ) and \"result\" in result . metadata ): return result . metadata [ \"result\" ] if hasattr ( result , \"content\" ): if isinstance ( result . content , dict ) and \"result\" in result . content : return result . content [ \"result\" ] return str ( result . content ) return str ( result ) except Exception as e : logger . warning ( f \"Extract final answer error: { e } . Raw: { result } \" ) return str ( result ) system_prompt = \"\"\" You are an agent specialized in solving math problems with tools. Please solve the math problem given to you. You can write and execute Python code to perform calculation or verify your answer. You should return your final answer within \\\\ boxed{{}}. \"\"\" class MathToolWorkflow ( Workflow ): # \u2728\u2728 inherit `Workflow` class name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key # the api key contain information, do not discard it model = OpenAIChatModel ( model_name = \"whatever\" , client_args = { \"base_url\" : base_url }, api_key = api_key , stream = False , ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = model , # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = OpenAIChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer })","title":"Explain with examples (Full Workflow Code)"},{"location":"en/support_http/","text":"Why use the Agent SDKs and all these abstractions? If you want to take control of the foundation of LLM Agents, in this AI era, you can always start from scratch and build your own \"high-scrapers\". Http use tuner.as_oai_baseurl_apikey() to obtain baseurl + apikey arguments Explain with examples Before Convertion After Convertion ( as_oai_baseurl_apikey ) # tuner to api key base_url = \"https://openrouter.ai/api/v1\" api_key = \"sk-1234567\" # take out query query = workflow_task . task . main_query messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] # use raw http requests (non-streaming) to get response response = requests . post ( f \" { base_url } /chat/completions\" , json = { \"model\" : \"fill_whatever_model\" , # Of course, this `model` field will be ignored. \"messages\" : messages , }, headers = { \"Authorization\" : f \"Bearer { api_key } \" } ) final_answer = response . json ()[ 'choices' ][ 0 ][ 'message' ][ 'content' ] # tuner to api key url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key # take out query query = workflow_task . task . main_query messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] # use raw http requests (non-streaming) to get response response = requests . post ( f \" { base_url } /chat/completions\" , json = { \"model\" : \"fill_whatever_model\" , # Of course, this `model` field will be ignored. \"messages\" : messages , }, headers = { \"Authorization\" : f \"Bearer { api_key } \" } ) final_answer = response . json ()[ 'choices' ][ 0 ][ 'message' ][ 'content' ] when you are using the tuner.as_oai_baseurl_apikey() api, you must enable the following feature in the yaml configuration. ajet : ... enable_experimental_interchange_server : True ...","title":"Raw HTTP"},{"location":"en/support_http/#http","text":"use tuner.as_oai_baseurl_apikey() to obtain baseurl + apikey arguments","title":"Http"},{"location":"en/support_http/#explain-with-examples","text":"Before Convertion After Convertion ( as_oai_baseurl_apikey ) # tuner to api key base_url = \"https://openrouter.ai/api/v1\" api_key = \"sk-1234567\" # take out query query = workflow_task . task . main_query messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] # use raw http requests (non-streaming) to get response response = requests . post ( f \" { base_url } /chat/completions\" , json = { \"model\" : \"fill_whatever_model\" , # Of course, this `model` field will be ignored. \"messages\" : messages , }, headers = { \"Authorization\" : f \"Bearer { api_key } \" } ) final_answer = response . json ()[ 'choices' ][ 0 ][ 'message' ][ 'content' ] # tuner to api key url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key # take out query query = workflow_task . task . main_query messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] # use raw http requests (non-streaming) to get response response = requests . post ( f \" { base_url } /chat/completions\" , json = { \"model\" : \"fill_whatever_model\" , # Of course, this `model` field will be ignored. \"messages\" : messages , }, headers = { \"Authorization\" : f \"Bearer { api_key } \" } ) final_answer = response . json ()[ 'choices' ][ 0 ][ 'message' ][ 'content' ] when you are using the tuner.as_oai_baseurl_apikey() api, you must enable the following feature in the yaml configuration. ajet : ... enable_experimental_interchange_server : True ...","title":"Explain with examples"},{"location":"en/support_langchain/","text":"This article introduce the way to convert different types of ways to convert your existing workflows into AgentJet workflows. AgentScope use tuner.as_oai_baseurl_apikey() to override OpenAIChatModel's baseurl + apikey argument Explain with examples Before Convertion After Convertion ( as_oai_baseurl_apikey ) from langchain_openai import ChatOpenAI # create openai model llm = ChatOpenAI ( model = \"gpt-5\" , ) agent = create_agent ( model = llm , system_prompt = self . system_prompt , ) # take out query query = workflow_task . task . main_query response = agent . invoke ({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : query } ], }) from langchain_openai import ChatOpenAI url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key llm = ChatOpenAI ( model = \"whatever\" , base_url = base_url , api_key = lambda : api_key , ) agent = create_agent ( model = llm , system_prompt = self . system_prompt , ) # take out query query = workflow_task . task . main_query response = agent . invoke ({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : query } ], }) when you are using the tuner.as_oai_baseurl_apikey() api, you must enable the following feature in the yaml configuration. ajet : ... enable_experimental_interchange_server : True ...","title":"Langchain"},{"location":"en/support_langchain/#agentscope","text":"use tuner.as_oai_baseurl_apikey() to override OpenAIChatModel's baseurl + apikey argument","title":"AgentScope"},{"location":"en/support_langchain/#explain-with-examples","text":"Before Convertion After Convertion ( as_oai_baseurl_apikey ) from langchain_openai import ChatOpenAI # create openai model llm = ChatOpenAI ( model = \"gpt-5\" , ) agent = create_agent ( model = llm , system_prompt = self . system_prompt , ) # take out query query = workflow_task . task . main_query response = agent . invoke ({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : query } ], }) from langchain_openai import ChatOpenAI url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key llm = ChatOpenAI ( model = \"whatever\" , base_url = base_url , api_key = lambda : api_key , ) agent = create_agent ( model = llm , system_prompt = self . system_prompt , ) # take out query query = workflow_task . task . main_query response = agent . invoke ({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : query } ], }) when you are using the tuner.as_oai_baseurl_apikey() api, you must enable the following feature in the yaml configuration. ajet : ... enable_experimental_interchange_server : True ...","title":"Explain with examples"},{"location":"en/support_oaisdk/","text":"This article introduce the way to convert different types of ways to convert your existing workflows into AgentJet workflows. AgentScope use tuner.as_raw_openai_sdk_client() to create a openai SDK use tuner.as_oai_baseurl_apikey() to override openai SDK's baseurl + apikey argument Explain with examples Before Convertion After Convertion ( as_raw_openai_sdk_client ) After Convertion ( as_oai_baseurl_apikey ) import openai client = openai . OpenAI ( api_key = 'sk-123456' ) messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] reply_message : ChatCompletion = await client . chat . completions . create ( messages = messages ) final_answer = reply_message . choices [ 0 ] . message . content client = tuner . as_raw_openai_sdk_client () messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] reply_message : ChatCompletion = await client . chat . completions . create ( messages = messages ) final_answer = reply_message . choices [ 0 ] . message . content import openai url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key client = openai . OpenAI ( api_key = api_key , base_url = base_url ) messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] reply_message : ChatCompletion = await client . chat . completions . create ( messages = messages ) final_answer = reply_message . choices [ 0 ] . message . content when you are using the tuner.as_oai_baseurl_apikey() api, you must enable the following feature in the yaml configuration. ajet : ... enable_experimental_interchange_server : True ...","title":"OpenAI SDK"},{"location":"en/support_oaisdk/#agentscope","text":"use tuner.as_raw_openai_sdk_client() to create a openai SDK use tuner.as_oai_baseurl_apikey() to override openai SDK's baseurl + apikey argument","title":"AgentScope"},{"location":"en/support_oaisdk/#explain-with-examples","text":"Before Convertion After Convertion ( as_raw_openai_sdk_client ) After Convertion ( as_oai_baseurl_apikey ) import openai client = openai . OpenAI ( api_key = 'sk-123456' ) messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] reply_message : ChatCompletion = await client . chat . completions . create ( messages = messages ) final_answer = reply_message . choices [ 0 ] . message . content client = tuner . as_raw_openai_sdk_client () messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] reply_message : ChatCompletion = await client . chat . completions . create ( messages = messages ) final_answer = reply_message . choices [ 0 ] . message . content import openai url_and_apikey = tuner . as_oai_baseurl_apikey () base_url = url_and_apikey . base_url api_key = url_and_apikey . api_key client = openai . OpenAI ( api_key = api_key , base_url = base_url ) messages = [ { \"role\" : \"system\" , \"content\" : self . system_prompt }, { \"role\" : \"user\" , \"content\" : query } ] reply_message : ChatCompletion = await client . chat . completions . create ( messages = messages ) final_answer = reply_message . choices [ 0 ] . message . content when you are using the tuner.as_oai_baseurl_apikey() api, you must enable the following feature in the yaml configuration. ajet : ... enable_experimental_interchange_server : True ...","title":"Explain with examples"},{"location":"en/task_judger/","text":"Task Judger evaluates agent outputs and assigns rewards during training. This page covers built-in judgers for common scenarios and how to create custom judgers for specific evaluation needs. When to use the task judger Is task judger necessary for all tasks? No : There are two options to generate reward: Compute reward inside the user-defined workflow ( WorkflowOutput.reward is not None ) Compute reward outside the user-defined workflow ( WorkflowOutput.reward is None ) Task judger is how AgentJet handles out-of-workflow reward computation. Task judger will be Disabled and Ignored when the user-defined workflow returned an effective WorkflowOutput.reward and WorkflowOutput.reward != None Task judger will be Enabled when the user-defined workflow returned WorkflowOutput.reward = None . When to use the task judger : When the user plan to re-used the reward function in multiple other workflows in the future. When the user want to decouple rollout and reward computation logic. When the user want to use our OpenJudge integration to generate Auto Rubrics reward . Overview A Task Judger evaluates the agent's execution results and returns two values: Return Value Type Description raw_reward float Numerical score representing output quality (often 0.0 to 1.0) is_success bool Whether the task was successfully completed These values guide the RL training process, helping agents learn which behaviors produce better outcomes. Base Interface All Task Judgers inherit from BaseJudge and implement the compute_reward method: base_judge.py from ajet.task_judge.base_judge import BaseJudge from ajet.workflow import WorkflowOutput , WorkflowTask class BaseJudge : def __init__ ( self , config ): self . config = config def compute_reward ( self , workflow_task : WorkflowTask , workflow_output : WorkflowOutput ) -> tuple [ float , bool ]: \"\"\" Args: workflow_task: Contains the task data, including metadata with reference answers workflow_output: Contains the agent's output, including metadata with generated answers Returns: tuple: (raw_reward: float, is_success: bool) \"\"\" raise NotImplementedError Built-in Task Judgers AgentJet provides three built-in judgers for common evaluation scenarios: 1. MathAnswerAsJudge Evaluates mathematical answers by exact string matching, designed for tasks where answers are formatted in LaTeX \\boxed{} notation. When to use Math problem solving tasks Tasks with deterministic, exact answers Answers formatted as \\boxed{result} Configuration How it works config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAsJudge Extracts the answer from \\boxed{...} in the agent's output Compares with the reference answer from workflow_task.task.metadata[\"answer\"] Returns (1.0, True) for correct answers, (0.0, False) otherwise Required metadata: Field Source Description final_answer workflow_output.metadata Agent's answer with \\boxed{} format answer workflow_task.task.metadata Reference answer 2. CountdownAnswerAsJudge Evaluates mathematical equations with partial credit for proper formatting. When to use Number puzzle tasks (e.g., Countdown game) Tasks where partial credit is appropriate Need to reward proper formatting even when answer is wrong Configuration Scoring config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : tutorial.example_countdown.countdown_answer_as_judge->CountdownAnswerAsJudge Score Condition 0.0 Invalid or missing answer 0.1 Properly formatted equation but wrong result 1.0 Correct equation and result 3. EnvServiceJudge Delegates evaluation to an external environment service, useful for complex interactive environments. When to use Tasks with external simulators (e.g., AppWorld) Interactive environments with built-in evaluators config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : ajet.task_judge.env_service_as_judge->EnvServiceJudge Creating Custom Task Judgers For specialized evaluation needs, create your own judger by inheriting BaseJudge : Custom Judger Steps Implement Your Judger Create a new file with your custom judger class. Configure Your Judger Point to your custom class in the YAML configuration. Pass Data to the Judger Populate `workflow_output.metadata` with the data your judger needs. Step 1: Implement Your Judger tutorial/my_task/my_judge.py from ajet.task_judge.base_judge import BaseJudge from ajet.workflow import WorkflowOutput , WorkflowTask class MyCustomJudge ( BaseJudge ): def __init__ ( self , config ): super () . __init__ ( config ) self . threshold = 0.8 def compute_reward ( self , workflow_task : WorkflowTask , workflow_output : WorkflowOutput ) -> tuple [ float , bool ]: agent_answer = workflow_output . metadata . get ( \"final_answer\" , \"\" ) reference_answer = workflow_task . task . metadata . get ( \"answer\" , \"\" ) similarity = self . _compute_similarity ( agent_answer , reference_answer ) is_success = similarity >= self . threshold return similarity , is_success def _compute_similarity ( self , text1 : str , text2 : str ) -> float : return len ( set ( text1 . split ()) & set ( text2 . split ())) / max ( len ( text1 . split ()), len ( text2 . split ()), 1 ) Step 2: Configure Your Judger config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : tutorial.my_task.my_judge->MyCustomJudge Step 3: Pass Data to the Judger workflow.py class MyWorkflow ( Workflow ): async def execute ( self , task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : final_answer = await self . agent . reply ( msg ) return WorkflowOutput ( reward = None , # Will be filled by the judger metadata = { \"final_answer\" : final_answer , } ) Configuration Summary config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : ajet.task_judge.<module>-><ClassName> Next Steps Configuration Complete reference for all configuration options. Math Agent See MathAnswerAsJudge in a complete training example.","title":"Task Judger"},{"location":"en/task_judger/#overview","text":"A Task Judger evaluates the agent's execution results and returns two values: Return Value Type Description raw_reward float Numerical score representing output quality (often 0.0 to 1.0) is_success bool Whether the task was successfully completed These values guide the RL training process, helping agents learn which behaviors produce better outcomes.","title":"Overview"},{"location":"en/task_judger/#base-interface","text":"All Task Judgers inherit from BaseJudge and implement the compute_reward method: base_judge.py from ajet.task_judge.base_judge import BaseJudge from ajet.workflow import WorkflowOutput , WorkflowTask class BaseJudge : def __init__ ( self , config ): self . config = config def compute_reward ( self , workflow_task : WorkflowTask , workflow_output : WorkflowOutput ) -> tuple [ float , bool ]: \"\"\" Args: workflow_task: Contains the task data, including metadata with reference answers workflow_output: Contains the agent's output, including metadata with generated answers Returns: tuple: (raw_reward: float, is_success: bool) \"\"\" raise NotImplementedError","title":"Base Interface"},{"location":"en/task_judger/#built-in-task-judgers","text":"AgentJet provides three built-in judgers for common evaluation scenarios:","title":"Built-in Task Judgers"},{"location":"en/task_judger/#1-mathanswerasjudge","text":"Evaluates mathematical answers by exact string matching, designed for tasks where answers are formatted in LaTeX \\boxed{} notation. When to use Math problem solving tasks Tasks with deterministic, exact answers Answers formatted as \\boxed{result} Configuration How it works config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : tutorial.example_math_agent.math_answer_as_judge->MathAnswerAsJudge Extracts the answer from \\boxed{...} in the agent's output Compares with the reference answer from workflow_task.task.metadata[\"answer\"] Returns (1.0, True) for correct answers, (0.0, False) otherwise Required metadata: Field Source Description final_answer workflow_output.metadata Agent's answer with \\boxed{} format answer workflow_task.task.metadata Reference answer","title":"1. MathAnswerAsJudge"},{"location":"en/task_judger/#2-countdownanswerasjudge","text":"Evaluates mathematical equations with partial credit for proper formatting. When to use Number puzzle tasks (e.g., Countdown game) Tasks where partial credit is appropriate Need to reward proper formatting even when answer is wrong Configuration Scoring config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : tutorial.example_countdown.countdown_answer_as_judge->CountdownAnswerAsJudge Score Condition 0.0 Invalid or missing answer 0.1 Properly formatted equation but wrong result 1.0 Correct equation and result","title":"2. CountdownAnswerAsJudge"},{"location":"en/task_judger/#3-envservicejudge","text":"Delegates evaluation to an external environment service, useful for complex interactive environments. When to use Tasks with external simulators (e.g., AppWorld) Interactive environments with built-in evaluators config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : ajet.task_judge.env_service_as_judge->EnvServiceJudge","title":"3. EnvServiceJudge"},{"location":"en/task_judger/#creating-custom-task-judgers","text":"For specialized evaluation needs, create your own judger by inheriting BaseJudge : Custom Judger Steps Implement Your Judger Create a new file with your custom judger class. Configure Your Judger Point to your custom class in the YAML configuration. Pass Data to the Judger Populate `workflow_output.metadata` with the data your judger needs.","title":"Creating Custom Task Judgers"},{"location":"en/task_judger/#step-1-implement-your-judger","text":"tutorial/my_task/my_judge.py from ajet.task_judge.base_judge import BaseJudge from ajet.workflow import WorkflowOutput , WorkflowTask class MyCustomJudge ( BaseJudge ): def __init__ ( self , config ): super () . __init__ ( config ) self . threshold = 0.8 def compute_reward ( self , workflow_task : WorkflowTask , workflow_output : WorkflowOutput ) -> tuple [ float , bool ]: agent_answer = workflow_output . metadata . get ( \"final_answer\" , \"\" ) reference_answer = workflow_task . task . metadata . get ( \"answer\" , \"\" ) similarity = self . _compute_similarity ( agent_answer , reference_answer ) is_success = similarity >= self . threshold return similarity , is_success def _compute_similarity ( self , text1 : str , text2 : str ) -> float : return len ( set ( text1 . split ()) & set ( text2 . split ())) / max ( len ( text1 . split ()), len ( text2 . split ()), 1 )","title":"Step 1: Implement Your Judger"},{"location":"en/task_judger/#step-2-configure-your-judger","text":"config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : tutorial.my_task.my_judge->MyCustomJudge","title":"Step 2: Configure Your Judger"},{"location":"en/task_judger/#step-3-pass-data-to-the-judger","text":"workflow.py class MyWorkflow ( Workflow ): async def execute ( self , task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : final_answer = await self . agent . reply ( msg ) return WorkflowOutput ( reward = None , # Will be filled by the judger metadata = { \"final_answer\" : final_answer , } )","title":"Step 3: Pass Data to the Judger"},{"location":"en/task_judger/#configuration-summary","text":"config.yaml ajet : task_judge : judge_type : customized_protocol judge_protocol : ajet.task_judge.<module>-><ClassName>","title":"Configuration Summary"},{"location":"en/task_judger/#next-steps","text":"","title":"Next Steps"},{"location":"en/tune_your_first_agent/","text":"In this document, we demonstrate how to implement and train, from scratch, an agent that can use Python to perform calculations and solve 'gsm8k' math problems. Training Pipeline Overview Define agent workflow Create your agent using AgentScope/Langchain/OpenaiSDK or only http requests, wrap it in a Workflow class. Define reward Configure how the agent's outputs are evaluated and scored. Prepare dataset Set up the dataset and configure the task reader. Debug (Optional) Test your workflow in debug mode before full training. Start training Launch the training process and track progress. Checkout the full code of this example by clicking here Step 1: \u2728Define agent Workflow + Reward First of all, create a directory for this training project: tutorial/example_math_agent \u251c\u2500\u2500 math_agent.py \u2514\u2500\u2500 math_agent.yaml Next, define your workflow (or convert an existing workflow). Here we use AgentScope to implement this agent. You can toggle two code before and after convertion to see the difference. If you prefer langchain or openai sdk, please refer to this article . math_agent.py - AgentJet Workflow (After Convertion) Original Workflow (Before Convertion) math_agent.py class MathToolWorkflow ( Workflow ): # \u2728\u2728 inherit `Workflow` class name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer }) math_agent.py class MathToolWorkflow ( object ): name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = DashScopeChatModel ( model = 'qwen-max' ), formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer }) Step 2: \u2728Prepare dataset Data Sources AgentJet provides multiple ways to read data: Read from local files on disk Read from a Hugging Face repo Read from an EnvService Download the openai/gsm8k dataset: python scripts/download_dataset.py --target = openai/gsm8k --path = /the/path/to/store/dataset Now, we have obtained all materials required to train the agent. math_agent.yaml - Configuration Yaml # ------------------ main configuration ------------------ ajet : project_name : example_math_agent task_reader : type : huggingface_dat_repo # \u2728\u2728\u2728\u2728 `env_service` or `dataset_file` or `huggingface_dat_repo` # effective when `type: huggingface_dat_repo` huggingface_dat_repo : dataset_path : 'openai/gsm8k' training_split : \"train\" validation_split : \"test\" task_judge : # \u2728\u2728\u2728\u2728 null, because in this certain case, we write reward function together with workflow judge_protocol : null model : # \u2728\u2728\u2728\u2728 set the model to be trained path : Qwen/Qwen2.5-7B rollout : user_workflow : \"tutorial.example_math_agent.math_agent->ExampleMathLearn\" # \u2728\u2728\u2728\u2728 write and select workflow num_repeat : 6 # grpo `n` tensor_model_parallel_size : 1 # vllm tp max_response_length_in_one_turn : 1024 max_model_len : 10000 data : train_batch_size : 100 max_prompt_length : 3000 max_response_length : 7000 debug : debug_max_parallel : 1 debug_first_n_tasks : 1 trainer_common : save_freq : 100 test_freq : 100 total_epochs : 100 logger : swanlab # ------------------ do not modify ------------------ hydra : searchpath : - file://ajet/default_config - file://ajet/default_config/verl - file://ajet/default_config/trinity # ------------------ do not modify ------------------ defaults : - verl_default - trinity_default - ajet_default - _self_ Configuration Parameters Category Parameter Description Example Value Project project_name Name of the training project example_math_agent Task Reader type Type of data source to read tasks from huggingface_dat_repo (options: env_service , dataset_file , huggingface_dat_repo ) dataset_path Path or identifier of the dataset openai/gsm8k training_split Dataset split used for training train validation_split Dataset split used for validation/testing test Model path Path or identifier of the model to be trained Qwen/Qwen2.5-7B Rollout user_workflow Python module path to the workflow class tutorial.example_math_agent.math_agent->ExampleMathLearn num_repeat Number of rollout repeats per task (GRPO n parameter) 6 tensor_model_parallel_size vLLM tensor parallelism size 1 max_response_length_in_one_turn Maximum token length for a single agent response 1024 max_model_len Maximum total context length for the model 10000 Data train_batch_size Number of tasks per training batch 100 max_prompt_length Maximum token length for input prompts 3000 max_response_length Maximum token length for model responses 7000 Debug debug_max_parallel Maximum parallel workers in debug mode 1 debug_first_n_tasks Number of tasks to process in debug mode 1 Trainer save_freq Frequency (in steps) to save model checkpoints 100 test_freq Frequency (in steps) to run validation 100 total_epochs Total number of training epochs 100 logger Logging backend for experiment tracking swanlab Task Judge judge_protocol Protocol for judging task completion null (reward is computed in workflow) Step 3: \u2728Debug (Optional) Before full training, you can run some test in debug mode, using raw base model to test whether bug exists. We choose VSCode to debug because it is open-source and fast. VS Code Debugging You can create .vscode/launch.json for breakpoint debugging: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"tutorial/example_math_agent/math_agent.yaml\" ], \"env\" : {} } ] } After .vscode/launch.json is created, press F5 to start debugging. (Do not forget to configure python venv path in VSCode.) For more debugging techniques, please refer to debugging guidelines . Step 4: \u2728Start Training After debugging, launch the full training: ajet --conf tutorial/example_math_agent/math_agent.yaml Output Location Training logs and checkpoints will be saved default to: ./saved_experiments/{exp_yaml_file_name}/ Full Code tutorial/example_math_agent/math_agent.py - AgentJet Workflow (After Convertion) tutorial/example_math_agent/math_agent.yaml - Configuration Yaml import re from loguru import logger from agentscope.message import Msg from agentscope.agent import ReActAgent from agentscope.formatter import DashScopeChatFormatter from agentscope.memory import InMemoryMemory from agentscope.tool import Toolkit , execute_python_code from ajet import AjetTuner , Workflow , WorkflowOutput , WorkflowTask def extract_final_answer ( result ) -> str : \"\"\"Extract the final answer from the agent's response.\"\"\" try : if ( hasattr ( result , \"metadata\" ) and isinstance ( result . metadata , dict ) and \"result\" in result . metadata ): return result . metadata [ \"result\" ] if hasattr ( result , \"content\" ): if isinstance ( result . content , dict ) and \"result\" in result . content : return result . content [ \"result\" ] return str ( result . content ) return str ( result ) except Exception as e : logger . warning ( f \"Extract final answer error: { e } . Raw: { result } \" ) return str ( result ) system_prompt = \"\"\" You are an agent specialized in solving math problems with tools. Please solve the math problem given to you. You can write and execute Python code to perform calculation or verify your answer. You should return your final answer within \\\\ boxed{{}}. \"\"\" class MathToolWorkflow ( Workflow ): # \u2728\u2728 inherit `Workflow` class name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer }) # ------------------ main configuration ------------------ ajet : project_name : example_math_agent task_reader : type : huggingface_dat_repo # \u2728\u2728\u2728\u2728 `env_service` or `dataset_file` or `huggingface_dat_repo` # effective when `type: huggingface_dat_repo` huggingface_dat_repo : dataset_path : 'openai/gsm8k' # '/mnt/data_cpfs/dataset_cache/openai/gsm8k/main' training_split : \"train\" validation_split : \"test\" model : # \u2728\u2728\u2728\u2728 set the model to be trained path : Qwen/Qwen2___5-7B-Instruct # /mnt/data_cpfs/model_cache/modelscope/hub/Qwen/Qwen/Qwen2___5-7B-Instruct rollout : user_workflow : \"tutorial/example_math_agent/math_agent.py->MathToolWorkflow\" # \u2728\u2728\u2728\u2728 write and select workflow num_repeat : 6 # grpo `n` tensor_model_parallel_size : 1 # vllm tp max_response_length_in_one_turn : 1024 max_model_len : 10000 task_judge : # \u2728\u2728\u2728\u2728 null, because in this certain case, we write reward function together with workflow judge_protocol : null data : train_batch_size : 100 max_prompt_length : 3000 max_response_length : 7000 debug : debug_max_parallel : 1 debug_first_n_tasks : 1 trainer_common : save_freq : 100 test_freq : 100 total_epochs : 100 logger : swanlab # ------------------ do not modify ------------------ hydra : searchpath : - file://ajet/default_config - file://ajet/default_config/verl - file://ajet/default_config/trinity # ------------------ do not modify ------------------ defaults : - verl_default - trinity_default - ajet_default - _self_ Next Steps Workflow Learn to define trainable workflows and multi-agent setups. Data Pipeline Configure data loading from various sources. Task Judger Set up reward functions for your training. Math Agent Example See the complete Math Agent implementation.","title":"Tune Your First Agent"},{"location":"en/tune_your_first_agent/#step-1-define-agent-workflow-reward","text":"First of all, create a directory for this training project: tutorial/example_math_agent \u251c\u2500\u2500 math_agent.py \u2514\u2500\u2500 math_agent.yaml Next, define your workflow (or convert an existing workflow). Here we use AgentScope to implement this agent. You can toggle two code before and after convertion to see the difference. If you prefer langchain or openai sdk, please refer to this article . math_agent.py - AgentJet Workflow (After Convertion) Original Workflow (Before Convertion) math_agent.py class MathToolWorkflow ( Workflow ): # \u2728\u2728 inherit `Workflow` class name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer }) math_agent.py class MathToolWorkflow ( object ): name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = DashScopeChatModel ( model = 'qwen-max' ), formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer })","title":"Step 1: \u2728Define agent Workflow + Reward"},{"location":"en/tune_your_first_agent/#step-2-prepare-dataset","text":"Data Sources AgentJet provides multiple ways to read data: Read from local files on disk Read from a Hugging Face repo Read from an EnvService Download the openai/gsm8k dataset: python scripts/download_dataset.py --target = openai/gsm8k --path = /the/path/to/store/dataset Now, we have obtained all materials required to train the agent. math_agent.yaml - Configuration Yaml # ------------------ main configuration ------------------ ajet : project_name : example_math_agent task_reader : type : huggingface_dat_repo # \u2728\u2728\u2728\u2728 `env_service` or `dataset_file` or `huggingface_dat_repo` # effective when `type: huggingface_dat_repo` huggingface_dat_repo : dataset_path : 'openai/gsm8k' training_split : \"train\" validation_split : \"test\" task_judge : # \u2728\u2728\u2728\u2728 null, because in this certain case, we write reward function together with workflow judge_protocol : null model : # \u2728\u2728\u2728\u2728 set the model to be trained path : Qwen/Qwen2.5-7B rollout : user_workflow : \"tutorial.example_math_agent.math_agent->ExampleMathLearn\" # \u2728\u2728\u2728\u2728 write and select workflow num_repeat : 6 # grpo `n` tensor_model_parallel_size : 1 # vllm tp max_response_length_in_one_turn : 1024 max_model_len : 10000 data : train_batch_size : 100 max_prompt_length : 3000 max_response_length : 7000 debug : debug_max_parallel : 1 debug_first_n_tasks : 1 trainer_common : save_freq : 100 test_freq : 100 total_epochs : 100 logger : swanlab # ------------------ do not modify ------------------ hydra : searchpath : - file://ajet/default_config - file://ajet/default_config/verl - file://ajet/default_config/trinity # ------------------ do not modify ------------------ defaults : - verl_default - trinity_default - ajet_default - _self_","title":"Step 2: \u2728Prepare dataset"},{"location":"en/tune_your_first_agent/#configuration-parameters","text":"Category Parameter Description Example Value Project project_name Name of the training project example_math_agent Task Reader type Type of data source to read tasks from huggingface_dat_repo (options: env_service , dataset_file , huggingface_dat_repo ) dataset_path Path or identifier of the dataset openai/gsm8k training_split Dataset split used for training train validation_split Dataset split used for validation/testing test Model path Path or identifier of the model to be trained Qwen/Qwen2.5-7B Rollout user_workflow Python module path to the workflow class tutorial.example_math_agent.math_agent->ExampleMathLearn num_repeat Number of rollout repeats per task (GRPO n parameter) 6 tensor_model_parallel_size vLLM tensor parallelism size 1 max_response_length_in_one_turn Maximum token length for a single agent response 1024 max_model_len Maximum total context length for the model 10000 Data train_batch_size Number of tasks per training batch 100 max_prompt_length Maximum token length for input prompts 3000 max_response_length Maximum token length for model responses 7000 Debug debug_max_parallel Maximum parallel workers in debug mode 1 debug_first_n_tasks Number of tasks to process in debug mode 1 Trainer save_freq Frequency (in steps) to save model checkpoints 100 test_freq Frequency (in steps) to run validation 100 total_epochs Total number of training epochs 100 logger Logging backend for experiment tracking swanlab Task Judge judge_protocol Protocol for judging task completion null (reward is computed in workflow)","title":"Configuration Parameters"},{"location":"en/tune_your_first_agent/#step-3-debug-optional","text":"Before full training, you can run some test in debug mode, using raw base model to test whether bug exists. We choose VSCode to debug because it is open-source and fast. VS Code Debugging You can create .vscode/launch.json for breakpoint debugging: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"Python Debugger: Launch rollout\" , \"type\" : \"debugpy\" , \"request\" : \"launch\" , \"module\" : \"ajet.launcher\" , \"console\" : \"integratedTerminal\" , \"args\" : [ \"--backbone\" , \"debug\" , \"--conf\" , \"tutorial/example_math_agent/math_agent.yaml\" ], \"env\" : {} } ] } After .vscode/launch.json is created, press F5 to start debugging. (Do not forget to configure python venv path in VSCode.) For more debugging techniques, please refer to debugging guidelines .","title":"Step 3: \u2728Debug (Optional)"},{"location":"en/tune_your_first_agent/#step-4-start-training","text":"After debugging, launch the full training: ajet --conf tutorial/example_math_agent/math_agent.yaml Output Location Training logs and checkpoints will be saved default to: ./saved_experiments/{exp_yaml_file_name}/","title":"Step 4: \u2728Start Training"},{"location":"en/tune_your_first_agent/#full-code","text":"tutorial/example_math_agent/math_agent.py - AgentJet Workflow (After Convertion) tutorial/example_math_agent/math_agent.yaml - Configuration Yaml import re from loguru import logger from agentscope.message import Msg from agentscope.agent import ReActAgent from agentscope.formatter import DashScopeChatFormatter from agentscope.memory import InMemoryMemory from agentscope.tool import Toolkit , execute_python_code from ajet import AjetTuner , Workflow , WorkflowOutput , WorkflowTask def extract_final_answer ( result ) -> str : \"\"\"Extract the final answer from the agent's response.\"\"\" try : if ( hasattr ( result , \"metadata\" ) and isinstance ( result . metadata , dict ) and \"result\" in result . metadata ): return result . metadata [ \"result\" ] if hasattr ( result , \"content\" ): if isinstance ( result . content , dict ) and \"result\" in result . content : return result . content [ \"result\" ] return str ( result . content ) return str ( result ) except Exception as e : logger . warning ( f \"Extract final answer error: { e } . Raw: { result } \" ) return str ( result ) system_prompt = \"\"\" You are an agent specialized in solving math problems with tools. Please solve the math problem given to you. You can write and execute Python code to perform calculation or verify your answer. You should return your final answer within \\\\ boxed{{}}. \"\"\" class MathToolWorkflow ( Workflow ): # \u2728\u2728 inherit `Workflow` class name : str = \"math_agent_workflow\" async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # run agentscope query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), # \u2728\u2728 compared with a normal agentscope agent, here is the difference! formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) # compute reward reference_answer = workflow_task . task . metadata [ \"answer\" ] . split ( \"####\" )[ - 1 ] . strip () match = re . search ( r \" \\\\ boxed\\{([^}]*)\\}\" , final_answer ) if match : is_success = ( match . group ( 1 ) == reference_answer ) else : is_success = False return WorkflowOutput ( reward = ( 1.0 if is_success else 0.0 ), metadata = { \"final_answer\" : final_answer }) # ------------------ main configuration ------------------ ajet : project_name : example_math_agent task_reader : type : huggingface_dat_repo # \u2728\u2728\u2728\u2728 `env_service` or `dataset_file` or `huggingface_dat_repo` # effective when `type: huggingface_dat_repo` huggingface_dat_repo : dataset_path : 'openai/gsm8k' # '/mnt/data_cpfs/dataset_cache/openai/gsm8k/main' training_split : \"train\" validation_split : \"test\" model : # \u2728\u2728\u2728\u2728 set the model to be trained path : Qwen/Qwen2___5-7B-Instruct # /mnt/data_cpfs/model_cache/modelscope/hub/Qwen/Qwen/Qwen2___5-7B-Instruct rollout : user_workflow : \"tutorial/example_math_agent/math_agent.py->MathToolWorkflow\" # \u2728\u2728\u2728\u2728 write and select workflow num_repeat : 6 # grpo `n` tensor_model_parallel_size : 1 # vllm tp max_response_length_in_one_turn : 1024 max_model_len : 10000 task_judge : # \u2728\u2728\u2728\u2728 null, because in this certain case, we write reward function together with workflow judge_protocol : null data : train_batch_size : 100 max_prompt_length : 3000 max_response_length : 7000 debug : debug_max_parallel : 1 debug_first_n_tasks : 1 trainer_common : save_freq : 100 test_freq : 100 total_epochs : 100 logger : swanlab # ------------------ do not modify ------------------ hydra : searchpath : - file://ajet/default_config - file://ajet/default_config/verl - file://ajet/default_config/trinity # ------------------ do not modify ------------------ defaults : - verl_default - trinity_default - ajet_default - _self_","title":"Full Code"},{"location":"en/tune_your_first_agent/#next-steps","text":"","title":"Next Steps"},{"location":"en/visualization/","text":"Monitoring training progress through visualized metrics is essential for understanding model behavior and tuning hyperparameters effectively. Supported Visualization Tools SwanLab \u2b50 Modern experiment tracking platform designed for AI research. Recommended. WandB Weights & Biases experiment tracking platform. Console Simple text-based logging to standard output. Quick Start with SwanLab Step 1: Configure SwanLab Simply set the logger backend to swanlab in your YAML configuration: config.yaml ajet : trainer_common : logger : swanlab Step 2: Start Training Launch your training as usual: ajet --conf tutorial/example_math_agent/math_agent.yaml Step 3: View Training Curves Automatic Tracking Once training starts, SwanLab will automatically: Track key metrics (reward, success rate, loss, etc.) Generate real-time training curves Provide a web dashboard for visualization You can access the SwanLab dashboard through the URL printed in the training logs. Understanding Training Curves Key Metrics to Monitor Metric Description Reward Average reward per episode, indicating task performance Success Rate Percentage of successfully completed tasks Loss Training loss from the policy optimization algorithm Response Length Average length of model responses KL Divergence Divergence between current and reference policy Interpreting the Curves Example Training Curve: A typical reward curve shows: Phase Description Initial Reward may be low or unstable as the model explores Learning Reward gradually increases as the model learns better strategies Convergence Reward plateaus when the model reaches optimal performance What to Look For Rising trend : Indicates successful learning Plateaus : May indicate convergence or need for hyperparameter adjustment Sudden drops : Could signal instability or overfitting Best Practices Monitor Multiple Runs Compare different hyperparameter settings by running multiple experiments and comparing their curves side-by-side. Set Appropriate Logging Frequency Balance between logging detail and training overhead: config.yaml ajet : trainer_common : log_freq : 1 # Log every N steps Save Checkpoints at Key Points Configure checkpoint saving to preserve models at peak performance: config.yaml ajet : trainer_common : save_freq : 100 # Save every 100 steps Next Steps Beast Logger Token-level debugging and visualization. Data Generation Auto-generate training data from documents. SwanLab Docs Official SwanLab documentation.","title":"Visualization"},{"location":"en/visualization/#supported-visualization-tools","text":"","title":"Supported Visualization Tools"},{"location":"en/visualization/#quick-start-with-swanlab","text":"","title":"Quick Start with SwanLab"},{"location":"en/visualization/#step-1-configure-swanlab","text":"Simply set the logger backend to swanlab in your YAML configuration: config.yaml ajet : trainer_common : logger : swanlab","title":"Step 1: Configure SwanLab"},{"location":"en/visualization/#step-2-start-training","text":"Launch your training as usual: ajet --conf tutorial/example_math_agent/math_agent.yaml","title":"Step 2: Start Training"},{"location":"en/visualization/#step-3-view-training-curves","text":"Automatic Tracking Once training starts, SwanLab will automatically: Track key metrics (reward, success rate, loss, etc.) Generate real-time training curves Provide a web dashboard for visualization You can access the SwanLab dashboard through the URL printed in the training logs.","title":"Step 3: View Training Curves"},{"location":"en/visualization/#understanding-training-curves","text":"","title":"Understanding Training Curves"},{"location":"en/visualization/#key-metrics-to-monitor","text":"Metric Description Reward Average reward per episode, indicating task performance Success Rate Percentage of successfully completed tasks Loss Training loss from the policy optimization algorithm Response Length Average length of model responses KL Divergence Divergence between current and reference policy","title":"Key Metrics to Monitor"},{"location":"en/visualization/#interpreting-the-curves","text":"Example Training Curve: A typical reward curve shows: Phase Description Initial Reward may be low or unstable as the model explores Learning Reward gradually increases as the model learns better strategies Convergence Reward plateaus when the model reaches optimal performance What to Look For Rising trend : Indicates successful learning Plateaus : May indicate convergence or need for hyperparameter adjustment Sudden drops : Could signal instability or overfitting","title":"Interpreting the Curves"},{"location":"en/visualization/#best-practices","text":"","title":"Best Practices"},{"location":"en/visualization/#monitor-multiple-runs","text":"Compare different hyperparameter settings by running multiple experiments and comparing their curves side-by-side.","title":"Monitor Multiple Runs"},{"location":"en/visualization/#set-appropriate-logging-frequency","text":"Balance between logging detail and training overhead: config.yaml ajet : trainer_common : log_freq : 1 # Log every N steps","title":"Set Appropriate Logging Frequency"},{"location":"en/visualization/#save-checkpoints-at-key-points","text":"Configure checkpoint saving to preserve models at peak performance: config.yaml ajet : trainer_common : save_freq : 100 # Save every 100 steps","title":"Save Checkpoints at Key Points"},{"location":"en/visualization/#next-steps","text":"","title":"Next Steps"},{"location":"en/workflow/","text":"This tutorial introduces how to define a trainable workflow. AgentJet provides two convenient and mutually compatible ways to wrap your Workflow: Simple : Emphasizes simplicity, ease of use, and readability Advanced : Emphasizes flexibility, controllability, and extensibility In this article we use AgentScope framework for demonstration. For other frameworks (OpenAI SDK, Langchain, HTTP Requests), please follow the same pattern. Simple Practice Simple Practice Abstract Simply set model argument in AgentScope ReActAgent argument to tuner.as_agentscope_model() when initializing your agent. Wrap your code with class MyWorkflow(Workflow) and your agent is ready to be tuned. 1. When to Use This Simple Practice Choose Simple Practice If You... Know exactly which agents should be trained, or the number of agents is small Already finished basic debugging of your workflow Do not need to change which agents are trained on the fly 2. Convert Your Workflow to AgentJet Trainable Workflow The very first step is to create a class as a container to wrap your code: converted_workflow.py - AgentJet Workflow from ajet import AjetTuner , Workflow , WorkflowOutput , WorkflowTask class MyWorkflow ( Workflow ): async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # ... your ReActAgent workflow here \ud83d\udee9\ufe0f ... return WorkflowOutput ( reward =... , metadata = { ... }) Next, use the tuner argument, call its tuner.as_agentscope_model() method: Before After model = DashScopeChatModel ( model_name = \"qwen-max\" , stream = False ) # \ud83d\udee9\ufe0f change here agent_instance = ReActAgent ( name = f \"Friday\" , sys_prompt = \"You are a helpful assistant\" , model = model , formatter = DashScopeChatFormatter (), ) model = tuner . as_agentscope_model () # \ud83d\udee9\ufe0f change here agent_instance = ReActAgent ( name = f \"Friday\" , sys_prompt = \"You are a helpful assistant\" , model = model , formatter = DashScopeChatFormatter (), ) AjetTuner AjetTuner also has .as_raw_openai_sdk_client() and .as_oai_baseurl_apikey() method. But .as_agentscope_model() is more convenient for AgentScope agent workflow. 3. Code Example Math Agent Training a math agent that can write Python code to solve mathematical problems. Learning to Ask Learning to ask questions like a doctor for medical consultation scenarios. Countdown Game Writing a countdown game using AgentScope and solving it with RL. Frozen Lake Solving a frozen lake walking puzzle using AgentJet's reinforcement learning. Advanced Practice Advanced Practice Abstract The tuner.as_agentscope_model() function has hidden parameters, please further complete them to tell AgentJet the identity of agents. The ajet.Workflow class has hidden attribute trainable_targets , please assign it manually to narrow down agents to be tuned. 1. When to Use Advanced Practice When designing a multi-agent collaborative workflow where each agent plays a different target_tag , AgentJet provides enhanced training and debugging capabilities. Multi-Agent Benefits With a multi-agent setup, you can: Precisely control which agents are fine-tuned Explicitly define the default model for agents not being trained Switch trainable targets on the fly without modifying source code 1. How to promote to advanced agent scenario: Simple, there are only two more issues that should be take care of in addition: i. .as_agentscope_model has three hidden (optional) parameters, complete them for each agent. parameter explanation agent_name The name of this agent target_tag A tag that mark the agent category debug_model The model used when this agent is not being tuned as_agentscope_model() parameters model_for_an_agent = tuner . as_agentscope_model ( agent_name = \"AgentFriday\" , # the name of this agent target_tag = \"Agent_Type_1\" , # `target_tag in self.trainable_targets` means we train this agent, otherwise we do not train this agent. debug_model = OpenAIChatModel ( model_name = \"Qwen/Qwen3-235B-A22B-Instruct-2507\" , stream = False , api_key = \"api_key\" , ), # the model used when this agent is not in `self.trainable_targets` ) ii. Workflow has a hidden (optional) attribute called trainable_targets , config it. trainable_targets value explanation trainable_targets = None All agents using as_agentscope_model will be trained trainable_targets = [\"Agent_Type_1\", \"Agent_Type_2\"] Agents with target_tag=Agent_Type_1 , target_tag=Agent_Type_2 , ... will be trained trainable_targets = [] Illegal, no agents are trained Scenario Model Used target_tag in trainable_targets Trainable model target_tag NOT in trainable_targets Registered debug_model Warning Regardless of target_tag differences, all agents share a single model instance (one model weight to play different roles, the model receives different perceptions when playing different roles). 2. Multi-Agent Example Here's a complete example with multiple agent roles (Werewolves game): tutorial/example_werewolves/start.py class ExampleWerewolves ( Workflow ): trainable_targets : List [ str ] | None = Field ( default = [ \"werewolf\" ], description = \"List of agents to be fine-tuned.\" ) async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # ensure trainable targets is legal assert self . trainable_targets is not None , \"trainable_targets cannot be None in ExampleWerewolves (because we want to demonstrate a explicit multi-agent case).\" # bad guys and good guys cannot be trained simultaneously # (because mix-cooperation-competition MARL needs too many advanced techniques to be displayed here) if \"werewolf\" in self . trainable_targets : assert len ( self . trainable_targets ) == 1 , \"Cannot train hostile roles simultaneously.\" else : assert len ( self . trainable_targets ) != 0 , \"No trainable targets specified.\" # make and shuffle roles (fix random seed for reproducibility) roles = [ \"werewolf\" ] * 3 + [ \"villager\" ] * 3 + [ \"seer\" , \"witch\" , \"hunter\" ] task_id = workflow_task . task . metadata [ \"random_number\" ] np . random . seed ( int ( task_id )) np . random . shuffle ( roles ) # initialize agents players = [] for i , role in enumerate ( roles ): default_model = OpenAIChatModel ( model_name = \"Qwen/Qwen3-235B-A22B-Instruct-2507\" , stream = False , api_key = \"no_api_key\" , ) model_for_this_agent = tuner . as_agentscope_model ( agent_name = f \"Player { i + 1 } \" , # the name of this agent target_tag = role , # `target_tag in self.trainable_targets` means we train this agent, otherwise we do not train this agent. debug_model = default_model , # the model used when this agent is not in `self.trainable_targets` ) agent = ReActAgent ( name = f \"Player { i + 1 } \" , sys_prompt = get_official_agent_prompt ( f \"Player { i + 1 } \" ), model = model_for_this_agent , formatter = DashScopeMultiAgentFormatter () if role in self . trainable_targets else OpenAIMultiAgentFormatter (), max_iters = 3 if role in self . trainable_targets else 5 , ) # agent.set_console_output_enabled(False) players += [ agent ] # reward condition try : good_guy_win = await werewolves_game ( players , roles ) raw_reward = 0 is_success = False if ( good_guy_win and self . trainable_targets [ 0 ] != \"werewolf\" ) or ( not good_guy_win and self . trainable_targets [ 0 ] == \"werewolf\" ): raw_reward = 1 is_success = True logger . warning ( f \"Raw reward: { raw_reward } \" ) logger . warning ( f \"Is success: { is_success } \" ) except BadGuyException as e : logger . bind ( exception = True ) . exception ( f \"Error during game execution. Game cannot continue, whatever the cause, let's punish trainable agents (Although they maybe innocent).\" ) raw_reward = - 0.1 is_success = False except Exception as e : logger . bind ( exception = True ) . exception ( f \"Error during game execution. Game cannot continue, whatever the cause, let's punish trainable agents (Although they maybe innocent).\" ) raw_reward = - 0.1 is_success = False return WorkflowOutput ( reward = raw_reward , is_success = is_success ) Configuration Flexibility In this example: role describes an agent's in-game identity (werewolf, villager, etc.) chosen_model defines the default model when the role is not being trained You can flexibly switch training targets by modifying trainable_targets TinkerJet Wrapping and training your agent on a machine without GPU. Working in progress and coming soon. Next Steps Data Pipeline Configure data loading from files, HuggingFace, or environments. Task Judger Set up reward functions to evaluate agent performance.","title":"Workflow"},{"location":"en/workflow/#simple-practice","text":"Simple Practice Abstract Simply set model argument in AgentScope ReActAgent argument to tuner.as_agentscope_model() when initializing your agent. Wrap your code with class MyWorkflow(Workflow) and your agent is ready to be tuned.","title":"Simple Practice"},{"location":"en/workflow/#1-when-to-use-this-simple-practice","text":"Choose Simple Practice If You... Know exactly which agents should be trained, or the number of agents is small Already finished basic debugging of your workflow Do not need to change which agents are trained on the fly","title":"1. When to Use This Simple Practice"},{"location":"en/workflow/#2-convert-your-workflow-to-agentjet-trainable-workflow","text":"The very first step is to create a class as a container to wrap your code: converted_workflow.py - AgentJet Workflow from ajet import AjetTuner , Workflow , WorkflowOutput , WorkflowTask class MyWorkflow ( Workflow ): async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # ... your ReActAgent workflow here \ud83d\udee9\ufe0f ... return WorkflowOutput ( reward =... , metadata = { ... }) Next, use the tuner argument, call its tuner.as_agentscope_model() method: Before After model = DashScopeChatModel ( model_name = \"qwen-max\" , stream = False ) # \ud83d\udee9\ufe0f change here agent_instance = ReActAgent ( name = f \"Friday\" , sys_prompt = \"You are a helpful assistant\" , model = model , formatter = DashScopeChatFormatter (), ) model = tuner . as_agentscope_model () # \ud83d\udee9\ufe0f change here agent_instance = ReActAgent ( name = f \"Friday\" , sys_prompt = \"You are a helpful assistant\" , model = model , formatter = DashScopeChatFormatter (), ) AjetTuner AjetTuner also has .as_raw_openai_sdk_client() and .as_oai_baseurl_apikey() method. But .as_agentscope_model() is more convenient for AgentScope agent workflow.","title":"2. Convert Your Workflow to AgentJet Trainable Workflow"},{"location":"en/workflow/#3-code-example","text":"","title":"3. Code Example"},{"location":"en/workflow/#advanced-practice","text":"Advanced Practice Abstract The tuner.as_agentscope_model() function has hidden parameters, please further complete them to tell AgentJet the identity of agents. The ajet.Workflow class has hidden attribute trainable_targets , please assign it manually to narrow down agents to be tuned.","title":"Advanced Practice"},{"location":"en/workflow/#1-when-to-use-advanced-practice","text":"When designing a multi-agent collaborative workflow where each agent plays a different target_tag , AgentJet provides enhanced training and debugging capabilities. Multi-Agent Benefits With a multi-agent setup, you can: Precisely control which agents are fine-tuned Explicitly define the default model for agents not being trained Switch trainable targets on the fly without modifying source code","title":"1. When to Use Advanced Practice"},{"location":"en/workflow/#1-how-to-promote-to-advanced-agent-scenario","text":"Simple, there are only two more issues that should be take care of in addition: i. .as_agentscope_model has three hidden (optional) parameters, complete them for each agent. parameter explanation agent_name The name of this agent target_tag A tag that mark the agent category debug_model The model used when this agent is not being tuned as_agentscope_model() parameters model_for_an_agent = tuner . as_agentscope_model ( agent_name = \"AgentFriday\" , # the name of this agent target_tag = \"Agent_Type_1\" , # `target_tag in self.trainable_targets` means we train this agent, otherwise we do not train this agent. debug_model = OpenAIChatModel ( model_name = \"Qwen/Qwen3-235B-A22B-Instruct-2507\" , stream = False , api_key = \"api_key\" , ), # the model used when this agent is not in `self.trainable_targets` ) ii. Workflow has a hidden (optional) attribute called trainable_targets , config it. trainable_targets value explanation trainable_targets = None All agents using as_agentscope_model will be trained trainable_targets = [\"Agent_Type_1\", \"Agent_Type_2\"] Agents with target_tag=Agent_Type_1 , target_tag=Agent_Type_2 , ... will be trained trainable_targets = [] Illegal, no agents are trained Scenario Model Used target_tag in trainable_targets Trainable model target_tag NOT in trainable_targets Registered debug_model Warning Regardless of target_tag differences, all agents share a single model instance (one model weight to play different roles, the model receives different perceptions when playing different roles).","title":"1. How to promote to advanced agent scenario:"},{"location":"en/workflow/#2-multi-agent-example","text":"Here's a complete example with multiple agent roles (Werewolves game): tutorial/example_werewolves/start.py class ExampleWerewolves ( Workflow ): trainable_targets : List [ str ] | None = Field ( default = [ \"werewolf\" ], description = \"List of agents to be fine-tuned.\" ) async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : # ensure trainable targets is legal assert self . trainable_targets is not None , \"trainable_targets cannot be None in ExampleWerewolves (because we want to demonstrate a explicit multi-agent case).\" # bad guys and good guys cannot be trained simultaneously # (because mix-cooperation-competition MARL needs too many advanced techniques to be displayed here) if \"werewolf\" in self . trainable_targets : assert len ( self . trainable_targets ) == 1 , \"Cannot train hostile roles simultaneously.\" else : assert len ( self . trainable_targets ) != 0 , \"No trainable targets specified.\" # make and shuffle roles (fix random seed for reproducibility) roles = [ \"werewolf\" ] * 3 + [ \"villager\" ] * 3 + [ \"seer\" , \"witch\" , \"hunter\" ] task_id = workflow_task . task . metadata [ \"random_number\" ] np . random . seed ( int ( task_id )) np . random . shuffle ( roles ) # initialize agents players = [] for i , role in enumerate ( roles ): default_model = OpenAIChatModel ( model_name = \"Qwen/Qwen3-235B-A22B-Instruct-2507\" , stream = False , api_key = \"no_api_key\" , ) model_for_this_agent = tuner . as_agentscope_model ( agent_name = f \"Player { i + 1 } \" , # the name of this agent target_tag = role , # `target_tag in self.trainable_targets` means we train this agent, otherwise we do not train this agent. debug_model = default_model , # the model used when this agent is not in `self.trainable_targets` ) agent = ReActAgent ( name = f \"Player { i + 1 } \" , sys_prompt = get_official_agent_prompt ( f \"Player { i + 1 } \" ), model = model_for_this_agent , formatter = DashScopeMultiAgentFormatter () if role in self . trainable_targets else OpenAIMultiAgentFormatter (), max_iters = 3 if role in self . trainable_targets else 5 , ) # agent.set_console_output_enabled(False) players += [ agent ] # reward condition try : good_guy_win = await werewolves_game ( players , roles ) raw_reward = 0 is_success = False if ( good_guy_win and self . trainable_targets [ 0 ] != \"werewolf\" ) or ( not good_guy_win and self . trainable_targets [ 0 ] == \"werewolf\" ): raw_reward = 1 is_success = True logger . warning ( f \"Raw reward: { raw_reward } \" ) logger . warning ( f \"Is success: { is_success } \" ) except BadGuyException as e : logger . bind ( exception = True ) . exception ( f \"Error during game execution. Game cannot continue, whatever the cause, let's punish trainable agents (Although they maybe innocent).\" ) raw_reward = - 0.1 is_success = False except Exception as e : logger . bind ( exception = True ) . exception ( f \"Error during game execution. Game cannot continue, whatever the cause, let's punish trainable agents (Although they maybe innocent).\" ) raw_reward = - 0.1 is_success = False return WorkflowOutput ( reward = raw_reward , is_success = is_success ) Configuration Flexibility In this example: role describes an agent's in-game identity (werewolf, villager, etc.) chosen_model defines the default model when the role is not being trained You can flexibly switch training targets by modifying trainable_targets","title":"2. Multi-Agent Example"},{"location":"en/workflow/#tinkerjet","text":"Wrapping and training your agent on a machine without GPU. Working in progress and coming soon.","title":"TinkerJet"},{"location":"en/workflow/#next-steps","text":"","title":"Next Steps"},{"location":"stylesheets/tuner_v2/","text":"class ExampleMathLearn ( Workflow ): async def execute ( self , workflow_task : WorkflowTask , tuner : AjetTuner ) -> WorkflowOutput : from agentscope.agent import ReActAgent from agentscope.formatter import DashScopeChatFormatter from agentscope.memory import InMemoryMemory from agentscope.tool import Toolkit , execute_python_code query = workflow_task . task . main_query self . toolkit = Toolkit () self . toolkit . register_tool_function ( execute_python_code ) self . agent = ReActAgent ( name = \"math_react_agent\" , sys_prompt = system_prompt , model = tuner . as_agentscope_model (), # \ud83c\udf1f this will do the trick formatter = DashScopeChatFormatter (), toolkit = self . toolkit , memory = InMemoryMemory (), max_iters = 2 , ) self . agent . set_console_output_enabled ( False ) msg = Msg ( \"user\" , query , role = \"user\" ) result = await self . agent . reply ( msg ) final_answer = extract_final_answer ( result ) return WorkflowOutput ( reward = None , metadata = { \"final_answer\" : final_answer }) Type 1-2: AgentScope Agents: Triple-M (Multi-Role, Multi-Agent, Multi-Turn) Case roles = [ \"werewolf\" ] * 3 + [ \"villager\" ] * 3 + [ \"seer\" , \"witch\" , \"hunter\" ] players = [] for i , agent_role in enumerate ( roles ): if agent_role != \"werewolf\" : chosen_model_for_current_agent = OpenAIChatModel ( model_name = \"qwen-max\" , stream = False ) else : chosen_model_for_current_agent = OpenAIChatModel ( model_name = \"qwen-plus\" , stream = False ) players += [ ReActAgent ( name = f \"Player { i + 1 } \" , sys_prompt = get_official_agent_prompt ( f \"Player { i + 1 } \" ), model = agentscope_model , model = tuner . as_agentscope_model ( agent_name = f \"Player { i + 1 } \" , target_tag = agent_role , # \ud83c\udf1f tag agents with their role debug_model = chosen_model_for_current_agent # \ud83c\udf1f assign a debug model, ONLY used when we are NOT training this agent ) formatter = OpenAIMultiAgentFormatter (), )] Type 2: Raw OpenAI SDK Agents import openai client = openai . OpenAI ( api_key = 'dummy-api-key' ) response = client . chat . completions . create ( model = \"gpt-3.5-turbo\" , # You can replace this with \"gpt-4\" if available messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello! Tell me a joke about programming.\" } ], max_tokens = 100 , # Limit the response length temperature = 0.7 # Control the randomness of the output )","title":"Tuner v2"},{"location":"stylesheets/tuner_v2/#type-1-2-agentscope-agents-triple-m-multi-role-multi-agent-multi-turn-case","text":"roles = [ \"werewolf\" ] * 3 + [ \"villager\" ] * 3 + [ \"seer\" , \"witch\" , \"hunter\" ] players = [] for i , agent_role in enumerate ( roles ): if agent_role != \"werewolf\" : chosen_model_for_current_agent = OpenAIChatModel ( model_name = \"qwen-max\" , stream = False ) else : chosen_model_for_current_agent = OpenAIChatModel ( model_name = \"qwen-plus\" , stream = False ) players += [ ReActAgent ( name = f \"Player { i + 1 } \" , sys_prompt = get_official_agent_prompt ( f \"Player { i + 1 } \" ), model = agentscope_model , model = tuner . as_agentscope_model ( agent_name = f \"Player { i + 1 } \" , target_tag = agent_role , # \ud83c\udf1f tag agents with their role debug_model = chosen_model_for_current_agent # \ud83c\udf1f assign a debug model, ONLY used when we are NOT training this agent ) formatter = OpenAIMultiAgentFormatter (), )]","title":"Type 1-2: AgentScope Agents: Triple-M (Multi-Role, Multi-Agent, Multi-Turn) Case"},{"location":"stylesheets/tuner_v2/#type-2-raw-openai-sdk-agents","text":"import openai client = openai . OpenAI ( api_key = 'dummy-api-key' ) response = client . chat . completions . create ( model = \"gpt-3.5-turbo\" , # You can replace this with \"gpt-4\" if available messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello! Tell me a joke about programming.\" } ], max_tokens = 100 , # Limit the response length temperature = 0.7 # Control the randomness of the output )","title":"Type 2: Raw OpenAI SDK Agents"}]}