# Math Agent

Train a **tool-using Math Agent** (ReAct + Python executor) to solve GSM8K-style math problems. Rewards come from a **judge** that checks final-answer correctness.

---

## Overview

<div class="callout-tip">
<p>
In <strong>Math Agent</strong>, each training sample is a math word problem (e.g., GSM8K). The agent learns to reason step by step (ReAct-style), call a Python tool when computation is needed, and produce a final answer that matches the reference.
</p>
</div>

This tutorial is organized in two steps:

1. **Run it**: Download the dataset and start training with the default YAML config
2. **Understand & customize**: Read the workflow and the judge/reward logic

---

## Quick Start

### Prepare Dataset

Download the `openai/gsm8k` dataset:

```bash
python scripts/download_dataset.py --target=openai/gsm8k --path=/the/path/to/store/dataset
```

### Start Training

```bash
# (optional) recommended cleanup before training
# ajet --kill="python|ray|vllm"

ajet --conf tutorial/example_math_agent/math_agent.yaml --backbone='trinity' --with-ray
```

??? tip "Quick Debugging (Optional)"
    If you want to breakpoint-debug the workflow/judge locally:

    ```bash
    # (optional) recommended cleanup before debug
    # ajet --kill="python|ray"

    clear && \
    ajet --conf tutorial/example_math_agent/math_agent.yaml --backbone='debug' --with-logview
    ```

    When `--backbone=debug`, Ray is disabled. You can use a VSCode launch config:

    ```json title=".vscode/launch.json"
    {
      "version": "0.2.0",
      "configurations": [
        {
          "name": "Python Debugger: Launch rollout",
          "type": "debugpy",
          "request": "launch",
          "module": "ajet.cli.launcher",
          "console": "integratedTerminal",
          "args": [
            "--backbone", "debug",
            "--conf", "./path/to/yaml.yaml"
          ],
          "env": {}
        }
      ]
    }
    ```

---

## Understanding the Training Pipeline

### What Happens Each Step

<div class="workflow-single">
<div class="workflow-header">Training Step Flow</div>

<div class="workflow">
<ol class="workflow-steps">
<li><strong>Load one problem</strong>

Load a math problem from the dataset via `task_reader`.</li>
<li><strong>Run the AgentScope workflow</strong>

Build the prompt, let the ReAct agent call Python tools, and extract the final answer.</li>
<li><strong>Register info for evaluation</strong>

Return `WorkflowOutput(reward=None, metadata={"final_answer": final_answer})`.</li>
<li><strong>Run the judge</strong>

Compare `final_answer` with reference, compute `raw_reward` and `is_success`.</li>
</ol>
</div>
</div>

### YAML Configuration

Most wiring happens in `tutorial/example_math_agent/math_agent.yaml`:

```yaml title="math_agent.yaml"
ajet:
  task_reader:
    type: huggingface_dat_repo   # also supports: dataset_file / env_service

  rollout:
    agentscope_workflow: tutorial.example_math_agent.math_agent->ExampleMathLearn

  task_judge:
    judge_protocol: tutorial.example_math_agent.math_answer_as_judge->MathAnswerAndLlmAsJudge

  model:
    path: YOUR_MODEL_PATH
```

| Field | Description |
|-------|-------------|
| `task_reader` | Where tasks come from |
| `agentscope_workflow` | Which workflow runs per sample |
| `judge_protocol` | Which judge computes rewards |
| `model.path` | Pretrained model to fine-tune |

### Code Walkthrough

**Workflow:** `tutorial/example_math_agent/math_agent.py`

```python title="Workflow Sketch"
self.toolkit = Toolkit()
self.toolkit.register_tool_function(execute_python_code)

self.agent = ReActAgent(
    name="math_react_agent",
    sys_prompt=system_prompt,
    model=model_tuner,  # trainer-managed model wrapper
    formatter=DashScopeChatFormatter(),
    toolkit=self.toolkit,
    memory=InMemoryMemory(),
)

msg = Msg("user", init_messages[0]["content"], role="user")
result = await self.agent.reply(msg)
final_answer = extract_final_answer(result)

# IMPORTANT: provide final answer to the judge via WorkflowOutput metadata
return WorkflowOutput(reward=None, metadata={"final_answer": final_answer})
```

!!! warning "Important"
    Always provide the final answer via `WorkflowOutput.metadata` so the judge can score it.

### Reward Computation

The judge receives:

| Object | Contains |
|--------|----------|
| `workflow_task` | Task info; reference answer from `metadata` |
| `workflow_output` | Workflow result; final answer from `metadata["final_answer"]` |

!!! tip "Extending the Judge"
    If you observe issues like "almost solved but messed up tool-call formatting", you can extend the judge to add:

    - Format penalty (invalid `<tool_call>`)
    - Behavior penalty (tool called but no `print`)
    - Keep answer correctness as the primary signal

---

## Results

### Training Curve

![Training curve](https://img.alicdn.com/imgextra/i4/O1CN01gzwgLq1fkCnauydEu_!!6000000004044-2-tps-1422-550.png)

!!! info "Visualization"
    Training curves are generated by SwanLab. See [Visualization Tools](./visualization.md) for setup.

**Interpretation:** As training progresses, reward increases. This usually means the agent becomes more stable at:

- **Using tools when needed**: Correctly emitting `<tool_call>` and calling `execute_python_code`
- **Producing reliable answers**: Using tool output to produce final answers aligned with reference

### Case Study: Tool Discipline Improvement

Before training, the agent may solve many problems but often fails at **tool-call discipline**:

=== "Bad Cases"

    ```text
    # bad case 1: forgot to print the result in python code
    <tool_call>
    {"name": "execute_python_code", "arguments": {"code": "... height_difference"}}
    </tool_call>

    # bad case 2: too impatient â€” outputs final answer without waiting for tool result
    <tool_call> {"name": "execute_python_code", ...} </tool_call>
    <tool_call> {"name": "generate_response", "arguments": {"response": "... \\boxed{48} ..."}} </tool_call>
    ```

    These failures are not because the model "can't do math", but because it **does not close the loop** by incorporating the tool execution result.

=== "Good Case (After Tuning)"

    After tuning, the agent follows a clean 3-stage pattern:

    1. **Message 3 (assistant)**: Decomposes problem + emits `<tool_call>` with `print(...)`
    2. **Message 4 (tool_response)**: Tool returns execution results
    3. **Message 5 (assistant)**: Reads `stdout` and produces final answer

    ![Good case](https://img.alicdn.com/imgextra/i4/O1CN01v1gGQZ1ftMiil5Cxg_!!6000000004064-2-tps-1367-684.png)

!!! note "Token-level Visualization"
    The colored blocks show token-level sequence visualization from [Beast-Logger](./beast_logger.md):

    - **Yellow tokens**: Excluded from loss computation
    - **Blue tokens**: Participate in loss computation (light to dark = high to low logprob)

---

## Next Steps

<div class="card-grid">
<a href="../example_werewolves/" class="feature-card"><div class="card-header"><img src="https://api.iconify.design/mdi:wolf.svg" class="card-icon card-icon-multimodal" alt=""><h3>Werewolves</h3></div><p class="card-desc">Explore multi-agent collaborative training.</p></a>
<a href="../example_app_world/" class="feature-card"><div class="card-header"><img src="https://api.iconify.design/mdi:application.svg" class="card-icon card-icon-agent" alt=""><h3>AppWorld</h3></div><p class="card-desc">Train agents for real-world app interactions.</p></a>
<a href="../visualization/" class="feature-card"><div class="card-header"><img src="https://api.iconify.design/mdi:chart-line.svg" class="card-icon card-icon-general" alt=""><h3>Visualization</h3></div><p class="card-desc">Monitor and analyze your training progress.</p></a>
</div>
