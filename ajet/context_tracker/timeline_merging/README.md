# AgentJet Timeline

In complex multi-agent LLM interactions, we define a Timeline as the token trajectory generated by repeatedly invoking an LLM during a task execution process.

A Timeline contains the following elements:
- Text message list
    - Note: In most Qwen models, messages start with `<|im_start|>` and end with `<|im_end|>`, depending on the model's tokenizer and chat_template
- Token sequence message list
    - Note: In most Qwen models, messages start with the Token ID corresponding to `<|im_start|>` and end with the Token corresponding to `<|im_end|>`, depending on the model's tokenizer
- Author list
    - Note: Identifies the producer of each message. Generally, there are two types: "llm" and "env".
- Token LogProb message list
    - Note: Records the log probability of each token generation. If a token's producer is not "llm", the value is `INVALID_LOG_PROB_VALUE` (set to 0 or `np.inf` as needed)
- Loss Mask message list
    - Note: Each bit of loss_mask corresponds one-to-one with tokens
    - loss_mask=1 means the token participates in loss calculation, and usually also indicates that the token is generated by the LLM
    - loss_mask=0 means it does not participate in loss calculation, and in most cases represents that the token comes from user input, tokenizer and chat_template additions, environment feedback, etc.


## Intertwining Timelines in Multi-Turn Conversations and Multi-Agent Scenarios

In multi-turn conversations and multi-agent scenarios, extracting clean and tidy timelines is not easy:

- For ease of use and compatibility with most Agentic frameworks and parallel LLM calls, AgentJet only perceives standard OpenAI-format LLM requests sent by users (or Agent frameworks), without requiring users to provide causal relationships between LLM requests.

- Some Agentic frameworks (such as Langchain, etc.) automatically execute retry operations to improve task success rates. For example, when LLM tool call parameters are invalid, the Agent framework appends error information as temporary context to the request. After achieving the expected result, these temporary contexts are removed. Without proper handling, samples generated during this process can significantly reduce RL algorithm efficiency.

- Application of dynamic memory mechanisms: You can use projects like [ReMe](https://github.com/agentscope-ai/ReMe) to provide agents with short-term and long-term memory capabilities, significantly improving agent performance in personal assistant tasks. When an agent decides to update knowledge existing in the historical context, it creates a fork point in the timeline.

- When multiple agents exist in the environment and the current task is in a partially observable environment (for example, individual agent contexts store secrets that cannot be mutually observed; or each agent actively shields some information through context offload techniques to better focus on the current task), multiple timelines naturally arise, with each timeline belonging to one agent.

- When a token sequence is decoded into text and then re-encoded back into a token sequence by the tokenizer, it sometimes cannot be precisely converted back to the original token sequence (this drift occurs with varying probabilities in different models). This token drift requires fine-grained processing to (1) improve training efficiency and (2) stabilize training.


In the AgentJet system, we adopt the approach of "timeline merging":

**AgentJet autonomously identifies differences between different timelines at the end of an episode, and according to user presets, finds timelines that can be merged and automatically completes the merge. This reduces the number of overlapping redundant samples and improves training efficiency.**

## Timeline Merging Algorithm

When an episode starts, AgentJet initializes a context tracker object that captures all LLM requests. Each LLM request starts from `<|im_start|>` and ends at `<|im_end|>` or when the token count overflows. Each LLM request is considered an independent initial timeline before merging. In an episode, n initial timelines from m agents can be collected:

$\text{Timelines} = \lbrace
T_1\left(M_\text{1}, m_\text{1}, a_\text{1}\right),
T_2\left(M_\text{2}, m_\text{2}, a_\text{2}\right),
\dots,
T_n\left(M_\text{n}, m_\text{n}, a_\text{n}\right)
\rbrace$

Where:
- $T_i$ represents the $i$-th (unmerged) timeline. $T_i = [T_{i}^{[1]}, T_{i}^{[2]}, \dots, T_{i}^{[|T_{i}|]}]$.
    - The last item $T_{i}^{[|T_{i}|]} = m_\text{i}$: always the output of this LLM request.
    - The first $|T_{i}|-1$ items: always the input $M_\text{i}$ of this LLM request.
- $a_\text{i} \in \lbrace A_1, \dots, A_m \rbrace$ represents the agent's name ID. It's worth mentioning that when the user's workflow doesn't provide an agent name, $\lbrace T_1, T_2, \dots, T_n\rbrace$ is considered to be from the same agent (default agent).
- $M_\text{i}$ and $m_\text{i}$ represent the input message list and output message respectively. Each message has a Text, Token, Loss Mask triplet.

At the end of an episode, all timelines are compared. If two timelines satisfy the following conditions:

- Condition 1: $|T_{i}| <= |T_{j}|$
- Condition 2: The token sequences of the first $|T_{i}|$ messages of $T_{i}$ and $T_{j}$ are identical. That is, $\text{Token}(T_{i}^{[k]}) = \text{Token}(T_{j}^{[k]}), \forall k \in \left[1, |T_{i}| \right]$.

Then the two timelines are merged:

- $T_{i}$ is the **absorbed** short timeline; $T_{j}$ is the long timeline to be updated.
- If there is a set of identical messages satisfying $\text{Author}(T_{i}^{[k]}) = \text{llm}$ and $\text{Author}(T_{j}^{[k]}) \neq \text{llm}$, then:
    - $\text{Author}(T_{j}^{[k]}) = \text{llm}$
    - $\text{Token}(T_{j}^{[k]}) = \text{Token}(T_{i}^{[k]})$
    - $\text{TokenLogProb}(T_{j}^{[k]}) = \text{TokenLogProb}(T_{i}^{[k]})$

    ```python
    def toggle_author_and_mask(
        source_timeline: List[ExtendedMessage], # the longer timeline
        target_timeline: List[ExtendedMessage], # the shorter timeline
    ) -> List[ExtendedMessage]:
        for k in range(len(target_timeline)):
            if target_timeline[k].author == "llm" and source_timeline[k].author != "llm":
                source_timeline[k].author = target_timeline[k].author
                source_timeline[k].token_arr = target_timeline[k].token_arr
                source_timeline[k].token_logprob_arr = target_timeline[k].token_logprob_arr
                assert source_timeline[k].need_training
        return source_timeline  # merged timeline
    ```

Note: Loss Mask is calculated in detail during post-processing based on the $\text{Author}(\cdot)$ list, so there's no need to focus on it when merging timelines.

## More Relaxed Merging Conditions for Faster Training

### Relaxed Token Matching

In practice, we found that when a token sequence is decoded into text and then re-encoded back into a token sequence by the tokenizer, it sometimes cannot be precisely converted back to the original token sequence.

Therefore, the following situation often occurs in reality:
- $\text{Author}(T_{i}^{[k]}) = \text{llm}$
- $\text{Author}(T_{j}^{[k]}) \neq \text{llm}$
- $\text{Text}(T_{j}^{[k]}) = \text{Text}(T_{i}^{[k]})$
- $\text{Token}(T_{j}^{[k]}) \neq \text{Token}(T_{i}^{[k]})$

This means the text sequences are completely equal, but during vLLM's internal tokenizer conversion, two variants of token sequences were produced.
In this case, you can control AgentJet's behavior by adjusting:
```yaml
ajet.context_tracker.timeline_merging_policy.timeline_compare_level = "text" / "token"  # (default text)
```


| Merge Strategy | Merge Condition | Use Case | Advantages | Disadvantages |
|---------------|----------------|----------|------------|---------------|
| **token** | Requires $\text{Token}(T_{i}^{[k]}) = \text{Token}(T_{j}^{[k]})$ | Token sequences must be completely identical to merge | Strict matching, high training data precision | Due to tokenizer encoding/decoding drift, may fail to merge timelines that should be merged, reducing training efficiency |
| **text** | Only requires $\text{Text}(T_{i}^{[k]}) = \text{Text}(T_{j}^{[k]})$ | Can merge when text content is the same, tolerates token sequence differences | More relaxed merging conditions, improves merge rate and training efficiency, reduces redundant samples | May merge samples with slightly different token representations, but impact is minimal in practice |

**Recommended Configuration:**
- Use the `"text"` strategy by default, which effectively handles token drift during tokenizer encoding/decoding.
- Use the `"token"` strategy when strict training-inference consistency is required.

### Relaxed Tool Matching

Most model tokenizer chat templates place the list of tools to be used at the very beginning (system prompt).
When the agent's tool list is fine-tuned but other context remains unchanged, you can control AgentJet's behavior by adjusting:
```yaml
ajet.context_tracker.timeline_merging_policy.ignore_tools = True / False  # (default True)
```

| Merge Strategy | Merge Condition | Use Case | Advantages | Disadvantages |
|---------------|----------------|----------|------------|---------------|
| **True** | Ignores tool list differences, can merge as long as other context is the same | Agent tool list changes dynamically, but core dialogue logic remains unchanged | Significantly improves merge rate, reduces redundant samples caused by tool list changes, enhances training efficiency | May merge samples with slightly different tool environments, but impact is limited in most scenarios |
| **False** | Strictly compares tool lists, tool lists must be completely identical to merge | Tool invocation is critical for training, requires precise tool configuration matching | Ensures timeline tool environments are completely consistent, training data strictly aligned | When tool list has minor changes, cannot merge timelines with the same context, reducing training efficiency |

**Recommended Configuration:**
- Use the `True` strategy to effectively reduce redundant samples.
- Use the `False` strategy when strict training-inference consistency is required. Also recommended when agent tools change significantly and infrequently (such as dynamic tool loading, tool version updates, etc.).

## Other Timeline Management Options

### Automatic Re-tokenization Drift Fixing

By default, AgentJet automatically performs re-tokenization drift fixing based on Token IDs returned by the vLLM engine. This consumes a little extra CPU power.

```yaml
ajet.context_tracker.fix_retokenization_drift = True  # (default True)
```

For details on the re-tokenization drift phenomenon, you can follow https://github.com/vllm-project/vllm/pull/22587 for more information.

### Detecting Timeline Divergence Points

In single-agent multi-turn conversation scenarios, if you are very concerned about training efficiency and want to diagnose in detail at what moment and for what reason your Agentic framework caused timeline forks, you can enable:

```yaml
ajet.context_tracker.detect_timeline_snap = False  # (default False)
```

Enable real-time detection of timeline divergence points. This consumes CPU power and slows down the training process. Only recommended for use in debug mode (`--backbone=debug`).
