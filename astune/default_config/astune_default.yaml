# ------------------ 主要配置 ------------------
astune:
  project_name: appworld_astune
  experiment_name: "read_yaml_name"

  task_reader:
    type: env_service # `env_service` or `dataset_file` or `huggingface_dat_repo`
    env_service:
      env_type: "appworld"
      env_url: "http://127.0.0.1:8080"
      env_action_preference: code # code, text, box
      training_split: train
      validation_split: dev
    dataset_file:
      training:
        file_path: "xxxx.jsonl"
      validation:
        file_path: "xxxx.jsonl"
    huggingface_dat_repo:
      dataset_path: "gsm8k"
      training_split: "train"
      validation_split: "validation"

  task_judge:
    judge_protocol: astune.task_judge.env_service_as_judge->EnvServiceJudge

  model:
    path: /mnt/data_cpfs/model_cache/modelscope/hub/Qwen/Qwen/Qwen2___5-14B-Instruct

  data:
    max_prompt_length: 3000
    max_response_length: 15000
    train_batch_size: 32

  rollout:
    use_agentscope_protocol: True
    agentscope_learn_protocol: tutorial.appworld->ExampleAgentScopeLearnProtocol
    agentscope_disable_toolcalls: False
    max_env_worker: 128
    use_step_reward_from_env: False
    binary_reward: False
    force_no_think: False
    force_think: False
    mode: async
    compute_madness_checklist:
      - "nonsense"
    gamma: 1.0
    agent_madness_termination: True # terminate_after_gone_mad
    agent_madness_reward: -1.0  # customize the reward when agent is detected as gone mad
    add_special_success_reward: False
    temperature: 0.9
    top_p: 1.0
    max_env_len: 4096
    max_response_length_in_one_turn: 4096
    max_model_len: 18000
    multi_turn:
      max_sample_per_task: 30
      max_steps: 30
      enable: True
    step_skip_action: 0 # skip action generation every N steps, 0 means never skip
    submit_oversample_multiplier: 1.5
    enable_oversample: True
    num_repeat: 4
    name: vllm
    val_kwargs:
      temperature: 0.0
      top_k: -1
      top_p: 1.0
      do_sample: False
      num_repeat: 1

  context_manager: # context manager protocol is used ONLY when `use_agentscope_protocol=False`
    context_manager_type: "linear"
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512
    auto_context_cm:
      train_sp_action: False
      token_num_trigger_clip: 8000
    sliding_window_cm:
      enable_llm_memory_extraction: False
    linear_think_cm:
      remove_think_before_submit_as_action: False
      extract_box_before_submit_as_action: False
      train_history_infer_token: True

  debug:
    debug_max_parallel: 16
    debug_first_n_tasks: 2
    debug_vllm_port: 18000
    debug_vllm_seed: 12345
    debug_tensor_parallel_size: 4

  trainer_common:
    val_before_train: True
    val_pass_n: 4
    nnodes: 1
    n_gpus_per_node: 8
    logger:
      - console
      - swanlab
    algorithm:
      task_norm_patch: False
      adv_estimator: grpo
      use_kl_in_reward: False
    mini_batch_num: 1
    fsdp_config:
      param_offload: True
      optimizer_offload: True
    optim:
      lr: 1e-6
    use_kl_loss: True
    kl_loss_coef: 0.002
    kl_loss_type: low_var_kl