# ------------------ main configuration ------------------
astune:
  project_name: appworld_astune
  experiment_name: "read_yaml_name"
  backbone: debug # debug or trinity or verl

  task_reader:
    type: env_service # `env_service` or `dataset_file` or `huggingface_dat_repo`
    env_service:
      env_type: "appworld"
      env_url: "http://127.0.0.1:8080"
      env_action_preference: code # code, text, box
      training_split: train
      validation_split: dev
    dataset_file:
      training:
        file_path: "xxxx.jsonl"
      validation:
        file_path: "xxxx.jsonl"
    huggingface_dat_repo:
      dataset_path: "gsm8k"
      training_split: "train"
      validation_split: "validation"

  task_judge:
    judge_type: rubrics_auto_grader  # Options: custom_protocal, rubrics_auto_grader
    judge_protocol: astune.task_judge.env_service_as_judge->EnvServiceJudge
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512

    rubrics_auto_grader:
      # rubrics begin
      model_name: qwen-max
      grader_mode: pointwise
      language: en
      min_score: 0
      max_score: 1
      success_threshold: 0.7
      sampling_mode: all_samples
      generate_number: 1
      max_epochs: 2
      max_retries: 3
      aggregation_mode: keep_all
      grader_name: Math Auto Grader
      num_reference_samples: 20

      query_field: main_query
      answer_field: final_answer
      reference_field: answer
      # rubrics end


  model:
    path: /mnt/data_cpfs/model_cache/modelscope/hub/Qwen/Qwen/Qwen2___5-14B-Instruct

  data:
    max_prompt_length: 3000
    max_response_length: 15000
    train_batch_size: 32

  rollout:
    use_agentscope_protocol: True
    agentscope_learn_protocol: tutorial.example_appworld.appworld->ExampleAgentScopeLearnProtocol
    agentscope_disable_toolcalls: False
    max_env_worker: 128
    use_step_reward_from_env: False
    binary_reward: False
    force_no_think: False
    force_think: False
    mode: async
    compute_madness_checklist:
      - "nonsense"
    gamma: 1.0
    agent_madness_termination: True # terminate_after_gone_mad
    agent_madness_reward: -1.0  # customize the reward when agent is detected as gone mad
    add_special_success_reward: False
    temperature: 0.9
    top_p: 1.0
    max_env_len: 4096
    max_response_length_in_one_turn: 4096
    max_model_len: 18000
    multi_turn:
      max_sample_per_task: 30
      max_steps: 30
      enable: True
    step_skip_action: 0 # skip action generation every N steps, 0 means never skip
    submit_oversample_multiplier: 1.5
    enable_oversample: True
    tensor_model_parallel_size: 1
    max_num_seqs: 10
    num_repeat: 4
    name: vllm
    val_kwargs:
      temperature: 0.0
      top_k: -1
      top_p: 1.0
      do_sample: False
      num_repeat: 1

  context_tracker: # context tracker protocol is used ONLY when `use_agentscope_protocol=False`
    context_tracker_type: "linear"
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512
    # auto_context_cm:
    #   train_sp_action: False
    #   token_num_trigger_clip: 8000
    # sliding_window_cm:
    #   enable_llm_memory_extraction: False
    # linear_think_cm:
    #   remove_think_before_submit_as_action: False
    #   extract_box_before_submit_as_action: False
    #   train_history_infer_token: True

  debug:
    debug_max_parallel: 16
    debug_first_n_tasks: 2
    debug_vllm_port: 18000
    debug_vllm_seed: 12345
    debug_tensor_parallel_size: 4

  trainer_common:
    val_before_train: False
    val_pass_n: 4
    save_freq: 20
    test_freq: 20
    total_epochs: 50
    nnodes: 1
    n_gpus_per_node: 8
    logger:
      - console
      - swanlab
    algorithm:
      task_norm_patch: False
      adv_estimator: grpo
      use_kl_in_reward: False
    mini_batch_num: 1
    fsdp_config:
      param_offload: True
      optimizer_offload: True
    optim:
      lr: 1e-6
    use_kl_loss: True
    kl_loss_coef: 0.002
    kl_loss_type: low_var_kl
    trinity_only__n_gpu_for_infer: 2

  # the tracing config
  tracing:
    base_url: ./.trash/database.sqlite
    train_output_path: ./.trash/tasks.jsonl
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 2048
    filters:
      - type: llm_evaluate
        enabled: true
        params:
          custom_rubrics: If the answer claims that it has written the output to a file, consider it an invalid response.
          temperature: 0.5
          print_reason: false
          max_thread: 16

  execute_test: False # DO NOT EDIT, FOR TESTING PURPOSE ONLY
  execute_testing_lambda: ""
