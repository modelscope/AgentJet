astune:
  model:
    path: /mnt/data/model_cache/modelscope/hub/Qwen/Qwen/Qwen2___5-14B-Instruct
  data:
    max_prompt_length: 3000
    max_response_length: 15000
    train_batch_size: 32
  rollout:
    use_agentscope_protocol: True
    agentscope_learn_protocol: null
    use_step_reward_from_env: False
    binary_reward: False
    force_no_think: False
    force_think: False
    compute_madness_checklist:
      - "nonsense"
    gamma: 1.0
    agent_madness_termination: True # terminate_after_gone_mad
    agent_madness_reward: -1.0  # customize the reward when agent is detected as gone mad
    add_special_success_reward: False
    temperature: 0.9
    top_p: 1.0
    max_env_len: 4096
    max_response_length_in_one_turn: 4096
    max_model_len: 18000
    multi_turn:
      max_sample_per_task: 30
      max_steps: 30
    step_skip_action: 0 # skip action generation every N steps, 0 means never skip
    submit_oversample_multiplier: 1.5
    enable_oversample: True
    num_repeat: 4
    val_kwargs:
      temperature: 0.0
      top_k: -1
      top_p: 1.0
  context_manager: # context manager protocol is used ONLY when `use_agentscope_protocol=False`
    context_manager_type: "linear"
    alien_llm_model: qwen3-235b-a22b-instruct-2507
    alien_llm_response_length: 512
    auto_context_cm:
      train_sp_action: False
      token_num_trigger_clip: 8000
    sliding_window_cm:
      enable_llm_memory_extraction: False
    linear_think_cm:
      remove_think_before_submit_as_action: False
      extract_box_before_submit_as_action: False
      train_history_infer_token: True



########################## verl config below ##########################
trainer:
  val_before_train: False
  hfmodelpath: ""
  experiment_name: "read_yaml_name"
  n_gpus_per_node: 8
  nnodes: 1
  save_freq: 20
  test_freq: 20
  total_epochs: 50
  project_name: appworldnew
  validation_data_dir: "experiments/exp_default/validation_log"
  rollout_data_dir: "experiments/exp_default/rollout_log"
  critic_warmup: 0
  eval_pass_n: 4
  logger:
    - console
    - swanlab


data:
  val_batch_size: 100000000000
  return_raw_chat: True
  filter_overlong_prompts: True
  truncation: error
  fast_eval: True
  train_batch_size: 32
  max_prompt_length: 3000
  max_response_length: 15000

algorithm:
  task_norm_patch: False
  adv_estimator: grpo
  use_kl_in_reward: False

actor_rollout_ref:
  hybrid_engine: True
  actor:
    entropy_coeff: 0
    loss_agg_mode: seq-mean-token-mean
    override_ppo_mini_batch_num: 1
    ppo_epochs: 1
    ppo_mini_batch_size: 16
    optim:
      lr: 1e-6
    use_kl_loss: True
    kl_loss_coef: 0.002
    kl_loss_type: low_var_kl
    ppo_micro_batch_size_per_gpu: 1
    ppo_max_token_len_per_gpu: 18000
    use_dynamic_bsz: True
    fsdp_config:
      param_offload: True
      optimizer_offload: True

  rollout:
    name: vllm
    mode: async
    max_env_len: 3000
    response_length: 3000
    prompt_length: 15000
    max_model_len: 18000
    use_agentscope_protocol: False
    ppo_micro_batch_size_per_gpu: 1
    tensor_model_parallel_size: 1
    max_num_seqs: 10
    gpu_memory_utilization: 0.9
    max_env_worker: 64
    log_prob_max_token_len_per_gpu: 18000
    temperature: 0.9
    top_p: 1.0
    gamma: 1.0
    enforce_eager: True
    log_prob_micro_batch_size_per_gpu: 4
    multi_turn:
      completion_callback: beyondagent.module.trainer.simple_completion_callback.SimpleCompletionCallback
      enable: True
      format: llama3_json
      max_steps: 30
      tool_config_path: null
    custom_dataflow_cls:
      path: ""
      name: ""
    val_kwargs:
      top_k: -1
      top_p: 1.0
      temperature: 0
      do_sample: False
  ref:
    use_dynamic_bsz: True
    log_prob_micro_batch_size_per_gpu: 4
    log_prob_max_token_len_per_gpu: 18000
    fsdp_config:
      param_offload: True

  model:
    use_remove_padding: True
    enable_gradient_checkpointing: True

