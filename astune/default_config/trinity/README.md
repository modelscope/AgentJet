# Trinity Configuration Guide ğŸ› ï¸

## How to Modify Trinity Configuration in ASTuner

1. ğŸ¯ **Recommended Method**: In most cases, you do not need to directly adjust Trinity parameters. Simply refer to and modify the upper-level `astune/default_config/astune_default.yaml` configuration file, and ASTuner will **automatically** handle parameter mapping for you.

2. âš™ï¸ **Special Cases**: Some Trinity tuning parameters are not yet mapped in ASTuner. You can refer to Trinityâ€™s documentation and modify them in the following format:

```yaml
trinity:
  algorithm:
    algorithm_type: multi_step_grpo
```

3. ğŸš« **Never Edit**:
   - Never edit `astune/default_config/trinity/trinity_launcher.yaml`
   - Never edit `astune/default_config/trinity/trinity_default.yaml`

## Configuration Mapping Modification ğŸ”„

Some ASTune configurations overlap with Trinity.
You can configure mappings via the `astune/default_config/trinity/config_auto_convertion_trinity.json` file.

## Trinity Hyperparameter Quick Guide ğŸ“Š

Trinity adopts a typical producer (explorer)-consumer (trainer) architecture:
- ğŸ­ **Producer**: Uses VLLM to generate samples
- ğŸ§  **Consumer**: Consumes samples to update the model
Both operate on different runtime schedules.

### Explorer Core Parameters ğŸ”

- `buffer.batchsize`: The minimum unit for reading task data from the dataset. Each read increments the explorer step count by 1.
- `repeat_times`: The number of repetitions per task, also the group size (G) in GRPO.
- `engine_num`: Number of VLLM engines.
- `tensor_parallel_size`: Number of GPUs occupied by each VLLM engine.
- `engine_num * tensor_parallel_size`: Total number of GPUs used by the explorer.
- `eval_interval`: Evaluation interval (in explorer steps).

### Trainer Core Parameters ğŸ‹ï¸

- `buffer.train_batch_size`: The minimum unit consumed from the explorerâ€™s production queue. Each read triggers one optimization step.
- `trainer.save_interval`: Parameter save interval (in trainer steps).

### Explorer-Trainer Coordination Parameters ğŸ¤

- `sync_interval`: Synchronization interval.
- `sync_offset`: Synchronization offset.
- `sync_style`: Synchronization method.

### Runtime Instance Analysis ğŸ“ˆ

**Supply Side**: Explorer runs 89 steps Ã— batch size (8) Ã— repeat times (4) Ã— tasks per round (â‰ˆ1) = 2,848 samples.

meanwhile

**Demand Side**: Trainer runs 10 steps Ã— training batch size (264) = 2,640 samples.

### Training Memory Control ğŸ’¾

Same as VERL, control training memory with the following parameters:
- `trainer.max_token_len_per_gpu`
- `ulysses_sequence_parallel_size`
